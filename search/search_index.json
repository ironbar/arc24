{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"arc24","text":"<p>Create an AI capable of solving reasoning tasks it has never seen before</p> <p>https://www.kaggle.com/competitions/arc-prize-2024</p> <p>https://arcprize.org/arc</p> <p></p>"},{"location":"#docs","title":"Docs","text":"<ul> <li>https://ironbar.github.io/arc24</li> <li>Solution summary</li> </ul>"},{"location":"#code-structure","title":"Code structure","text":"<pre><code> |_ notebooks: jupyter notebooks made during the challenge. They start by number for easier sorting.\n |_ docs: documents made during the challenge according to CRISP-DM methodology\n |_ tests: folder with tests for the library\n |_ data: folder with light data from the challenge\n |_ rules: the official rules of the challenge\n |_ scripts: scripts made during the challenge for training, data processing...\n</code></pre>"},{"location":"01_Business_Understanding/","title":"Business Understanding","text":""},{"location":"01_Business_Understanding/#challenge-description","title":"Challenge description","text":"<p>The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark measures an AI system's ability to efficiently learn new skills. Humans easily score 85% in ARC, whereas the best AI systems only score 34%. The ARC Prize competition encourages researchers to explore ideas beyond LLMs, which depend heavily on large datasets and struggle with novel problems.</p> <p>The objective of this competition is to create an algorithm that is capable of solving abstract reasoning tasks. Critically, these are novel tasks: tasks that the algorithm has never seen before. Hence, simply memorizing a set of reasoning templates will not suffice.</p> <p></p>"},{"location":"01_Business_Understanding/#evaluation","title":"Evaluation","text":"<p>This competition evaluates submissions on the percentage of correct predictions. For each task, you should predict exactly 2 outputs for every test input grid contained in the task. (Tasks can have more than one test input that needs a predicted output.) Each task test output has one ground truth. For a given task output, any of the 2 predicted outputs matches the ground truth exactly, you score 1 for that task test output, otherwise 0. The final score is the sum averaged of the highest score per task output divided by the total number of task test outputs.</p>"},{"location":"01_Business_Understanding/#assess-situation","title":"Assess situation","text":"<p>I will devote at least the 2 following months (July and August) to better understand intelligence. This challenge is a good applied exercise of that.</p> <p>It is a code competition and runtime should be less than 12 hours.</p>"},{"location":"01_Business_Understanding/#terminology","title":"Terminology","text":""},{"location":"01_Business_Understanding/#questions","title":"Questions","text":""},{"location":"01_Business_Understanding/#project-plan","title":"Project Plan","text":"<p>No one knows how to solve this problem yet. So this competition is about deeply understanding the problem, designing and building a solution.</p> <p>The first step for this competition is to read about the state of the art and explore the space of possible solutions.</p>"},{"location":"02_Data_Understanding/","title":"Data Understanding","text":""},{"location":"02_Data_Understanding/#collect-initial-data","title":"Collect initial data","text":"<p>https://github.com/fchollet/ARC-AGI</p> <p>There are 400 training tasks and 400 evaluation tasks. The evaluation tasks are said to be more difficult than the training tasks.</p> <p>The test set is hidden and it has 100 new and unseen tasks.</p> <p>The tasks are stored in JSON format.</p>"},{"location":"02_Data_Understanding/#external-data","title":"External data","text":"<p>There are some variations of the ARC dataset:</p> <ul> <li>ConceptARC is a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on many basic spatial and semantic concepts. It differs from the original ARC dataset in that it is specifically organized around \"concept groups\" -- sets of problems that focus on specific concepts and that vary in complexity and level of abstraction. It seems to be easier than the original ARC benchmark.</li> <li>Mini-ARC a 5 \u00d7 5 compact version of the ARC, was generated manually to maintain the original\u2019s level of difficulty.</li> <li>1D-ARC A simpler version of ARC tasks with only one dimension.</li> <li>Sort-of-ARC shares ARC\u2019s input space but presents simpler problems with 20\u00d720 images containing three distinct 3\u00d73 objects. I only could find the paper, not the dataset.</li> <li>https://github.com/michaelhodel/re-arc RE-ARC: Reverse-Engineering the Abstraction and Reasoning Corpus by Michael Hodel, member of MindsAI team</li> <li>MC-LARC Text descriptions for the ARC training set.</li> <li>arc-generative-DSL-infinite-data Jack Cole repo \"Slowly building a collection of infinite riddle generators for benchmarking data-hungry methods\"</li> <li>Abstract Reasoning Challenge - community resources</li> <li>arc-dataset-collection Multiple datasets for ARC (Abstraction and Reasoning Corpus)</li> <li>Vercel app to create new tasks</li> </ul>"},{"location":"02_Data_Understanding/#describe-data","title":"Describe data","text":""},{"location":"02_Data_Understanding/#explore-data","title":"Explore data","text":""},{"location":"02_Data_Understanding/#verify-data-quality","title":"Verify data quality","text":""},{"location":"02_Data_Understanding/#amount-of-data","title":"Amount of data","text":""},{"location":"03_State_of_the_art/","title":"State of the art","text":""},{"location":"03_State_of_the_art/#papers","title":"Papers","text":"<p>These are the sources of papers used:</p> <ul> <li>Citations to the \"On the measure of intelligence\" paper on Google Scholar</li> <li>Papers on Arxiv with <code>abstraction reasoning corpus</code> in the title</li> <li>ARC prize resources</li> </ul>"},{"location":"03_State_of_the_art/#getting-50-sota-on-arc-agi-with-gpt-4o-by-ryan-greenblatt","title":"\u2b50\u2b50 Getting 50% (SoTA) on ARC-AGI with GPT-4o by Ryan Greenblatt","text":"<p>I recently got to 50% accuracy on the public test set for ARC-AGI by having GPT-4o generate a huge number of Python implementations of the transformation rule (around 8,000 per problem) and then selecting among these implementations based on correctness of the Python programs on the examples (if this is confusing, go to the next section). I use a variety of additional approaches and tweaks which overall substantially improve the performance of my method relative to just sampling 8,000 programs.</p> <p>As Chollet says in the section below, this would not be SOTA but it is a great result.</p> <p>The additional approaches and tweaks are:</p> <ul> <li>I use few-shot prompts which perform meticulous step-by-step reasoning.</li> <li>I have GPT-4o try to revise some of the implementations after seeing what they actually output on the provided examples.</li> <li>I do some feature engineering, providing the model with considerably better grid representations than the naive approach of just providing images. (See below for details on what a \u201cgrid\u201d in ARC-AGI is.)</li> <li>I used specialized few-shot prompts for the two main buckets of ARC-AGI problems (cases where the grid size changes vs doesn\u2019t).</li> </ul> <p>The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations.</p> <p>My approach is similar in spirit to the approach applied in AlphaCode in which a model generates millions of completions attempting to solve a programming problem and then aggregates over them to determine what to submit.</p> <p>Actually getting to 50% with this main idea took me about 6 days of work. This work includes constructing few-shot prompts, building better text representations of these grids, iterating against the train set, and implementing various other tweaks to improve performance</p> <ul> <li>Github ARC Draw more samples<ul> <li>Few shot prompts with reasoning and python code</li> </ul> </li> </ul>"},{"location":"03_State_of_the_art/#high-level-method","title":"High level method","text":"<ul> <li>Provide the ARC-AGI problem to GPT-4o, with both an image representation and with various text representations for each grid in the problem. The text representations include showing which cells are occupied by different connected components of colors and showing diffs between the input and output (in cases where the grid shapes are the same).</li> <li>Instruct GPT-4o to reason about what the transformation is, reason how to implement the transformation as code, and then finally actually implement the transformation in code.</li> <li>Use a few-shot prompt with several carefully handwritten examples of step-by-step reasoning to actually get GPT-4o to do this reasoning somewhat effectively. The resulting prompt is usually around 30k tokens long including images.<ul> <li>We actually have a pair of few-shot prompts: one prompt for when the grid size changes (in any of the examples) and one for when it doesn\u2019t.</li> <li>We also ensemble over multiple pairs of few-shot prompts. This doesn\u2019t help much, but I already had the samples in the cache.</li> </ul> </li> <li>Sample vast, vast numbers of completions (~5,000 per problem) from GPT-4o.</li> <li>Take the most promising 12 completions for each problem, and then try to fix each by showing GPT-4o what this program actually outputs on the examples, and then asking GPT-4o to revise the code to make it correct. We sample ~3,000 completions that attempt to fix per problem in total across these 12 starting implementations.<ul> <li>We use a few-shot prompt for revisions with step-by-step reasoning.</li> <li>We also have a text representation which shows the diff between the actual output and the expected output.</li> </ul> </li> <li>Then, we select 3 submissions to make based on a majority vote over programs which get the examples correct. (In the case where we don\u2019t have 3 distinct submissions from programs which get the examples right, we apply some heuristics to pick a submission, but this doesn\u2019t matter much.).</li> </ul> <p>In addition to iterating on the training set, I also did a small amount of iteration on a 100 problem subset of the public test set. All the results I presented here were computed on a different subset of the public test set that does not overlap.  The train and test set are not IID, and the test set is both much harder and somewhat qualitatively different (I think), so using a subset of the test set for iteration was useful for quickly getting a better sense of how things change with difficulty. It's unfortunate that these sets aren\u2019t IID: it makes iteration harder and more confusing.</p>"},{"location":"03_State_of_the_art/#what-are-the-returns-to-more-sampling","title":"What are the returns to more sampling?","text":"<p>There appears to be a relatively clean scaling law. The fit is in terms of log base 2. So, it indicates an additional 3% correct per doubling of k</p>"},{"location":"03_State_of_the_art/#tricks-in-the-solution","title":"Tricks in the solution","text":"<ul> <li>Using a better representation is crucial, allows to sample 10 times more efficiently</li> <li>Also revision of the proposed solutions is crucial, allows to sample 10 times more efficiently</li> <li>So without revision and good representation it would have needed close to 1M samples instead of 8k</li> </ul>"},{"location":"03_State_of_the_art/#chollet-reaction","title":"Chollet reaction","text":"<p>SotA on the public evaluation set of ARC-AGI is ~60% for a heterogenous ensemble, 54% for a single approach. But what really matters is the private leaderboard on Kaggle (33% there so far vs 85% needed to beat the challenge)</p> <p>In general, we see 10-20 pts worse performance on the private test set compared to the public eval set.  This is likely in part due to information leak from the public tasks into the model (which can happen in a number of ways, including LLM pretraining, and tuning of an approach's knobs based on eval set performance, aka overfitting). This is also likely in part due to the fact that the eval set contains more \"easy\" tasks. The eval set and test set were not calibrated for difficulty. So while all tasks across the board are feasible for humans, the tasks in the test set may be harder on average. This was not intentional, and is likely either a fluke (there are only 100 tasks in the test set) or due to the test set having been created last.</p>"},{"location":"03_State_of_the_art/#implications-of-this-work","title":"Implications of this work","text":"<p>The fact that they have to generate thousands of responses seems the typical tradeoff between train and inference compute. It is very likely that future and bigger models will need far less generations to achieve the same level of accuracy (see f.e. AlphaCode2 where they replaced PaLM by Gemini with great improvements. They could reach the performance of AlphaCode1 with 100 samples instead of 1e6 samples.)</p> <p>The fact that an LLM without fine-tuning can achieve such high score on the ARC dataset is a contradiction with the postulates of Chollet that say that LLM cannot do in-context learning.</p> <p>I believe this result is more impressive that MindsAI team because GPT4o was not fine-tuned for this task. The MindsAI team fine-tune a model on ARC like tasks and do test time fine-tuning on the test tasks. Thus considering the priors and experience of both models GPT4o is clearly more intelligent than MindsAI team model.</p> <p>There might be some risk of data contamination because the data is publicly available on Github. However the approach is not directly solving the task, but writing python code to solve the task. And that kind of data is not available to the best of my knowledge.</p>"},{"location":"03_State_of_the_art/#neural-networks-for-abstraction-and-reasoning-towards-broad-generalization-in-machines","title":"\u2b50 Neural networks for abstraction and reasoning: Towards broad generalization in machines","text":"<p>Nicely written paper that tries to solve the ARC challenge with two methods:</p> <ol> <li>Dreamcoder. Is a method to create programs given a set of primitive functions</li> <li>LLMs. They show that using <code>transpose</code> and <code>rot90</code> augmentations can double the accuracy of the models. This highlights the sequential and non-2d nature of the typical LLM data.</li> </ol> <p>Results are weak and do not surpass the state of the art.</p> <p>Abstraction and reasoning - developing computer systems that can learn new concepts from a small number of examples, something that humans find relatively easy</p> <p>We revisit whether new advances can allow computers to extrapolate to new concepts rather than merely interpolate.</p> <p>With only a handful of training examples per ARC task, and 10900 possible answers (of which exactly one gains credit), traditional machine learning (ML) methods that require large datasets have so far been unable to make progress.</p> <p>Analogy-making has been considered central to the notion of intelligence. When presented with novel situations (for example; opening a new type of door, or conversing about a new topic), humans effortlessly solve these situations by creating analogies to previous experiences and concepts.</p> <p>Chollet notes that while excellent progress has been made in solving specific tasks to approach or surpass human-level (such as detecting cats and playing Go), these models generally require a huge amount of training and are limited to performing well on situations that they were trained on. The failure of neural network models to perform when extrapolating outside the training data has been widely explored</p> <p>Inductive programming describes algorithms that derive programs that explain a series of examples.</p> <p>when a person attempts an ARC task, we see a similar process: one tries to abstract the training tasks to a \u2018program\u2019 (generally in natural language in one\u2019s head, for example \u201crotate by 90 degrees\u201d); human intuition is the search procedure.</p>"},{"location":"03_State_of_the_art/#icecuber-1st-place-solution-on-arc2020","title":"\u2b50 Icecuber 1st place solution on ARC2020","text":"<p>Unfortunately, I don't feel like my solution itself brings us closer to AGI. The main component is a DSL which applies up to 4 of 142 unary transformations (42 different functions, some have different variants). This DSL is solved by enumeration (exploiting duplicates) + a greedy stacking combiner. Everything is implemented efficiently in C++ (with no dependencies) and running in parallel.</p> <ul> <li>Icecuber solution repo</li> <li>Icecuber solution documentation</li> </ul> <p>I then noticed that distribution of training, evaluation and LB were quite different, so I decided the evaluation dataset was better used as training data. I hand-coded 100 evaluation tasks, which I used to add more transformations, and improve my DSL. I noticed that functions taking more than 1 argument were growing my search space super-exponentially, and that they usually just took either the input or output image size as second argument. This led me to only keep unary functions. I also added lists of images as a type, to solve tasks that required looping. I also started representing my DSL in a DAG to exploit duplicates (multiple transformations giving same output). This required me to rewrite most of my code, but gave me 14 tasks on LB.</p> <p>I double the sample inputs by adding the same tasks flipped along a diagonal. This makes for 3 full runs (depth 3 for performance reasons): one normal, and one for each diagonal. This simple addition moved me from 17 to 21 tasks solved on the LB (and hence helped more than all the optimizations needed for doing depth 4 search instead of depth 3).</p>"},{"location":"03_State_of_the_art/#implications-of-this-work_1","title":"Implications of this work","text":"<p>This work shows that at least 20% of the test set tasks can be solved using just 3-4 transformations (because the search was limited to a depth of 3-4).</p> <p>The ARC set can be solved given enough compute and a complete DSL. This will be a brute-force approach to the problem that won't be intelligent. The intelligence lies in the developer that produced the complete DSL and the search algorithm. This system won't generalize to new problems.</p> <p>The world is too complex to have a DSL, that's why we write new python code for each application. The way we solve the ARC is important, we have to design the more general solution possible.</p>"},{"location":"03_State_of_the_art/#llms-and-the-abstraction-and-reasoning-corpus-successes-failures-and-the-importance-of-object-based-representations","title":"\u2b50 LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations","text":"<p>This paper highlights the fact that LLMs have difficulties understanding the 2d nature of the ARC dataset.</p> <ol> <li>Evaluate the model with few-shot prompt as it is my idea, giving reasoning as input. However I don't believe this is done as extensively as I want to do myself</li> <li>Create a 1D ARC dataset where the models perform better</li> <li>Using a object based representation of the problem where GPT4 solves 23/50 problems (an easy subset from ARC)</li> </ol> <p>What would be the best input for the LLMs to understand 2d structures? I could fine-tune a model to do row, col, diagonal addition or presence detection. This might force the model to create a great representation for the problem.</p> <p>Reasoning is using evidence, arguments, and logic to arrive at conclusions or make judgments</p> <p>A closer examination of the tasks that GPT-4 solved correctly using the direct-grid approach reveals some interesting patterns in the reasoning provided by the model. Out of the 13 tasks that were correctly solved, only three tasks were accompanied by the correct reasoning steps.</p> <p>To address the challenges we have identified thus far and to enhance LLM performance, we propose the integration of an external tool to aid in producing an object representation of a task. More specifically, we leverage the ARGA algorithm to execute object abstraction before prompting LLMs for the solution.</p> <p>Our exploration started with a straightforward, grid-based textual encoding approach, which revealed that GPT struggles due to the non-sequential representation of complex objects in text. We then introduced the 1D-ARC, a simplified, single-dimensional version of the ARC. By reducing the task complexity and dimensionality, we aimed to make ARC tasks more approachable for LLMs. Our evaluations on the 1D-ARC indicated improvements in performance but also highlighted that simplification alone could not bridge all the gaps in GPT\u2019s reasoning processes. In the third phase of our exploration, we adopted an object-based approach, integrating an external tool, the ARGA framework, to assist in object abstraction. This led to significant improvements in GPT\u2019s problem-solving abilities, reaffirming the importance of structured, object-based representations in complex reasoning tasks.</p>"},{"location":"03_State_of_the_art/#addressing-the-abstraction-and-reasoning-corpus-via-procedural-example-generation","title":"\u2b50 Addressing the Abstraction and Reasoning Corpus via Procedural Example Generation","text":"<p>This paper presents the re-arc repo, which allows to generate at least 10k new samples for each task in the ARC training dataset.</p> <p>Could I modify it to output text descriptions of the synthetic inputs? That could allow the model to learn a good representation of the grids and also to learn what the transformation is.</p> <p>The sample-efficiency of learning algorithms might be improved by building a curriculum that increases example difficulty over the course of training - as opposed to training on instances of the full range of difficulties throughout the entire training</p> <p>Each generator is a standalone Python function merely making use of the DSL and functions from the random module from the standard library. The median generator consists of 40 lines of code and uses 22 DSL primitive calls and 10 calls to the random module</p> <p>It is curious that so many (22) primitive function calls are needed on median.</p> <p>My rough estimation of primitive functions in arc-dsl is 160 (count the number of occurrences of <code>def</code>). We know that this set of primitives is complete for the train set, but is it for the evaluation and test set?</p>"},{"location":"03_State_of_the_art/#communicating-natural-programs-to-humans-and-machines","title":"\u2b50 Communicating Natural Programs to Humans and Machines","text":"<p>This paper is very interesting, they supplement the ARC dataset with text descriptions.</p> <p>They argue that those natural descriptions are equivalent to the input/output examples, it's just another way of expressing the same message.</p> <p>We present LARC, the Language-complete ARC: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88% of the ARC tasks. We analyze the collected instructions as \u2018natural programs\u2019, finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.</p> <p></p> <p>In this work, we adopt the Wizard-of-Oz approach by using a human as an interpreter of natural language instructions (Fig 3 top-left). We define a natural program as instructions constructed by a person that can be interpreted by another person to produce a specific output. This program is natural\u2013it can be understood by speakers of the language4 without a prior consensus\u2013but behaves as a program, in that it produces a definitive output, which can be unambiguously checked for correctness. For instance, the original ARC tasks are natural programs: Given a program consisting of input-output examples, a fellow human can readily interpret this program to produce an output on a new input, which can be checked for correctness. By starting with (linguistic) natural programs, one can directly observe the set of concepts and strategies necessary to master a domain (such as ARC), without committing to a specific interpreter.</p> <p>This is really interesting: it implies that a program defined with natural language is equivalent to a program defined with input-output samples in the ARC dataset. Thus translating an ARC task to a text description would not be necessary, although it would be helpful to verify that the task has been understood.</p> <p>However results do not show a big benefit from using the text descriptions.</p>"},{"location":"03_State_of_the_art/#on-the-paradox-of-learning-to-reason-from-data","title":"\u2b50 On the Paradox of Learning to Reason from Data","text":"<p>Logical reasoning is needed in a wide range of NLP tasks. Can a BERT model be trained end-to-end to solve logical reasoning problems presented in natural language? We attempt to answer this question in a confined problem space where there exists a set of parameters that perfectly simulates logical reasoning. We make observations that seem to contradict each other: BERT attains near-perfect accuracy on in-distribution test examples while failing to generalize to other data distributions over the exact same problem space. Our study provides an explanation for this paradox: instead of learning to emulate the correct reasoning function, BERT has in fact learned statistical features that inherently exist in logical reasoning problems. We also show that it is infeasible to jointly remove statistical features from data, illustrating the difficulty of learning to reason in general. Our result naturally extends to other neural models and unveils the fundamental difference between learning to reason and learning to achieve high performance on NLP benchmarks using statistical features.</p> <p>This paper shows that an LLM is able to achieve high performance on a NLP reasoning benchmark without learning to reason. Instead it uses statistical features and when faced with a different data distribution it does not generalize well.</p> <p>This is a very interesting result, but I have one big criticism:</p> <p>Would the results be the same if instead of just giving the solution the model would have been given the reasoning traces? I believe that to teach to reason we have to go \"step by step\".</p>"},{"location":"03_State_of_the_art/#reasoning-abilities-of-large-language-models-in-depth-analysis-on-the-abstraction-and-reasoning-corpus","title":"Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus","text":"<p>This papers studies if LLMs are good at the three elements of Language of Thought Hypothesis:</p> <ol> <li>Logical Coherence: Using prompting techniques, LLMs are set to solve ARC tasks. By analyzing the types of ARC tasks it can solve and the process of solving them, we aim to determine whether LLMs are capable of logical reasoning and whether its logic is consistent.</li> <li>Compositionality: ARC tasks are known to be solvable through the application of step-by-step functions. With such functions provided, we aim to ascertain whether LLMs can identify the combinations of functions that are needed to solve a task. This process can be broken down into two parts: understanding how LLMs manipulate the problem input and determining whether they can achieve the desired results through multiple steps of manipulation.</li> <li>Productivity: We tested if LLMs can create new input-output pairs for ARC tasks. We selected tasks with multiple inputs leading to the same output, devised prompts for inverse transformation, and assessed LLMs\u2019 ability to generate varied inputs based on these prompts.</li> </ol> <p></p> <p>It finds that current LLMs are weak at all the three elements.</p>"},{"location":"03_State_of_the_art/#large-language-models-are-not-strong-abstract-reasoners","title":"Large Language Models Are Not Strong Abstract Reasoners","text":"<p>Results on ARC challenge are very weak, but they don't add task descriptions that I believe would have helped the models (in addition to few-shot prompting)</p> <p>Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data</p> <p>We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks, even when applying techniques that have been shown to improve performance on other NLP tasks.</p> <p>In this paper, we present what is, to the best of our knowledge, the first extensive evaluation of Large Language Models for abstract reasoning. We show that LLMs do not perform well on all types of tasks, although not all models are equally poor. Prompting and refinement techniques that improve performance on NLP tasks do not work for abstract reasoning. Our experiments show that the bottleneck in the performance lies in the recognition of new unseen abstract patterns and not in a lack of understanding of the task or the prompt. These results hold in discriminative settings, where the models must find the correct answer within a small set of propositions. A qualitative study of selected failure cases in the appendix further reveals that models tend to reason inconsistently and in a shallow way. We hypothesize that current self-supervised autoregressive LLMs lack fundamental properties for strong abstract reasoning tasks and human-like cognition. We posit that methods based on causal reasoning and program induction could help improve the reasoning abilities of neural networks.</p>"},{"location":"03_State_of_the_art/#comparing-humans-gpt-4-and-gpt-4v-on-abstraction-and-reasoning-tasks","title":"Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks","text":"<p>This paper shows an evaluation of GPT4 on ConceptARC dataset, an easier version of ARC dataset that has well defined categories.</p> <p>However there isn't task description so that is equivalent to trying to learn MATH problems with just the answer and not the reasoning.</p> <p>The defining characteristic of abstract reasoning is the ability to induce a rule or pattern from limited data or experience and to apply this rule or pattern to new, unseen situations.</p> <p>Here, we performed evaluations using a more informative, one-shot prompt for text versions of tasks, and experimented with similar zero- and one-shot prompts for the multimodal case in which task-grids were given as images. We found that our more informative one-shot prompt improved GPT-4\u2019s performance in the text case, but its performance remained well below that of humans and the special-purpose Kaggle-ARC program.</p>"},{"location":"03_State_of_the_art/#large-language-models-as-general-pattern-machines","title":"Large Language Models as General Pattern Machines","text":"<p>This seems to be one of the initial evaluations of ARC using LLMs. The results are modest, but they show they can solve the problems for arbitrary symbols (not just 0-9 but any symbol)</p> <p>The capacity of LLMs to act as general pattern machines is driven by their ability to perform in-context learning on sequences of numeric or arbitrary tokens.</p> <p>Observation: consistent tokenization matters.</p> <p>Observation: token mapping invariance. The hypothesis that LLMs can serve as general pattern machines stems from the observation that they can still solve a non-trivial number of ARC problems using alphabets A sampled randomly from the LLM\u2019s token vocabulary.</p>"},{"location":"03_State_of_the_art/#do-large-language-models-solve-arc-visual-analogies-like-people-do","title":"Do Large Language Models Solve ARC Visual Analogies Like People Do?","text":"<p>They create  a very simple version of ARC challenge for kids. They compare the results between kids and LLMs.</p> <p>Not very useful for our task, we need to solve the real and difficult one.</p>"},{"location":"03_State_of_the_art/#large-language-model-as-a-system-of-multiple-expert-agents-an-approach-to-solve-the-abstraction-and-reasoning-corpus-challenge","title":"Large Language Model as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus Challenge","text":"<p>They use GPT4 to write python code to solve the tasks. Is similar to the Ryan's approach but with less computation.</p> <p>It's a bit weird that they make some proposals but do not implement all of them.</p> <p>Input tokens is a limitation because at the time of the publication GPT4 could only receive 8k tokens. In Kaggle we could have a similar limitation due to GPU RAM memory.</p> <p>While GPT-4 has proven to be a general purpose solver, being (currently) a text-based model, GPT-4 lacks some of the innate human priors necessary to solve the ARC challenge. For example, GPT-4 is not able to identify objects accurately from text alone.</p> <p>For our method, we use only three views - Grid View, Object View, Pixel View - and that has already achieved quite good results. In brief, Grid View provides the entire grid repre- sentation, except we change the pixel numbers to characters so that we do not bias GPT-4 to treat it as an arithmetic problem to perform arithmetic on the pixel values. This also has the added benefit of ensuring that GPT-4 has not seen the ARC tasks before as it is now of a different form. The Object View groups pixels that are contiguous together, so that they can be manipulated as a group. Pixel View gives the coordinates for each pixel, which can help with more fine-grained movement tasks or relational tasks between pixels.</p>"},{"location":"03_State_of_the_art/#generalized-planning-for-the-abstraction-and-reasoning-corpus","title":"Generalized Planning for the Abstraction and Reasoning Corpus","text":"<p>Another paper that uses the DSL approach, but from the perspective of planning. It achieves slightly better results than icecuber and ARGA.</p>"},{"location":"03_State_of_the_art/#tackling-the-abstraction-and-reasoning-corpus-arc-with-object-centric-models-and-the-mdl-principle","title":"Tackling the Abstraction and Reasoning Corpus (ARC) with Object-centric Models and the MDL Principle","text":"<p>This paper proposes to search a representation of the grids using the Minimun Description Length (MDL) principle. Then it searches a program to transform the input into the output. So it does not only use a DSL but also a representation. Patterns + Functions.</p> <p>It scores very poorly on private test set (2%)</p>"},{"location":"03_State_of_the_art/#a-neurodiversity-inspired-solver-for-the-abstraction-and-reasoning-corpus-using-visual-imagery-and-program-synthesis","title":"A Neurodiversity-Inspired Solver for the Abstraction and Reasoning Corpus Using Visual Imagery and Program Synthesis","text":"<p>I haven't read the whole paper but it's just another DSL with some inspiration on Neurodiversity.</p> <p>It only achieves 2% on the private test set, which is omitted in the paper and only said that they got the 4th-place.</p>"},{"location":"03_State_of_the_art/#graphs-constraints-and-search-for-the-abstraction-and-reasoning-corpus","title":"Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus","text":"<p>This paper proposes to use an object centric representation of the grids based on graphs. Using that representation and a small Domain Specific Language (DSL) it is able to solve a comparable number of tasks to icecuber solution (but using only tasks about objects).</p> <p>This representation was later used on other papers as the input to GPT.</p>"},{"location":"03_State_of_the_art/#teaching-large-language-models-to-reason-with-reinforcement-learning","title":"Teaching Large Language Models to Reason with Reinforcement Learning","text":"<p>This paper applies different RL methods to Llama 2 to improve reasoning at math problems. All RL methods have a similar result.</p> <p>They don't use ARC data at all.</p>"},{"location":"03_State_of_the_art/#can-large-language-models-learn-independent-causal-mechanisms","title":"Can Large Language Models Learn Independent Causal Mechanisms?","text":"<p>A new architecture is proposed to enhance reasoning, but the results are similar to Llama 2.</p> <p>I don't believe this is relevant to solve the ARC challenge.</p>"},{"location":"03_State_of_the_art/#learn-abstraction-in-an-abstract-way-the-long-journey-ahead","title":"Learn Abstraction in an Abstract Way: The Long Journey Ahead","text":"<p>This paper is not relevant for the task.</p>"},{"location":"03_State_of_the_art/#repos","title":"Repos","text":"<ul> <li>arc-dsl Domain Specific Language for the Abstraction and Reasoning Corpus by Michael Hodel, member of MindsAI team</li> <li>https://github.com/michaelhodel/re-arc RE-ARC: Reverse-Engineering the Abstraction and Reasoning Corpus by Michael Hodel, member of MindsAI team</li> <li>ARC Draw more samples is the repo for the article \"Getting 50% (SoTA) on ARC-AGI with GPT-4o by Ryan Greenblatt\"</li> <li>Generalized-Planning-for-the-Abstraction-and-Reasoning-Corpus</li> <li>Icecuber solution repo</li> </ul>"},{"location":"03_State_of_the_art/#videos","title":"Videos","text":"<p>I could use downsub to get subtitles from a Youtube video.</p>"},{"location":"03_State_of_the_art/#dwarkesh-patel-francois-chollet-llms-wont-lead-to-agi-1000000-prize-to-find-true-solution","title":"\u2b50 Dwarkesh Patel | Francois Chollet - LLMs won\u2019t lead to AGI - $1,000,000 Prize to find true solution","text":"<p>Each task in the ARC is novel. You cannot memorize the solution programs in advance. You have to synthesize a new program for each task.</p> <p>There are two definitions of reasoning:</p> <ol> <li>I have a set of program templates. When faced a new problem I fetch the right template and input the values to solve the problem. This is what GPT does. It needs some intelligence to fetch the correct template.</li> <li>When faced with a new puzzle there isn't a template available you have to synthesize on the fly a new program based on bits and pieces of existing programs that you have.</li> </ol>"},{"location":"03_State_of_the_art/#machine-learning-street-talk-chollets-arc-challenge-current-winners","title":"\u2b50 Machine Learning Street Talk | Chollet's ARC Challenge + Current Winners","text":"<p>Basically that we use so many training examples. It's it's not to necessarily teach it so many concepts. It's to teach it a space around the concepts and to also prevent it from kind of using the shortcuts that the models are prone to.</p> <p>Ideally we would train a model on internet data and generalize to the ARC dataset, that's what Chollet would love to see.</p> <p>So I just figured, okay, how well could we do? Uh, even just something very simple, like learning a task in isolation. If we had an unlimited number of examples for a given task.</p> <p>Probably 20 different kind of many experiments in formatting the data in various ways.</p> <p>If you train a model on the re-arc dataset you will get like 1% on the test set. But if you apply their techniques of active inference the score will increase to 23%</p> <p>There are some scaling laws that suggest that the bigger the model the less test data needs to learn</p> <p>The DSL has 160 functions, but the author believes it could rewrite it to be just 30</p> <p>Their method is:</p> <ol> <li>Fine-tune an LLM on augmented ARC tasks. Probably on the re-arc dataset, maybe a bigger version of it.</li> <li>On inference they augment the test samples (I don't know how) and fine-tune the LLM again on those tasks</li> <li>Make predictions with the LLM</li> </ol> <p>Test-Time Augmentation to solve ARC, interview with Jack Cole Almost no details about the approach.</p>"},{"location":"03_State_of_the_art/#llms-as-a-system-to-solve-the-abstraction-and-reasoning-corpus-arc-challenge","title":"LLMs as a system to solve the Abstraction and Reasoning Corpus (ARC) Challenge!","text":"<p>TODO:</p>"},{"location":"03_State_of_the_art/#conclusions","title":"Conclusions","text":"<ul> <li>Problem representation is very relevant. Ryan Greenblatt and Jack Cole mention they have worked to create   a good problem representation.</li> <li>It is likely that the MindsAI team is expanding re-arc to the evaluation dataset, or maybe to synthesize   new tasks.</li> </ul>"},{"location":"03_State_of_the_art/#definitions-of-abstraction-and-reasoning","title":"Definitions of abstraction and reasoning","text":"<p>Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data</p> <p>The defining characteristic of abstract reasoning is the ability to induce a rule or pattern from limited data or experience and to apply this rule or pattern to new, unseen situations.</p> <p>Abstraction and reasoning - developing computer systems that can learn new concepts from a small number of examples, something that humans find relatively easy</p> <p>Reasoning is a knowledge acquisition efficiency</p>"},{"location":"03_State_of_the_art/#other-links","title":"Other links","text":"<ul> <li>https://lab42.global/community-2023-july-arc-sota/</li> <li>Large Language Model Agents</li> <li>Monte Carlo - Abstract Reasoning with Graph Abstractions</li> <li>Simon ARC Lab</li> <li>Simon notes</li> </ul>"},{"location":"03_State_of_the_art/#todo","title":"TODO","text":"<ul> <li> Jack Cole approach (active inference)</li> <li> Buck approach (write python programs with GPT4o)</li> <li> Icecuber approach (DSL): https://www.kaggle.com/competitions/abstraction-and-reasoning-challenge/discussion/154597</li> <li> What is the best way to encode 2d information for an LLM like Llama3?</li> <li> How can we learn from few examples? Do we need a good representation of the data? Why ML methods need huge datasets? That is where the priors kick in, those priors influence the representation of the data.</li> <li> Search more relevant papers<ul> <li> Citations from 2024</li> <li> Papers with Abstraction and Reasoning Corpus in the title</li> <li> Read Kaggle's forum to see if more relevant papers were added</li> <li> Links from https://arcprize.org/guide</li> </ul> </li> <li> Contrastive learning. Can a model predict if two input/output pairs belong to the same task?</li> </ul>"},{"location":"05_Solution_Summary/","title":"Solution Summary","text":""},{"location":"05_Solution_Summary/#abstract","title":"Abstract","text":"<p>This paper presents the Omni-ARC approach to the 2024 Abstraction and Reasoning Challenge (ARC), focusing on data efficiency and the abstraction aspect of the challenge. The core of this solution lies in training a transformer to perform multiple tasks related to ARC problems, going beyond the challenge's primary objective of predicting outputs from input-output examples. This multi-task learning approach aims to develop a robust representation of the ARC problem space.</p> <p>Omni-ARC leverages publicly available LLMs, fine-tuning them on various ARC-related tasks, including output prediction and input distribution learning, using augmented data from multiple ARC datasets. The solution incorporates test-time fine-tuning to enhance accuracy on the private test set, achieving a score of 40 on the public leaderboard, securing 3rd place in the challenge.</p> <p>Further analysis demonstrates the potential of training models to verify output correctness and select the most probable solution from multiple options, paving the way for future research in integrating reasoning capabilities into the Omni-ARC framework.</p>"},{"location":"05_Solution_Summary/#intro","title":"Intro","text":""},{"location":"05_Solution_Summary/#abstraction-and-reasoning-challenge","title":"Abstraction and Reasoning Challenge","text":"<p>There are two different definitions of artificial intelligence. The first says that artificial intelligence is the science of creating machines that can do the tasks that humans can do. According to this definition we would be very close to artificial general intelligence (AGI) because systems like ChatGPT can do many tasks that only humans were able to do before such as solving math problems, answering all kind of questions, improving the style of some text... Many AI researchers believe that scale is all we need, and simply scaling the models and the data will lead to AGI.</p> <p>But there is also another view championed by Fran\u00e7ois Chollet that says that skill is not intelligence, that intelligence is the ability to handle novelty and learn new skills.</p> <p>The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty.</p> <p>According to this definition we are far from AGI because the accuracy of the current deep learning models on some task does not depend on the complexity of the task, but on the familiarity of the model with the task.</p> <p>To spark research into this view of intelligence Chollet created the Abstraction and Reasoning Corpus (ARC) and so far it has resisted the test of time. Whereas LLMs are saturating all kind of publicly available benchmarks, they still do very poorly on the ARC dataset. This can be explained by two big reasons:</p> <ol> <li>The test dataset is private</li> <li>All the problems in the test dataset are novel</li> </ol> <p>In 2024 Chollet joined forces with Mike Knoop to launch the ARC Prize 2024, with a total prize pool of 1M$. The goal was to rise awareness of the unbeaten ARC and to increase the number of people working to solve it.</p>"},{"location":"05_Solution_Summary/#motivation-of-my-approach","title":"Motivation of my approach","text":"<p>In the ARC challenge we have to learn a transformation rule given a few high-dimensional pairs of input and output images. The images can have a size of up to 30x30 pixels and each pixel can take 10 different colors. The images are not as complex as real world images, but nevertheless they are high dimensional data.</p>"},{"location":"05_Solution_Summary/#how-can-we-learn-from-few-high-dimensional-examples","title":"How can we learn from few high-dimensional examples?","text":"<p>To solve each ARC problem we have to find the right representation of the data. When humans solve the tasks, the biggest challenge is to find the right perspective to look at the problem. Once we have the right perspective of the data the ARC problems are trivial to solve.</p> <p>The right representation of the data allows to decrease the dimensionality of the data and makes possible to learn the transformation from very few examples.</p>"},{"location":"05_Solution_Summary/#how-can-we-learn-a-good-representation-of-the-arc-problems","title":"How can we learn a good representation of the ARC problems?","text":"<p>If we train a model to do tasks that require a good representation of the data, it's likely that the model will internally develop the required representation.</p> <p>My insight was that we could use the ARC problems in many different ways to learn that representation, not just in the original proposed task that asks to generate the output for an image given a few input-output pairs. The next section shows the different ways to use the ARC problems.</p>"},{"location":"05_Solution_Summary/#omni-arc-training-a-single-model-to-do-multiple-arc-related-tasks","title":"Omni-ARC: Training a single model to do multiple ARC-related tasks","text":"examples + input -&gt; output examples -&gt; code code + input -&gt; output inputs -&gt; input code -&gt; inputs inputs -&gt; code <ul> <li><code>examples + input -&gt; output</code>. The original task of the ARC dataset.</li> <li><code>inputs -&gt; input</code>. Generating new inputs requires to understand the distribution of the grids. It could also be done with the outputs, that should also follow some distribution.</li> <li><code>examples -&gt; code</code>. This is the approach used by Ryan Greenblat with GPT-4o, generate code to solve the task given some ARC examples.</li> <li><code>code + input -&gt; output</code>. This is equivalent to the first task, but instead of giving examples as input, it gives the code definition of the problem.</li> <li><code>code -&gt; inputs</code>. Each input to a task follows some distribution, given a description of the   distribution the model should be able to generate samples of that distribution.</li> <li><code>inputs -&gt; code</code>. We could also do the opposite task, given some inputs write code to generate that distribution.</li> <li><code>examples + input + output -&gt; is the output correct?</code>. It is possible to train the model to verify wether a proposed output is correct.</li> <li><code>examples + input + output options-&gt; select the correct output</code>. We can train a model to select the correct output between multiple options.</li> </ul> <p>All the listed tasks require that the model learns some useful representation of the ARC image. The idea behind the Omni-ARC approach is to train a single model to do all the tasks, with the expectation that a shared representation across all the tasks will generalize better than training the model to do a single task.</p> <p></p> <p>Omni-ARC, a single model that does all the ARC-related tasks (and it has a very cool logo)</p>"},{"location":"05_Solution_Summary/#prior-work","title":"Prior work","text":""},{"location":"05_Solution_Summary/#mindsai","title":"MindsAI","text":"<p>The most relevant prior work is the information given by the MindsAI team about they approach. On interviews they have told that they biggest contribution is to do test-time fine-tuning. There was little information but enough to make educated guesses and replicate their results:</p> <ul> <li>Test-Time Augmentation to solve ARC, interview with Jack Cole</li> <li>Machine Learning Street Talk | Chollet's ARC Challenge + Current Winners</li> </ul> <p>Our ARC solution stands out due to several key elements. Firstly, we fine-tune models on synthetic and augmented data. Secondly, we employ test-time fine-tuning. Lastly, we have developed an approach called AIRV (augment, inference, reverse augmentation, and vote), which is analogous to test-time augmentation. These innovations are crucial, as transformer models perform relatively poorly on ARC without them.</p> <p>I could summarize my solution as an extension of the MindsAI approach, in addition to the three points cited above my approach trains the model to perform more tasks around the ARC data. That way we can improve the data efficiency of the system and get better results for the same amount of data.</p>"},{"location":"05_Solution_Summary/#ryan-greenblatt","title":"Ryan Greenblatt","text":"<p>I recently got to 50%1 accuracy on the public test set for ARC-AGI by having GPT-4o generate a huge number of Python implementations of the transformation rule (around 8,000 per problem) and then selecting among these implementations based on correctness of the Python programs on the examples (if this is confusing, go to the next section)2. I use a variety of additional approaches and tweaks which overall substantially improve the performance of my method relative to just sampling 8,000 programs.</p> <p>The approach taken by Ryan Greenblatt was very inspiring because he didn't fine-tune any model for the ARC challenge.</p> <p>I tried to emulate his approach using open and smaller LLMs with the aim to combine it with the MindsAI approach but my efforts failed. However I believe that if I devote more work to this approach it might work.</p>"},{"location":"05_Solution_Summary/#approach","title":"Approach","text":"<p>The solution on a nutshell:</p> <ol> <li>Take <code>Qwen2.5-0.5B-Instruct</code> and fine-tune it on publicly available ARC datasets. The model was fine-tuned to:<ol> <li>generate the outputs for the test samples</li> <li>learn the inputs distribution and generate new inputs.</li> </ol> </li> <li>Do test-time fine-tuning with the private test data, only for the task of generating the test outputs.</li> <li>Inference with data augmentation, and voting to select the predictions</li> <li>Ensemble with the 2020 public solution</li> </ol> <p></p>"},{"location":"05_Solution_Summary/#training","title":"Training","text":""},{"location":"05_Solution_Summary/#data","title":"Data","text":"<p>I used the following publicly available datasets for training:</p> dataset number of unique tasks original ARC dataset 800 Michael Hodel's RE-ARC dataset 400 PQA dataset 7 Simon Strandgaard's Tama dataset 50 Mini-ARC 149 nosound's hand crafted ARC tasks 9 Andy Penrose's tasks 5 TOTAL 1420 <p>For all the datasets I trained the model to do two tasks:</p> <ul> <li><code>examples + input -&gt; output</code>. The original task of the ARC dataset.</li> <li><code>inputs -&gt; input</code>. Generating new inputs requires to understand the distribution of the grids. It could also be done with the outputs, that should also follow some distribution.</li> </ul> Click to see examples of newly generated inputs <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"05_Solution_Summary/#data-augmentation","title":"Data augmentation","text":"<p>For each problem the same data augmentation was applied to all the inputs and outputs. Data augmentation was a composition of the following augmentations:</p> <ul> <li>Rotations</li> <li>Flips</li> <li>Color changes</li> <li>Swap between train and test examples</li> </ul>"},{"location":"05_Solution_Summary/#problem-augmentation","title":"Problem augmentation","text":"<p>In addition to the data augmentation I also did problem augmentation by applying a transformation only to the inputs or to the outputs. This transformation created new ARC problems by composing the original ARC transformation with randomly chosen new ones.</p> <p>These new transformations needed to be reversible, otherwise the new generated problems might not be solvable. I used the following additional transformations:</p> <ul> <li>Rotations and/or flips</li> <li>Padding the image</li> <li>Upscale</li> <li>Mirror</li> </ul> Click to see examples of problem augmentation <p>Original task:</p> <p></p> <p>Rotate the inputs:</p> <p></p> <p>Upscale x2 the outputs:</p> <p></p> <p>Add padding to the inputs:</p> <p></p> <p>Mirroring the outputs:</p> <p></p>"},{"location":"05_Solution_Summary/#problem-representation","title":"Problem representation","text":"<p>I used a very simple text representation of the ARC grids as an input to the LLMs. The grid was enclosed on a Markdown code snippet, the shape was defined at the first line and each row was numbered.</p> <pre><code>```grid shape: 3x3\n1 100\n2 010\n3 001\n```\n</code></pre>"},{"location":"05_Solution_Summary/#training-hyperparameters","title":"Training hyperparameters","text":"<p>The model was fine-tuned using LoRA. No significative improvement was found when doing full model fine-tuning and also on test-time fine-tuning it seemed to be beneficial to just fine-tune the already trained LoRA adapter instead of creating a fresh new adapter.</p> <ul> <li>Model: <code>Qwen2.5-0.5B-Instruct</code></li> <li>LoRA rank: 128 (17M of parameters)</li> <li>Learning rate: 5e-5, with a linear schedule with warmup</li> <li>Batch size: 16</li> <li>Training steps: 2e5</li> <li>Max sequence length: 8196</li> <li>Trained on 2xA6000 GPUs</li> </ul> <p>I used huggingface's trl and accelerate libraries for the training.</p>"},{"location":"05_Solution_Summary/#test-time-fine-tuning","title":"Test-time fine-tuning","text":"<p>Fine-tuning a model on ARC tasks is not enough to do well on the private test set. By applying test-time fine-tuning we could improve the number of solved problems from 11 to 33 for one of the models that I trained along the challenge.</p> <p>This is my interpretation of the test-time fine-tuning:</p> <ul> <li>For each test problem that had <code>n</code> train samples, I fine-tuned the model using <code>n-1</code> train samples and   using the remaining sample as a test sample. The selection of the test sample was done randomly on the fly during training.</li> <li>I used data augmentation just like in the previous training</li> <li>I fine-tuned a model for each of the test problems, so 100 fine-tuned models were generated on each submission.</li> <li>I used batch size 1 in the test-time fine-tuning to be able to learn the new problems as fast as possible.</li> <li>The model was fine-tuned for ~300 steps on each problem</li> <li>Suprisingly the best learning rate for test-time fine-tuning was 8e-5, higher than the one used for   training (5e-5). It's very likely that better results could be obtained if more computation was   available, training with a slower learning rate, with higher batch size and for longer.</li> </ul> <p>Due to the limited submission time test-time fine-tuning was only applied to the canonical ARC task of predicting the test outputs. But it could also be applied to the task of generating new inputs, or to the tasks of verifying the correctness of the outputs.</p> <p>The unusual configuration of training a single model for each task with batch size 1 arose due to the limitations of compute and submission time. It was the configuration that allowed to learn faster the new test problems.</p>"},{"location":"05_Solution_Summary/#inference","title":"Inference","text":"<p>Data augmentation was applied also at inference, and the data augmentation was reverted from the prediction to get the original output. 96 predictions were done for each problem and voting was used to select the most promising predictions. So just like MindsAI's AIRV (augment, inference, reverse augmentation, and vote).</p> <p>Inference was done using a temperature of 0.</p> <p>vLLM was used to generate the predictions. Each fine-tuned model was used to generate predictions for its problem.</p>"},{"location":"05_Solution_Summary/#ensemble","title":"Ensemble","text":"<p>I ensembled my model predictions with the 2020 solution. Since the 2020 solution only requires CPU, I managed to run it on the background while I used the GPU for fine-tuning and inference with my model. I only had to be careful with the RAM usage because both jobs had to share the same memory.</p> <p>The ensemble strategy was very simple, just take the first attempt from each solution.</p>"},{"location":"05_Solution_Summary/#results","title":"Results","text":"<p>This approach scored 40 on the ARC leaderboard.</p> <p></p> <p>The same approach (without test-time fine-tuning) could solve 32% of the evaluation tasks, and when using voting with 32 predictions it achieved a top_2 accuracy of 22%. Due to limited hardware resources I didn't usually evaluate the models with test-time fine-tuning on the evaluation dataset. Kaggle provides 30 hours of GPU each week, but we could make 3 submissions a day which is equivalent to 36 hours of compute. Thus it was much cheaper to use the submissions to see the performance of the test-time fine-tuning where we had 7 times more compute available per week.</p>"},{"location":"05_Solution_Summary/#learnings","title":"Learnings","text":""},{"location":"05_Solution_Summary/#prompting-is-not-enough-test-time-fine-tuning-is-needed","title":"Prompting is not enough, test-time fine-tuning is needed","text":"<p>Clearly this competition has shown that LLMs need test-time fine-tuning to do new tasks. Few-shot prompting is not enough for the model to learn novel tasks.</p>"},{"location":"05_Solution_Summary/#its-possible-to-train-the-model-to-verify-the-correctness-of-the-tasks","title":"It's possible to train the model to verify the correctness of the tasks","text":"<p>During the last weeks of the challenge I tried to continue with the Omni-ARC approach and train the model to:</p> <ol> <li>Verify if an output is correct</li> <li>Select the correct output between two options</li> </ol> <p>The idea was that we could improve the leaderboard (LB) score if we replaced the voting selection mechanism by a more accurate one. Using trained models I generated wrong predictions for the original ARC dataset using a sampling temperature close to 1.</p> method top 1 accuracy top 2 accuracy voting 60.0% 70.0% verification 59.3% 77.4% selection 68.7% 81.0% <p>As the table above shows I was able to achieve promising results on the evaluation dataset. Those numbers are for 32 predictions.</p> <p>However I was not able to improve the LB score using this approach. My hypothesis is that the distribution of the predictions of a test-time fine-tuned model is different from the distribution of a frozen model. Thus the accuracy of voting for a test-time fine-tuned model might be much higher than the shown in the table for a frozen model.</p> <p>This verifier models could benefit from test-time fine-tuning, but I could not test the hypothesis due to the limited submission time.</p> <p>More information on Iteration 47 and Iteration 45.</p>"},{"location":"05_Solution_Summary/#solving-the-tasks-using-code-did-not-work-for-me","title":"Solving the tasks using code did not work for me","text":"<p>I also tried to expand on the Omni-ARC approach by training the model to do the additional tasks:</p> <ul> <li><code>examples -&gt; code</code>. This is the approach used by Ryan Greenblat with GPT-4o</li> <li><code>code + input -&gt; output</code>. This is equivalent to the first task, but instead of giving examples as input, it gives the code definition of the problem.</li> </ul> <p>To do so I built a small domain specific language (DSL) and recreated 285 of the ARC training tasks with python code. This was a laborious process that took around 3 weeks.</p> <p>Unfortunately the model did not generalize well. It could only solve 5% of the evaluation tasks, and those tasks were very similar to the training tasks. On the private test set a lucky submission was able to solve 1 task.</p> <p>I believe this approach has great potential, but I had to change to other approaches because the end of the challenge was close and other teams were improving in the leaderboard.</p> <p>More info on Iteration 26, Iteration 40 and Iteration 42.</p>"},{"location":"05_Solution_Summary/#the-quality-of-the-datasets-is-relevant","title":"The quality of the datasets is relevant","text":"<p>On the last weeks of the challenge I tried adding the BARC datasets to the training data. Surprisingly despite the enormous claimed number of different tasks (400k) I did not see any significative improvement either on the evaluation dataset or in the leaderboard. More information on Iteration 48. More external data.</p> <p>This is surprising because the original ARC dataset shows a clear trend when increasing the number of training tasks:</p> <p></p> <p>My guess is that the automatically generated tasks by GPT4 did not have too much novelty respect to the original ARC tasks.</p>"},{"location":"05_Solution_Summary/#the-right-model-size","title":"The right model size","text":"<p><code>Qwen2.5-0.5B</code> was the right model size for my approach and the available compute for submission.</p> <p>On a first step I tried smaller models such as <code>SmolLM2-135M</code> and <code>NanoLM-0.3B</code> but they did not achieve the same accuracy as <code>Qwen2.5-0.5B</code>. More on Iteration 46. Revisit small LLMs</p> <p>On my final attempt I also tried bigger models such as <code>Qwen2.5-1.5B</code> and <code>Qwen2.5-7B</code>. These models exhibit a higher data efficiency, they reach a smaller training loss for the same amount of training steps. The problem with these models is that they are slower to fine-tune and inference at submission. Moreover due to VRAM requirements we have to decrease the length of the training samples. It's very likely that LB score could be improved with this bigger models if better hardware and more submission time is given.</p>"},{"location":"05_Solution_Summary/#training-using-problem-augmentation-is-helpful","title":"Training using problem augmentation is helpful","text":"<p>The plot below shows the accuracy of a model on the evaluation dataset when being trained with a different amount of problem augmentation. Using problem augmentation 50% of the times seems to be the best option. Accuracy measures how many of the predictions made by the model are correct and it's a more robust and stable metric than vote_2 or pass_n (because it is computed with a bigger number of samples).</p> <p></p> <p>More info on Iteration 21. More data augmentation</p>"},{"location":"05_Solution_Summary/#learning-the-inputs-distribution-is-helpful-to-solve-arc-problems","title":"Learning the inputs distribution is helpful to solve ARC problems","text":"<p>The table below shows the results on the evaluation dataset of an experiment that tried to see if learning the inputs and/or the outputs distribution was helpful. There is a clear improvement on accuracy when learning the inputs distribution. However learning the outputs distribution does not seem to be helpful.</p> new tasks accuracy correct_pixels correct_size pass_n vote_2 - 4.61% 68.54% 87.34% 17.25% 10.73% 5k inputs 5.36% 69.40% 88.38% 19.75% 13.13% 5k inputs, 5k outputs 5.13% 68.34% 87.18% 19.75% 12.37% 2.5k inputs 2.5k outputs 4.68% 68.55% 87.66% 17.38% 11.99% <p>Iteration 22. Learning the inputs distribution</p>"},{"location":"05_Solution_Summary/#conclusion","title":"Conclusion","text":""},{"location":"05_Solution_Summary/#future-steps","title":"Future steps","text":"<p>The approach of using a transformer and test-time fine-tuning could likely keep improving and maybe solve the ARC prize if we generate enough synthetic data to densely cover the space of the ARC problems. However that kind of solution won't give us more knowledge about how to reach AGI. It might be worth pursuing that direction just to know where it can get us, but I don't feel it is interesting.</p> <p>On this year competition I have focused on abstraction, on building the best possible representation of the ARC problems. But the reasoning part was missing from my solution. When I try to solve the ARC problems I make a hypothesis of the transformation, see if it works on the train data and fix it if it doesn't. Finding the solution is typically an iterative process of trial and error.</p> <p>I believe that we can teach a model to reason, just like OpenAI is developing the new o1 models. First we will need to have a model that is able to generate code to solve the problems, otherwise we cannot verify the solution and iterate over the results. Then we will generate many reasoning traces for the training tasks and the model could learn to iteratively create a python code solution to the problems. The main obstacle that I see to this approach is that it will require a much bigger context size than the current MindsAI approach because in addition to the original task the prompt will also have the different code iterations and the outputs. So we could be talking about using up to 50k tokens instead of the current 10k tokens. That requires better hardware both for training and inference.</p> <p>Probably the best approach is the one that first tries to generate code to solve the problems and finally uses test-time fine-tuning only on the problems that the code approach could not solve. Thus I believe that the Omni-ARC approach, training a single model to do multiple ARC-related tasks, has a great change of being used in the solution that beats the 85% goal.</p> <p>Finally I'm going to buy an Omni-man funko pop figure to celebrate the prize.</p>"},{"location":"05_Solution_Summary/#links","title":"Links","text":"<ul> <li>Documentation of all the work done during the challenge</li> <li>Github repo</li> <li>Submission notebook</li> <li>Kaggle post</li> <li>TODO: create a Youtube video explaining my approach and comparing to other teams solutions.</li> <li>NotebookLM podcast</li> <li>LinkedIn profile</li> <li>Twitter profile</li> </ul>"},{"location":"05_Solution_Summary/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Veridas for providing me access to its compute cluster during all the challenge. Most of the experiments were done on Veridas cluster, using A6000 GPUs with 48GB of VRAM.</li> <li>Strong Compute for providing compute for training the last models for   the challenge. They gave me access to A100 GPUs with 80GB of VRAM, which allowed me to train bigger models.</li> <li>Qwen for training and releasing a family of very capable LLMs with   many different sizes.</li> <li>Weigths and Biases I used it to track all the experiments in a single place.   It's an amazing tool and free for individuals.</li> <li>Lambdalabs. I did some short (but expensive) experiments on the last   week of the challenge in Lambdalabs. They provide me with some free credits that partially covered   this experiments.</li> <li>ARC team. It's been a pleasure to work in this super interesting challenge for a few months. Thanks   for creating the challenge and specially to Chollet for all his wisdom and teachings.</li> <li>Family. I couldn't have done all this work without the help of my wife and the appreciation from   my children. My family followed my progress during the challenge and cheered me up when I advanced in the leaderboard.</li> </ul>"},{"location":"06_Winning_Model_Documentation/","title":"Winning model documentation","text":"<p>Winning Model Documentation Guidelines</p>"},{"location":"06_Winning_Model_Documentation/#a-model-summary","title":"A. MODEL SUMMARY","text":""},{"location":"06_Winning_Model_Documentation/#a1-background-on-youyour-team","title":"A1. Background on you/your team","text":"<ul> <li>Competition Name:</li> <li>Team Name:</li> <li>Private Leaderboard Score:</li> <li>Private Leaderboard Place:</li> <li>Name: Guillermo Barbadillo</li> <li>Location: Pamplona, SPAIN</li> <li>Email: guilllermobarbadillo@gmail.com</li> </ul>"},{"location":"06_Winning_Model_Documentation/#a2-background-on-youyour-team","title":"A2. Background on you/your team","text":""},{"location":"06_Winning_Model_Documentation/#what-is-your-academicprofessional-background","title":"What is your academic/professional background?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-have-any-prior-experience-that-helped-you-succeed-in-this-competition","title":"Did you have any prior experience that helped you succeed in this competition?","text":""},{"location":"06_Winning_Model_Documentation/#what-made-you-decide-to-enter-this-competition","title":"What made you decide to enter this competition?","text":""},{"location":"06_Winning_Model_Documentation/#how-much-time-did-you-spend-on-the-competition","title":"How much time did you spend on the competition?","text":""},{"location":"06_Winning_Model_Documentation/#if-part-of-a-team-how-did-you-decide-to-team-up","title":"If part of a team, how did you decide to team up?","text":""},{"location":"06_Winning_Model_Documentation/#if-you-competed-as-part-of-a-team-who-did-what","title":"If you competed as part of a team, who did what?","text":""},{"location":"06_Winning_Model_Documentation/#a3-summary","title":"A3. Summary","text":""},{"location":"06_Winning_Model_Documentation/#a4-features-selection-engineering","title":"A4. Features Selection / Engineering","text":""},{"location":"06_Winning_Model_Documentation/#what-were-the-most-important-features","title":"What were the most important features?","text":""},{"location":"06_Winning_Model_Documentation/#how-did-you-select-features","title":"How did you select features?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-make-any-important-feature-transformations","title":"Did you make any important feature transformations?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-interactions-between-features","title":"Did you find any interesting interactions between features?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-use-external-data-if-permitted","title":"Did you use external data? (if permitted)","text":""},{"location":"06_Winning_Model_Documentation/#a5-training-methods","title":"A5. Training Method(s)","text":""},{"location":"06_Winning_Model_Documentation/#what-training-methods-did-you-use","title":"What training methods did you use?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-ensemble-the-models","title":"Did you ensemble the models?","text":""},{"location":"06_Winning_Model_Documentation/#if-you-did-ensemble-how-did-you-weight-the-different-models","title":"If you did ensemble, how did you weight the different models?","text":""},{"location":"06_Winning_Model_Documentation/#a6-interesting-findings","title":"A6. Interesting findings","text":""},{"location":"06_Winning_Model_Documentation/#what-was-the-most-important-trick-you-used","title":"What was the most important trick you used?","text":""},{"location":"06_Winning_Model_Documentation/#what-do-you-think-set-you-apart-from-others-in-the-competition","title":"What do you think set you apart from others in the competition?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-relationships-in-the-data-that-dont-fit-in-the-sections-above","title":"Did you find any interesting relationships in the data that don't fit in the sections above?","text":""},{"location":"06_Winning_Model_Documentation/#a7-simple-features-and-methods","title":"A7. Simple Features and Methods","text":""},{"location":"06_Winning_Model_Documentation/#a8-model-execution-time","title":"A8. Model Execution Time","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-your-model","title":"How long does it take to train your model?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-using-your-model","title":"How long does it take to generate predictions using your model?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-the-simplified-model-referenced-in-section-a6","title":"How long does it take to train the simplified model (referenced in section A6)?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-from-the-simplified-model","title":"How long does it take to generate predictions from the simplified model?","text":""},{"location":"06_Winning_Model_Documentation/#a9-references","title":"A9. References","text":""},{"location":"06_Winning_Model_Documentation/#b-submission-model","title":"B. SUBMISSION MODEL","text":""},{"location":"06_Winning_Model_Documentation/#b1-all-code-data-and-your-trained-model-goes-in-a-single-archive","title":"B1. All code, data, and your trained model goes in a single archive","text":""},{"location":"06_Winning_Model_Documentation/#b2-readmemd","title":"B2. README.md","text":""},{"location":"06_Winning_Model_Documentation/#b3-configuration-files","title":"B3. Configuration files","text":""},{"location":"06_Winning_Model_Documentation/#b4-requirementstxt","title":"B4. requirements.txt","text":""},{"location":"06_Winning_Model_Documentation/#b5-directory_structuretxt","title":"B5. directory_structure.txt","text":""},{"location":"06_Winning_Model_Documentation/#b6-settingsjson","title":"B6. SETTINGS.json","text":""},{"location":"06_Winning_Model_Documentation/#b7-serialized-copy-of-the-trained-model","title":"B7. Serialized copy of the trained model","text":""},{"location":"06_Winning_Model_Documentation/#b8-entry_pointsmd","title":"B8. entry_points.md","text":""},{"location":"modeling/","title":"Modeling","text":""},{"location":"modeling/#space-of-possible-solutions","title":"Space of possible solutions","text":"<p>To solve the ARC challenge we need to do two things:</p> <ol> <li>Understand what the transformation is. This is the abstract reasoning step. We need to build a representation of the input to be able to do that:<ul> <li>Deep learning model<ul> <li>LLM</li> <li>Vision model</li> </ul> </li> <li>Python code. The grids are not as complex as real images, so we could build python code to extract    representations from the grids.</li> </ul> </li> <li>Implement the transformation. This could be done:<ul> <li>With code<ul> <li>Using python code<ul> <li>With primitive functions</li> <li>Without primitive functions</li> </ul> </li> <li>With a Domain Specific Language (DSL)</li> </ul> </li> <li>Using a model (very likely an LLM)</li> </ul> </li> </ol>"},{"location":"modeling/#icecuber-approach-dsl","title":"Icecuber approach: DSL","text":"<p>It is possible to solve the challenge given enough compute and a complete DSL. Validating a solution is cheap and fast, this approach will try a huge number of solutions until it finds the correct one.</p> <p>It skips completely the abstraction and reasoning and uses brute force to search the solution.</p> <p>Clearly this is not the path to more intelligent systems but we must notice that this approach could work and will scale well with compute.</p>"},{"location":"modeling/#mindsai-team-approach","title":"MindsAI team approach","text":"<p>Little details are given about their approach but in a nutshell is something like this:</p> <ol> <li>Fine-tune an LLM on augmented ARC tasks. Probably on the re-arc dataset, maybe a bigger version of it.</li> <li>On inference they augment the test samples (I don't know how) and fine-tune the LLM again on those tasks</li> <li>Make predictions with the LLM</li> </ol> <p>By fine-tuning an LLM on ARC tasks the LLM will likely learn a good representation of the grid. However they have said that without test time fine-tuning they will only score 1% so that step is crucial (they are currently at 39%)</p>"},{"location":"modeling/#ryan-greenblatt-approach","title":"Ryan Greenblatt approach","text":"<p>Ryan uses GPT4o so he cannot fine-tune the model. He has to rely on his default knowledge. In the other hand it can make use of the strong programming abilities and revision capabilities of the top models.</p> <p>He uses few-shot prompt with meticulous step-by-step reasoning and handcrafted better grid representations.</p>"},{"location":"modeling/#single-model-llm","title":"Single model LLM","text":"<p>Given the inputs the LLM will reason with text what the transformation is and apply it to the test sample.</p> <p>This model is capable of generating an output given:</p> <ul> <li>Task text description and input</li> <li>Input-output pairs and input</li> <li>Task text description, input-output pairs and input</li> </ul> <p>Thus it is a pretty general model, the previous capabilities will be used at evaluation, but in addition to them the model is also capable of:</p> <ul> <li>Describe the grids and answer questions about them (showcasing that it has learned a good representation of the grids)</li> <li>Describe changes and constants between pairs of grids</li> <li>Generate more inputs from some distribution</li> <li>Guess if some grid belongs to a distribution of grids</li> <li>Predict which symmetries could be applied to some task without changing the meaning (similar to the previous point)</li> </ul> <p>To be able to do abstraction from a few examples a good representation is needed.</p> <p>That same representation could be used by the LLM to generate the output.</p> <p>On evaluation the goodness of the text description could be validated against the examples, leaving one out. This could enable revisions and refining of the solution.</p> <p>This approach could be enhanced with test time fine-tuning that has been probed to work.</p> <p>Training this model will require a grid generator that also outputs text descriptions. It could be later replicated with 3d renderings in the real world.</p> <pre><code>text -&gt; image | easy\nimage -&gt; text | hard\n</code></pre> <p>This is true for real world data, it is also true for ARC? I'm not sure but we will find out. When we build the grid generator we will see how easy is to create different representations using python code.</p> <p>This is arguably the most similar model to a human person. This approach is similar to MindsAI team (little details are known). The strength of this approach is that learning all those tasks together will hopefully create a strong and general model.</p>"},{"location":"modeling/#llm-to-generate-python-code","title":"LLM to generate python code","text":"<p>Instead of doing the transformation directly with the LLM, we ask the LLM to write python code that does the transformation. In fact it could be the same model that could be able to do both tasks!</p> <p>Writing python code to do the transformation is non trivial, many times we have to deal with objects and that is not easy.</p> <p>Having a set of primitive functions could make the problem easier. Also being able to work with high level abstractions or representation of the grids could also simplify the problem.</p> <p>The goodness of this approach is that we can test the python programs easily using the examples.</p> <p>It requires a base model that is good at coding. Fine-tuning is more difficult because we need good python solutions. Maybe an iterative approach could work:</p> <ul> <li>Solve n problems</li> <li>Fine-tune on those solutions</li> <li>Solve additional m problems</li> <li>Fine-tune on those solutions</li> <li>Repeat...</li> </ul>"},{"location":"modeling/#dsl-intuition","title":"DSL + Intuition","text":"<p>This requires a complete DSL which is not trivial to build.</p> <p>The intuition module will give importance to each of the DSL functions given the examples</p> <p>To work well the intuition needs to build a good representation of the examples. Abstraction is all about learning a good representation.</p>"},{"location":"modeling/#few-shot-prompting","title":"Few-shot prompting","text":"<p>A really intelligent LLM could solve the ARC challenge using few-shot prompting. The prior knowledge would be injected in those few-shot samples. It is very likely that given text descriptions and step-by-step reasoning of the tasks would be helpful.</p> <p>This approach would require a big context window because tokenizing the examples can require a considerable window size.</p>"},{"location":"modeling/#summary","title":"Summary","text":"solution pros cons DSL - Search space grows exponentially with the number of primitives- The intelligence lies on the developer- Does not generalize to different problems- I believe it's very difficult to implement a complete DSL DSL + intuition - The search space is reduced using intuition - A DSL needs to be implemented- The DSL needs to be complete to solve the test set LLM + test time fine-tuning - So far is the state of the art - I believe this is very dependent on the data augmentation trick- Probably does not generalize to other domains where data augmentation is not possible- Needs millions of examples to train LLM to generate python code - Having a program as the output makes the method very general - I believe this is harder than creating the output by hand- Maybe a big LLM is needed to use this approach Fine-tuned LLM - The approach of using synthetic data could be scaled to other domains- It would be nice to be able to talk with the model about the tasks - Would the LLM be able to generalize to unseen tasks?- The creation of useful synthetic tasks is not trivial- How much compute would be needed to fine-tune a good model? Few-shot LLM - Very simple and elegant approach- Would allow to test which grid representations are better for LLMs - Unlikely to work with current LLMs"},{"location":"modeling/#select-modeling-technique","title":"Select modeling technique","text":"<p>I don't want to implement a DSL, so my plan is going to be:</p> <ol> <li>Few experiments with Few-Shot LLM. A quick iteration where I try with different models and problem encodings.</li> <li>Fine-tune an LLM to create a good representation of the grids by many tasks such as question answering. It will    also learn how to do transformations on the data.</li> <li>If the second step does not work as expected, I could try to generate python code.</li> </ol>"},{"location":"modeling/#generate-experimentation-design","title":"Generate experimentation design","text":""},{"location":"modeling/Iteration_01_few_shot/","title":"Iteration 1. Few-shot prompting","text":"<p>16-07-2024</p>"},{"location":"modeling/Iteration_01_few_shot/#goal","title":"Goal","text":"<p>How far can we go using few-shot prompting? What is the best way to encode the grids for an LLM?</p>"},{"location":"modeling/Iteration_01_few_shot/#motivation","title":"Motivation","text":"<p>I want to do a quick iteration where I take an LLM (f.e. phi-3) and use few-shot prompting. I will give different solved problems as input and see how well the LLM do both on the validation and the test set.</p>"},{"location":"modeling/Iteration_01_few_shot/#development","title":"Development","text":"<p>All the work has been done on this notebook.</p> <p>I have tried using Phi-3 model and few-shot prompting to solve ARC tasks. I have chosen Phi-3 because its context length of 128k tokens allows to give many ARC tasks as few-shot samples.</p>"},{"location":"modeling/Iteration_01_few_shot/#vllm","title":"VLLM","text":"<p>Using VLLM allows to use a context size of 61k with 2xT4 GPUs. If I use the transformers library directly I can only uses a context size of 4k.</p>"},{"location":"modeling/Iteration_01_few_shot/#grid-encoding","title":"Grid encoding","text":"<p>Some tasks require quite a big context. F.e. imagine a grid of 30x30 that has 4 train examples. At least we will require <code>30x30x5x2=9000</code> tokens. Thus I believe that we should try to use the encoding that uses the least amount of tokens possible. For Phi-3 that is simply to write the numbers without spaces.</p>"},{"location":"modeling/Iteration_01_few_shot/#results","title":"Results","text":""},{"location":"modeling/Iteration_01_few_shot/#zero-shot-baseline","title":"Zero-shot baseline","text":"<p>On a first step I have tried a very simple baseline where I give input grids to the assistant and the assistant replies with the output for each grid. This is done with all the train samples until we give the test input and use the response of the model as the prediction. In addition I also use data augmentations (flips and rotations) to make up to two predictions for each task. The data augmentation is also useful because sometimes the prediction of the model is invalid, so we have to make multiple predictions to have 2 valid responses.</p> train evaluation test 6.40% 2.50% 0% <p>This approach is able to solve some of the train and evaluation task, but it does not solve any of the test tasks.</p>"},{"location":"modeling/Iteration_01_few_shot/#few-shot-results","title":"Few-shot results","text":"<p>Using samples from the evaluation dataset I have evaluated the effect of using few-shot prompting. In this case I have changed the prompt style: the user shows input-output pairs to the assistant and then requests the assistant to predict the output given some input.</p> n shots accuracy correct_pixels correct_size unanswered 0 5.80% 55.10% 73.50% 17.40% 1 4.50% 44.80% 61.00% 23.60% 2 4.80% 37.70% 54.40% 29.80% 4 2.50% 22.40% 33.10% 33.10% 8 2.30% 23.10% 35.50% 36.80% <p>The results show that Phi-3 does not benefit from few-shot prompting with ARC tasks. As we give more examples the results get worse.</p>"},{"location":"modeling/Iteration_01_few_shot/#add-reasoning","title":"Add reasoning","text":"<p>I have manually described with text the transformation of some of the evaluation tasks. Then repeat the few-shot experiment but adding the reasoning before creating the grid.</p> uses reasoning accuracy correct_pixels correct_size unanswered No 2.50% 22.40% 33.10% 33.10% Yes 1% 19% 30.70% 42.50% <p>The model does not understand the puzzles. The examples and reasoning are not useful</p>"},{"location":"modeling/Iteration_01_few_shot/#different-models-zero-shot","title":"Different models, zero-shot","text":"<p>Since the best results were obtained for the 0-shot setup, I could try using different models. I can make submissions without using compute time, so I could see if some of the models is able to solve some task from the test set.</p> model test Phi-3 0 Mistral 7b 0 Llama 3 8b 1 <p>Llama 3 is able to solve one of the tasks from the test set. To better compare the models I should evaluate them on the public data, but I don't have Kaggle compute available.</p>"},{"location":"modeling/Iteration_01_few_shot/#conclusion","title":"Conclusion","text":"<p>Few-shot or zero-shot inference with current LLMs is not the way to solve the ARC challenge. The performance is very poor.</p>"},{"location":"modeling/Iteration_01_few_shot/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_01_few_shot/#todo","title":"TODO","text":"<ul> <li> What is the best way to encode the grids?</li> <li> Does using reasoning and description of the grids helps?</li> </ul>"},{"location":"modeling/Iteration_02_learn_to_count/","title":"Iteration 2. Learn to count","text":"<p>22-07-2024</p>"},{"location":"modeling/Iteration_02_learn_to_count/#goal","title":"Goal","text":"<p>Can we teach a model to learn a to count objects in a grid?</p>"},{"location":"modeling/Iteration_02_learn_to_count/#motivation","title":"Motivation","text":"<p>A good representation is crucial for abstraction. We need to teach the model the priors needed to solve the ARC challenge.</p> <p>This could be a paper titled: <code>Learning priors via Visual Question Answering to solve the ARC challenge.</code> To solve the challenge the model needs to build a representation with the core knowledge priors. This iteration will show if an LLM is able to learn a representation of a single grid. If it works I could move on to learn other concepts that need two grids.</p> <p>Recent winners from AIMO challenge have created a huge dataset to solve the challenge. Data is really important. If I'm able to create a good grid synthesizer that would be an advantage.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#development","title":"Development","text":""},{"location":"modeling/Iteration_02_learn_to_count/#priors-to-learn","title":"Priors to learn","text":""},{"location":"modeling/Iteration_02_learn_to_count/#core-knowledge-priors-from-chollets-paper","title":"Core Knowledge priors from Chollet's paper","text":"<p>Which are the priors we can learn from a single grid? I have copied the priors from Chollet's paper excluding the priors that need two grids to be learned.</p> <p>Object cohesion: Ability to parse grids into \u201cobjects\u201d based on continuity criteria including color continuity or spatial contiguity (figure 5), ability to parse grids into zones, partitions.</p> <p></p> <p>I believe there is some ambiguity regarding the diagonals, f.e. the blue object in the right image is a single object or 2 objects?</p> <p>I have been visualizing many train examples and could not find an example where the diagonal continuity was harmful. What I have find is that we might have to look at the images from the color and spatial perspectives, because they both are useful.</p> <p>Numbers and Counting priors : Many ARC tasks involve counting or sorting objects (e.g. sorting by size), comparing numbers (e.g. which shape or symbol appears the most (e.g. figure 10)? The least? The same number of times? Which is the largest object? The smallest? Which objects are the same size?). All quantities featured in ARC are smaller than approximately 10.</p> <p>Basic Geometry and Topology priors: ARC tasks feature a range of elementary geometry and topology concepts, in particular:</p> <ul> <li>Lines, rectangular shapes (regular shapes are more likely to appear than complex shapes).</li> <li>Symmetries</li> <li>Containing / being contained / being inside or outside of a perimeter.</li> </ul>"},{"location":"modeling/Iteration_02_learn_to_count/#summary-of-priors-to-learn","title":"Summary of priors to learn","text":"<ul> <li>Parse grids into objects based on continuity criteria (color continuity or spatial contiguity)</li> <li>Parse grids into zones or partitions</li> <li>Count</li> <li>Sort objects by size</li> <li>Comparing numbers (e.g. which shape or symbol appears the most (e.g. figure 10)? The least? The same number of times? Which is the largest object?)</li> <li>Recognize lines, rectangular shapes</li> <li>Symmetries</li> <li>Containing / being contained / being inside or outside of a perimeter.</li> </ul>"},{"location":"modeling/Iteration_02_learn_to_count/#questions-to-ask-the-model","title":"Questions to ask the model","text":"<ul> <li>General description, name all the objects, color, size, type of the object (square, rectangular, line...)</li> <li>Color of the background</li> <li>Number of objects</li> <li>Area/count for some color</li> <li>Is there some symmetry?</li> <li>Questions about different grids, do not just give one grid as input but give many and ask about certain grid. This will ensure that it is able to create individual representations for the grids.</li> </ul> <p>The grid creator should return some metadata that will be used later to create the questions and answers.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#learn-to-count","title":"Learn to count","text":"<p>In this iteration we are going to focus in generating data to teach a model to count. Learning to count will imply that the model has builtin the prior of objectness and also to count.</p> <p>The idea is to create a random grid generator that also generates questions and answers about the objects in the grid. The generator should generate objects of arbitrary sizes, colors and shapes. The background color should be predominantly black, but less frequently the other colors should be allowed. To make it a little bit more complex it should ask for the number of objects of certain color, and maybe for the area of a certain object.</p> <p>I have prepared a Kaggle notebook to generate the grids and the question-answer pairs.</p> <p>These are some random samples from the generated grids:</p> <p></p> <p>And this is a sample grid along with questions and answers.</p> <p></p> <pre><code>- How many objects are there in the grid? 4\n- Please list the area of all the objects in the grid, in ascending order and separated by commas 1, 1, 7, 7\n- What is the area of the biggest object in the grid? 7\n- What is the area of the smallest object in the grid? 1\n- How many objects with color 2 are there in the grid? 1\n- How many objects with color 3 are there in the grid? 2\n- How many objects with color 4 are there in the grid? 1\n</code></pre> <p>I estimate that I can generate around 1M grids in less than one hour. That is probably enough to train a good model.</p> <p>I have generated the grids following a kind of curriculum learning approach: there are more small grids (easier) than big grids (more difficult).</p>"},{"location":"modeling/Iteration_02_learn_to_count/#fine-tuning","title":"Fine-tuning","text":"<p>On a first step I could try doing some parameter efficient fine-tuning such as LoRA or DoRA. But I believe I should also try with a full fine-tuning which was done by Numina team on the AIMO competition.</p> <p>The training data should be masked so that the model only has to learn the answers to the responses. I believe the best way to do it is to frame all the questions-answers as a chat with different turns.</p> <p>Resources:</p> <ul> <li>LoRA conceptual guide</li> <li>Tutorial for full fine-tuning</li> <li>PEFT lora methods</li> <li>DoRA is supported now on PEFT</li> <li>Fine-tuning Llama 3, wandb</li> <li>My notebook where I fine-tune Llama with QLoRa for prompt recovery</li> <li>Train on completions only</li> <li>Alejandro Vaca discussing LoRA parameters</li> </ul>"},{"location":"modeling/Iteration_02_learn_to_count/#first-trainings","title":"First trainings","text":"<p>Wandb plots</p> <p>I have been able to train a Phi-3 model using LoRA with 1e5 train samples for 2 epochs. It has taken around 8 hours. The train and validation loss look very good, there is no sign of overfitting and the validation loss decreases steadily.</p> <p></p> <p>Looking at the training metrics it seems that the model is learning to count. I have to evaluate it to verify that it works correctly.</p> <p>I did some smaller experiments with the learning rate and seems that the sweet spot is around 4e-4 for Phi-3. There might be room for improvement in train speed if using cosine schedule with restarts.</p> <p>I have also done a quick fine-tuning with Llama 3 8b that took 65 minutes compared to same experiment with Phi-3 that took 25 minutes. However Llama achieved a better train and validation loss. (Maybe with the same LoRA parameters it has more capacity)</p> <p>27/07/2024. I have continued the training of Phi-3 for an additional 400k samples. It takes around 16 hours. I want to run a new training with <code>rslora</code> and higher <code>r</code> to see if having more capacity is useful.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#evaluation","title":"Evaluation","text":"<p>I have been doing first manual evaluation of the model, I have randomly used some of the validation grids and see how the model answers. It turns out that the model is not perfect, but it counts the true number of objects almost all the times.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#results","title":"Results","text":""},{"location":"modeling/Iteration_02_learn_to_count/#does-an-llm-learn-to-count","title":"Does an LLM learn to count?","text":"<p>Yes, it is possible to train an LLM to count really well (almost perfectly) by training on synthetic data. I haven't done any special with the input grids format.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#dora-and-rslora","title":"DoRA and rslora","text":"<p>I have tried adding DoRA and rslora to the training. I haven't seen a clear improvement because I didn't have an exact same experiment without them. Maybe the only thing I can say is that they don't hurt.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#the-order-of-the-questions-matter","title":"The order of the questions matter","text":"<p>The answers of the questions can be used as hints or shortcuts for the following questions. Thus the order of the questions matter. If I want to achieve a better representation I should randomize the order of the questions to force the model to be able to answer without hints or shortcuts.</p> <p>This explains why I see weird results on evaluation, I was changing the order by only showing one question.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#does-learning-to-count-improve-accuracy-at-arc-tasks","title":"Does learning to count improve accuracy at ARC tasks?","text":"<p>First quick evaluation with Phi-3 shows terrible results, predicting only black grids. I see a similar behaviour with Llama 3.1, the inference results after being fine-tuned do not have too much sense.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#llama-vs-phi","title":"Llama vs Phi","text":"<p>Training Llama is around 4 times slower than training Phi on my computer. This implies that in the same time I could train Phi on 4e5 samples or Llama on 1e5 samples. I might need to go to a more powerful hardware than 2x3090 gpus</p> <p>It might be possible that Llama could get better results than Phi, but I don't have a direct comparison due to the slow training of Llama. For the comparison to be fair I should tune the parameters of the training such as learning rate, and lora configuration (since they have different size using the same lora parameters leads to different lora capacity).</p>"},{"location":"modeling/Iteration_02_learn_to_count/#conclusion","title":"Conclusion","text":"<p>By using synthetic data we have taught LLMs to count objects on a grid. However this fine-tuned models do not make better predictions for ARC tasks, in fact they make worse predictions than the base model.</p>"},{"location":"modeling/Iteration_02_learn_to_count/#next-steps","title":"Next steps","text":"<ul> <li>Once the grid representation is learned, we need to teach the model to learn transformation between grids. Some priors are change-related and cannot be learned from a single grid</li> <li>I might have to go to a more powerful hardware, with more GPU memory.</li> </ul>"},{"location":"modeling/Iteration_02_learn_to_count/#todo","title":"TODO","text":"<ul> <li> How to evaluate the representation of the models? Phi-3, Llama3, Gemma2</li> <li> Curriculum learning might be helpful to do cause attribution</li> <li> LoRA parameters<ul> <li> Read the full huggingface documentation</li> <li> Does DoRA get better results?</li> <li> Can I measure the effect of r in LoRA? Probably I need a big r because the task is pretty new for this LLMs</li> <li> <code>use_rslora</code> is said to work better in the documentation, maybe needed for larger r values.</li> <li> See Vaca video.</li> </ul> </li> <li> ARC tasks fine-tuning, maybe using Michael Hodel augmentations. That would be a good way to see if       learning to count is useful or not. If I find that is useful then I should create other tasks.</li> <li> Llama vs Phi</li> <li> Does learning to count improve the solving of ARC tasks?</li> <li> Does the loss change if the order of the questions is randomized? I feel that from the evaluation results.</li> <li> Packing. If I can bundle smaller samples into a single sample along with longer samples I could speedup training. https://huggingface.co/docs/trl/v0.4.2/en/sft_trainer#packing-dataset-constantlengthdataset   ValueError: You passed a <code>DataCollatorForCompletionOnlyLM</code> to the SFTTrainer. This is not compatible with the <code>packing</code> argument.</li> </ul>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/","title":"Iteration 3. Fine-tune on ARC tasks","text":"<p>29-07-2024</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#goal","title":"Goal","text":"<p>Let's fine-tune an LLM on ARC tasks and see if:</p> <ol> <li>Can I learn/overfit the train and eval tasks?</li> <li>Does learning the train or eval tasks improves the accuracy of the model on the other dataset?</li> <li>Does training on train/eval tasks improves the results on test dataset?</li> <li>Does it help to start from a model that learned to count?</li> </ol>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#motivation","title":"Motivation","text":"<p>On the previous iteration I have fine-tuned a Phi-3 model to learn to count. However it seemed that the capacity to solve ARC tasks was worsened due to that fine-tuning. I still believe that learning core knowledge priors is important, but maybe we have to do a multi-step learning process: first learn the priors and second learn to solve the training tasks.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#development","title":"Development","text":""},{"location":"modeling/Iteration_03_fine-tune_on_arc/#iterable-dataset","title":"Iterable dataset","text":"<p>I have tried implementing an Iterable dataset for the train dataset, which would be more memory efficient and training will start faster. However it seems that <code>SFTTrainer</code> is not ready for it.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#train-script","title":"Train script","text":"<p>I'm going to copy and adapt the script that was used to teach the models to count. It's a little bit dirty but that will allow to start training quickly. Later I could think about refactoring a single training script.</p> <p>I'm going to apply rotations and flips to augment the train samples by x8. I also believe I could swap some of the train samples by the test sample to increase the dataset by an additionally x4 (estimated) Thus in the best case I will take the 400 train samples and get 12800.</p> <p>I have concerns about the memory usage. When training to learn to count the number of tokens was below 1k, but here it might grow to 8k.</p> <p>TODO: color swap (does it have sense?) or to remap the colors on each task</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#gpu-memory-requirements","title":"GPU memory requirements","text":"<p>With 2x24GB of gpu memory I can only fit one sample of 4096 tokens when fine-tuning Phi-3. I cannot fine-tune Llama 3, at least without quantization.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#going-to-the-cloud","title":"Going to the cloud","text":""},{"location":"modeling/Iteration_03_fine-tune_on_arc/#aws","title":"AWS","text":"<p>P5 instances have 8xH100 GPUs and P4 instances have 8xA100 GPUs. There does not seem to be an option with a smaller number of GPUs.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#google-cloud","title":"Google Cloud","text":"<p>Google cloud allows to create machines with 1 or more A100 GPUs, f.e. <code>a2-highgpu-1g</code>, <code>a2-ultragpu-1g</code>, <code>a2-highgpu-2g</code>... Ultra machines have 80GB of GPU memory, the others have 40GB.</p> <p>When it comes to H100 GPUs we have to use 8, there are no smaller options.</p> <p>I don't see any other available option in Google Cloud with 40GB or more.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#vastai","title":"Vast.ai","text":"<p>The prices here are also much better than in Google Cloud.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#lambdalabs","title":"Lambdalabs","text":"<p>After a quick comparison the prices on Lambdalabs seem to be much better than Google Cloud. So I'm probably starting here.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#veridas-cluster","title":"Veridas cluster","text":""},{"location":"modeling/Iteration_03_fine-tune_on_arc/#re-arc","title":"RE-ARC","text":"<p>I have published a notebook to generate training data in the same format as ARC tasks.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#results","title":"Results","text":"<p>Training metrics on wandb</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#can-we-overfit-to-the-train-set","title":"Can we overfit to the train set?","text":"experiment accuracy Phi-3 baseline 1.6% Phi-3 baseline dialog 6.4% Fine-tune without data augmentation 94.3% <p>We can improve the accuracy of the train set if we fine-tune on the train set. I had to disable KV cache quantization to achieve that accuracy, check section below.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#can-we-improve-eval-accuracy-if-we-fine-tune-on-the-train-set","title":"Can we improve eval accuracy if we fine-tune on the train set?","text":"experiment accuracy Phi-3 baseline 0.0% Phi-3 baseline dialog 2.5% Fine-tune with data augmentation 6.2% <p>The table shows a clear improvement after fine-tuning the model on the train data. Thus we can see that there is some generalization.</p> <p>By training on the train dataset the validation loss is decreased. Data augmentation is helpful to decrease the validation loss</p> <p>Could I try with test time fine-tuning to improve the accuracy?</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#does-it-help-to-start-from-a-model-that-learned-to-count","title":"Does it help to start from a model that learned to count?","text":"<p>Starting from the model that was taught to count is not helpful, starting loss is higher and also final. This follows the bad results observed when trying to solve arc tasks with that model. Thus it seems that doing a previous fine-tuning in count tasks is not helpful. Maybe a single stage fine-tuning could be better.</p> <p></p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#dimensions-of-the-data","title":"Dimensions of the data","text":"<p>Training with re-arc allows me to learn how the different dimensions help to generalize:</p> <ul> <li>number of tasks</li> <li>different examples per task</li> </ul> <p>The plot below shows the train and validation loss for different experiments. The validation dataset is fixed, but the train dataset is different and data augmentation is also changed.</p> <p></p> <p>This plots suggest that the number of different tasks is more important than having different examples per task. When using the re-arc dataset that has 100 different variations of the same task (<code>07_phi-3</code>) we can see that the training has a similar dynamic to using the train dataset without data augmentation: the model does not generalize to the eval dataset and the train loss decreases fast. The effect of having x100 more data is seen in the fact that it is harder to decrease the train loss and the divergence in the eval dataset is slower, but the dynamic is the same.</p> <p>In the other hand if we apply data augmentation to the re-arc dataset we see that the eval loss improves (<code>08_phi-3</code>) and decreasing the train loss is more difficult. When we apply data augmentations such as geometric transformations or color swaps we can transform the task (sometimes the task won't be changed, it depends on symmetries). This is a very strong evidence that the number of different tasks is much more important than the number of variations of the same task. Thus if I could create a task generator it would be valuable, or if I get other arc-like datasets. This has sense because the model is evaluated on new tasks, so ideally it would be trained in all different tasks.</p> <p>Training on the arc dataset reaches a lower validation loss than on the re-arc dataset. My guess is that the distribution of the samples is more similar to the evaluation. The re-arc dataset has different colors and sizes distribution.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#do-not-preserve-the-original-colors-of-the-tasks","title":"Do not preserve the original colors of the tasks","text":"<p>The plots above show the exactly same experiment with just one variation: the orange line uses color swap data augmentation in addition to using the original task colors, the green line does not preserve the original colors, applies augmentation to all the tasks.</p> <p>The difference in validation loss is dramatic. This is another strong evidence in favour of having as many different tasks as possible in training.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#training-with-re-arc-is-slower","title":"Training with re-arc is slower","text":"<p>I don't know exactly why, but training with re-arc dataset is slower than training with arc dataset. My guess is that each batch is padded to the element with the max length. I'm using a batch size of just 1, so I guess the difference in speed is simply due to the re-arc dataset having a mean prompt length higher than arc.</p> <p>It is 70% slower (145 vs 247 min for the same number of steps).</p> <p>The re-arc dataset has different distribution than the ARC dataset: different sizes and colors.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#kv-cache-quantization-is-harmful","title":"KV cache quantization is harmful!","text":"<p>I have found the reason for not being able to overfit and get good accuracies on the train set: KV cache quantization</p> model accuracy quantized accuracy not quantized 02_phi-3 60.20% 94.30% 11_phi-3 43.20% 78.60% <p>The table above shows train accuracy for models that have been trained to overfit on the train dataset. It can be shown a huge improvement when not quantizing the kv cache.</p> <p>TODO: what about regular not overfitted models?</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#submission","title":"Submission","text":"model train eval test 10_phi-3_1rearc100_2train_lr5e-5_color-swap-no-preserve_continue/checkpoint-1000 24.50% 6.13% 3% <p>We have improved the test score from 1 (with Llama 3) to 3 by submitting a fine-tuned version of Phi-3. Train accuracy is low, so I believe we should be able to improve it by increasing the model capacity or the train duration.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#conclusion","title":"Conclusion","text":"<p>On this iteration I have probed with Phi-3 that:</p> <ul> <li>I can overfit to the train set</li> <li>Fine-tuning on train set improves accuracy on the eval dataset</li> <li>Starting from a model that learned to count was not helpful</li> <li>The most important feature of the train set is to have different tasks. We have to train on the biggest number possible of different tasks.   Using data augmentations that change the meaning of the task, such as geometric transformations or color swaps are very helpful.</li> <li>The re-arc dataset has different distribution than the ARC dataset: different sizes and colors. Its utility is limited because of this difference</li> </ul> <p>We have improved the leaderboard score from 1 to 3.</p>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#next-steps","title":"Next steps","text":"<ul> <li>Could I frame the problem as a 2 player game where the first player needs to describe in text the   transformation and the second player needs to implement it given the text description and the input?</li> <li>I need more computing power</li> <li>I could study different active inference techniques on the eval dataset. F.e. n-1 train. Eval loss should be a good proxy to see if the different techniques are useful</li> <li>smollm</li> <li>The number of different tasks is the more important factor during training. Thus downloading ARC like datasets   or creating a task synthesizer would be valuable. Maybe the MindsAI team knows this and is simply   working to implement new tasks, train a model on them and use test time inference. This hypothesis   seems very plausible to me: they would have the advantage of using more data and the new test inference   technique.</li> <li>I would like to make submissions with the fine-tuned models</li> <li>Does predicting the grid shape helps? Previosly to predict the grid print the shape. Maybe also on the input and output pairs. I have the intuition that this will help. Previously to do this I should refactor the code  to enable easy experimentation with different prompts.</li> <li>What if I add new symbols to the tokenizer to represent the grids: &lt;0&gt;, &lt;1&gt;...</li> </ul>"},{"location":"modeling/Iteration_03_fine-tune_on_arc/#todo","title":"TODO","text":"<ul> <li> Evaluate fine-tuned model on arc tasks</li> <li> Prepare hodel data</li> <li> Try again with the iterable dataset: https://huggingface.co/docs/trl/en/sft_trainer#trl.trainer.ConstantLengthDataset</li> <li> What if I first fine-tune with augmentation and then without augmentation</li> <li> Maybe not preserving the original color space creates a more challenging train dataset that results on better generalization.</li> <li> Improve evaluation notebook<ul> <li> Free gpu memory after run</li> <li> Wait for free gpu</li> <li> Better configuration</li> <li> Verify that training and evaluation is the same. They are the exact same prompt.</li> <li> Should I refactor the way to create train and validation samples?</li> </ul> </li> <li> Is there a relation between train loss and accuracy?</li> <li> Make a submission with a fine-tuned model, to do this I should create a new notebook.<ul> <li> How to handle code and data in Kaggle</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/","title":"Iteration 4. Test time fine-tuning","text":"<p>2-8-2024</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#goal","title":"Goal","text":"<p>Explore different ways to do test time fine-tuning and see if we can improve the results on the evaluation dataset.</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#motivation","title":"Motivation","text":"<p>So far I have seen that I can fine-tune an LLM such as Phi-3 on ARC train tasks and improve the accuracy of the model on the evaluation dataset. On an interview the MindsAI team said the following:</p> <p>If you train a model on the re-arc dataset you will get like 1% on the test set. But if you apply their techniques of active inference the score will increase to 23%</p> <p>Thus although I know that there is room for improvement in the part of training a model in ARC like tasks, I believe I should investigate first on test time fine-tuning techniques because they seem to be very relevant for this challenge.</p> <p>Thus my idea is to explore different ways to use the development dataset (without the test outputs) and see if I can improve the validation loss.</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#development","title":"Development","text":""},{"location":"modeling/Iteration_04_test_time_fine-tuning/#results","title":"Results","text":"<p>Training metrics in wandb</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#first-results","title":"First results","text":"experiment val loss val accuracy baseline 0.1445 6.50% train with n-1 samples 0.1398 7.20% add geometric augmentations 0.1107 TODO add color swap 0.099 16.80% preserve original color when using color swap - - train with n-2 samples - - <ul> <li>Test-time fine-tuning has clearly improved the validation loss and accuracy of the baseline model.</li> <li>One surprising finding is that using geometric and color augmentations is helpful. I would have said that training with the exact same data would have been better. F.e. preserving the original color when using color swap did not brought improvements.</li> <li>Training with n-2 samples did not improve over training with n-1 samples. This matched my intuition because we are not increasing the number of tasks, we are just removing input information.</li> <li>Best trainings are taking in the order of 20h in my PC, which is better than Kaggle hardware. This should be at least 4 times faster to be able to use it at submission. Maybe I can try to increase the learning rate to see if the training is faster.</li> </ul>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#trying-to-speedup-fine-tuning","title":"Trying to speedup fine-tuning","text":"<p>I have tried lowering the batch size and increasing the learning rate to see if I could fine-tune in less steps. I haven't been succesfull, but it seems that lowering the batch size worsens the validation loss.</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#test-time-fine-tuning-on-kaggle","title":"Test time fine-tuning on Kaggle","text":"Model parameters (M) max context length (k) seq_len 10 steps time (min) 1000 steps time (hours) Phi-3 (3.8B) 3800 128 1024 16 26.7 Phi-3 int4 (3.8B) 3800 128 4096 34 56.7 <p>I had to quantize Phi-3 to int4 to be able to fine-tune with a context length of 4096. However the hardware is slow and it would require 56 hours to fine-tune for 1k steps. Thus I cannot do test time fine-tuning with Phi-3 on Kaggle, I should find another model.</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#conclusion","title":"Conclusion","text":"<p>By using test time fine-tuning I have been able to improve eval accuracy from 6.5% to 16.8%. However I have found that I cannot do test time fine-tuning on Kaggle with Phi-3, it would take 56 hours and I only have 12.</p>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#next-steps","title":"Next steps","text":"<ul> <li>Find another smaller model that can do test time fine-tuning on Kaggle.</li> </ul>"},{"location":"modeling/Iteration_04_test_time_fine-tuning/#todo","title":"TODO","text":"<ul> <li> Can I think of additional augmentations? So far I cannot think of other general augmentations such as geometric transforms or color swap.</li> <li> Can I speedup training?</li> <li> Can I do test-time fine-tuning on Kaggle with Phi-3? I have doubts about memory requirements. Create a notebook to validate the idea.</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/","title":"Iteration 5. Search for smaller LLMs","text":"<p>09-08-2024</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#goal","title":"Goal","text":"<p>Search an LLM that is smaller than Phi-3 and is fast enough to be fine-tuned on Kaggle within the submission time (12 hours)</p> <p>Can I get a better result than Phi-3 by using a smaller model with test time fine-tuning?</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#motivation","title":"Motivation","text":"<p>In the previous iteration I have seen that test time fine-tuning works on the eval set. However when doing a proof of concept on Kaggle's hardware I have found that to be able to fine-tune Phi-3 with a sequence length of 4096 first I have to use int4 quantization and second the fine-tuning would take 56 hours for 1k steps.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#development","title":"Development","text":""},{"location":"modeling/Iteration_05_search_for_smaller_llms/#candidate-search","title":"Candidate search","text":"<ul> <li>I could try with previous versions of Phi-3 that I believe were smaller.</li> <li>I have also read recently about smollm</li> <li>MobileLLM is cited by smollm, seems to be developed by Facebook but the models do not seem to be available.</li> </ul> <p>However Smollm models have only 2k context length.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#results","title":"Results","text":"<ul> <li>Fine-tuning on ARC tasks</li> <li>Test-time fine-tuning</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#results-summary","title":"Results summary","text":"model parameters (B) eval accuracy ttft eval accuracy test accuracy ttft test accuracy Phi-3 3.8 6.50% 16.80% 3.00% - Qwen2-1.5B-Instruct 1.5 7.10% 15.90% 4.00% *4.00% Qwen2-0.5B-Instruct 0.5 4.60% 12.40% 0.00% *5.00% <p>We have been able to improve accuracy on the test set by using Qwen2 models instead of Phi-3. I'm not sure if test-time fine-tuning is correctly implemented on Kaggle, there were some weird results. I have to unify the code to verify that everything is correct.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#test-time-fine-tuning-speed-on-kaggle","title":"Test time fine-tuning speed on Kaggle","text":"Model parameters (M) max context length (k) seq_len 10 steps time (min) 1000 steps time (hours) Phi-3 (3.8B) 3800 128 1024 3 5.0 Phi-3 int4 (3.8B) 3800 128 4096 30 50.0 Qwen/Qwen2-1.5B-Instruct 1500 32 3584 2 3.3 Qwen/Qwen2-0.5B-Instruct 500 32 4096 1 1.7 <p>So far Phi-3 and Qwen2 are the only small LLMs that I have found that have enough context lenght to deal with ARC tasks. However Phi-3 requires too much memory for fine-tuning and if I use int4 quantization it is very slow.</p> <p>The option of using Qwen2 seems to be the best one given the current hardware limitations.</p> <p>This inference times are measured when using <code>float16</code> type, I have found that it is 4 times faster than <code>bfloat16</code> on the T4 GPU. The next section shows the same measurements with more models and <code>bfloat16</code> for reference.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#slower-previous-results-with-bfloat16","title":"Slower previous results with <code>bfloat16</code>","text":"Model parameters (M) max context length (k) seq_len 10 steps time (min) 1000 steps time (hours) Phi-3 (3.8B) 3800 128 1024 16 26.7 Phi-3 int4 (3.8B) 3800 128 4096 34 56.7 Phi-2 (2.7B) int4 2700 2 4096 15 25.0 Phi-1.5 (1.3B) 1300 2 3072 7 11.7 Phi-1.5 (1.3B) int4 1300 2 4096 7 11.7 SmolLM-1.7B-Instruct 1700 2 2048 11 18.3 SmolLM-1.7B-Instruct int4 1700 2 4096 15.5 25.8 SmolLM-360M-Instruct 360 2 4096 3.75 6.3 SmolLM-135M-Instruct 135 2 4096 1.5 2.5 Qwen/Qwen2-0.5B-Instruct 500 32 3072 4 6.7 <ul> <li>To be able to run the code on Kaggle I have a limit of 12 hours. The table above shows that many of the tried models cannot be fine-tuned as fast as necessary to be able to make a submission.</li> <li><code>SmolLM</code> models only have a context length of 2k tokens, that is not enough for the ARC challenge</li> <li><code>Qwen2-0.5B-Instruct</code> is a good candidate because it has a context length of 32 and I should be able to train in 7h which is enough to make a submission.</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#role-of-the-rank-of-lora","title":"Role of the rank of LoRA","text":"<p>I have made experiments with different values for the rank of LoRA. It has a clear effect on the train loss: when we use a higher rank we get a lower train loss. However the effect on the validation loss is not clear, it seems that using a very small rank such as 4 is harmful but other than that the differences do not seem to be significative.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#kaggle-runtimes-for-reference","title":"Kaggle runtimes for reference","text":"<ul> <li><code>qwen2-0.5b-instruct</code> test set evaluation: 1h15</li> <li><code>qwen2-0.5b-instruct</code> 1k steps fine-tuning: 2h19</li> <li><code>qwen2-1.5b-instruct</code> test set evaluation: 1h45</li> <li><code>qwen2-1.5b-instruct</code> 1k steps fine-tuning: ~5h</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#2-stage-test-time-fine-tuning","title":"2 stage test-time fine-tuning","text":"<p>I had the intuition that maybe doing the test-time fine-tuning in two stages could improve the validation loss: a first one with data augmentation and a second one without data augmentation.</p> <p>However I have tried with a wide range of learning rates for fine-tuning and no consistent improvement was seen.</p> <p></p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#float16-vs-bfloat16","title":"float16 vs bfloat16","text":"<p>I have already seen that in Kaggle fine-tuning with float16 is 4 times faster than with bfloat16. Let's see if that is the case for my computer (2x3090):</p> <ul> <li><code>bfloat16</code> 50 steps fine-tuning: 2m45, 2m48</li> <li><code>float16</code> 50 steps fine-tuning: 2m43, 2m46</li> </ul> <p>Memory use is the same and speed is the same. Maybe it is using <code>bfloat16</code> internally all the time, I remember listening something like that long ago.</p> <p>I have also run a quick experiment with 2 samples per GPU but there was no speedup.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#qwen2-05b-instruct-vs-qwen2-15b-instruct","title":"Qwen2-0.5B-Instruct vs Qwen2-1.5B-Instruct","text":"<p>The bigger 1.5B parameters model learned faster on the train set and generalized better to the eval set.</p> <p>Would this continue to scale with bigger models? That would be interesting to see, f.e. fine-tuning GPT4-o. Given the hardware constraints of the challenge I cannot afford using much bigger models than this.</p> <p>For reference Qwen2-1.5B with LoRA r32 uses 8.5M of parameters, while Qwen2-0.5B with r128 uses ~17M of parameters. So the number of trainable parameters is not the difference betweeen trainings. Simply the bigger model seems to have more prior knowledge about the task.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#optimal-number-of-steps-for-test-time-fine-tuning","title":"Optimal number of steps for test-time fine-tuning","text":"<p>There are no clear signs of overfitting, but it seems to be around 3k steps when using the whole eval set.</p>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#conclusion","title":"Conclusion","text":"<p><code>Qwen2-1.5B-Instruct</code> and <code>Qwen2-0.5B-Instruct</code> are two replacement for <code>Phi-3</code> that are possible to be test-time fine-tuned on Kaggle.</p> <p>We have improved the test leaderboard score from 3 to 5 on this iteration.</p> <ul> <li>I have found that <code>float16</code> is 4 times faster than <code>bfloat16</code> when fine-tuning on Kaggle</li> <li>The bigger qwen2-1.5b model learns faster than the 0.5b version</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#next-steps","title":"Next steps","text":"<ul> <li>Decomposing the task into step by step could be helpful. That is, creating multiple grids until creating the final output.</li> <li> What is the effect of changing the train data? Keep the same train configuration and just change the data.     - [ ] How the test accuracy changes if I train in the eval set?     - [ ] How the test accuracy changes if I train in both the train and eval set?     - [ ] What if I use the train dataset and n-1 eval dataset?     - [ ] What if I only do test-time fine-tuning?     - [ ] Is it helpful to first train with re-arc?     - [ ] Use other arc-like datasets for training</li> <li> Could I speedup Qwen2 training by using a single gpu?</li> <li> Maybe I have to start using just 100 eval tasks for validation, that way my test-time fine-tuning experiments will match the Kaggle setup. Also I would have an additional 300 tasks for training.</li> </ul>"},{"location":"modeling/Iteration_05_search_for_smaller_llms/#todo","title":"TODO","text":"<ul> <li> Can I run Smollm models with VLLM? The architecture is supported... https://docs.vllm.ai/en/latest/models/supported_models.html</li> <li> Long context fine-tuning https://huggingface.co/blog/wenbopan/long-context-fine-tuning</li> <li> What is the role of LoRA <code>r</code>?</li> <li> Read about previous editions of the ARC challenges to see if any other data augmentation is used.</li> <li> Make a submission with test fine-tuning</li> <li> Remember that the test is 4 times smaller than the eval set</li> <li> 2 stage test-time fine-tuning</li> <li> Bfloat vs float</li> <li> Qwen/Qwen2-1.5B-Instruct</li> <li> What is the number of steps in the test-time fine-tuning where overfitting starts to happen? On previous experiments it seemed to be above 5k steps</li> <li> Document Kaggle training and inference speeds.</li> <li> Why experiments on Kaggle are getting worse training losses?</li> <li> Document results and close iteration</li> </ul>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/","title":"Iteration 6. Ensemble with 2020 solution","text":"<p>14-08-2024</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#goal","title":"Goal","text":"<p>There is a public notebook that scores 26 just using program search. If I can ensemble that with my current LLM approach there is a good chance to achieve a higher score (hopefully the LLM and program search will be very orthogonal)</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#motivation","title":"Motivation","text":"<p>Using LLMs and test-time fine-tuning I'm currently scoring 5 on the leaderboard. However there is a public notebook that scores 26 and runs in 3-4 hours.</p> <p>There is a good opportunity to create an ensemble. And if I can improve the accuracy of the LLMs the accuracy of the ensemble will also likely go up.</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#development","title":"Development","text":""},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#explaining-the-notebook","title":"Explaining the notebook","text":"<p>I will write my preliminary understandings of what the notebook is doing (they might change over time when I become more familiar with the code.)</p> <ul> <li>As far as I understand the basis of the solution is icecuber solution from 2020</li> <li>On top of that they try specialized approaches that might work on some tasks</li> <li>In my opinion it would be more readable if I compute the solution for icecuber with one script, solution with other approaches in other script, and have another script to combine the solutions. And that is what I have done.</li> <li>It seems that icecuber sometimes leaves empty answers <code>[]</code>, it happened 8 times on the train set, always on the 2\u00ba attempt. Sometimes it generates more than 2 answers. This might be exploited in the future if I can train a model   to predict what is the correct answer given a series of options. Maybe simply the logloss could be enough.</li> </ul>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#results","title":"Results","text":""},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#first-attempt-is-clearly-better-than-second-for-both-approaches","title":"First attempt is clearly better than second for both approaches","text":"<p>The results below show a big drop in accuracy and precision when using the second attempt instead of the first.</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#icecuber-approach","title":"Icecuber approach","text":"<pre><code># training\nattempt_1 recall: 100.0% precision: 46.2% accuracy: 46.2%\nattempt_2 recall: 93.5% precision: 6.2% accuracy: 5.8%\n# evaluation\nattempt_1 recall: 100.0% precision: 34.4% accuracy: 34.4%\nattempt_2 recall: 96.9% precision: 4.9% accuracy: 4.8%\n</code></pre>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#multiple-solvers-approach","title":"Multiple solvers approach","text":"<pre><code># training\nattempt_1 recall: 16.3% precision: 57.4% accuracy: 9.4%\nattempt_2 recall: 11.5% precision: 6.2% accuracy: 0.7%\n# evaluation\nattempt_1 recall: 8.6% precision: 72.2% accuracy: 6.2%\nattempt_2 recall: 6.4% precision: 0.0% accuracy: 0.0%\n</code></pre>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#give-preference-to-multiple-solvers-approach","title":"Give preference to multiple solvers approach","text":"<p>As it is shown in the previous section the multiple solvers approach has higher precision on the first attempt, so it's better to give preference to that approach. Next results show that we achieve higher accuracy on the first attempt by doing that.</p> <pre><code># Training results\n## preference to multiple solvers\nattempt_1 recall: 100.0% precision: 46.4% accuracy: 46.4%\nattempt_2 recall: 96.4% precision: 8.5% accuracy: 8.2%\n## preference to icecuber\nattempt_1 recall: 100.0% precision: 46.2% accuracy: 46.2%\nattempt_2 recall: 96.4% precision: 8.7% accuracy: 8.4%\n\n# Evaluation results\n## preference to multiple solvers\nattempt_1 recall: 100.0% precision: 34.6% accuracy: 34.6%\nattempt_2 recall: 97.6% precision: 5.9% accuracy: 5.7%\n## preference to icecuber\nattempt_1 recall: 100.0% precision: 34.4% accuracy: 34.4%\nattempt_2 recall: 97.6% precision: 6.1% accuracy: 6.0%\n</code></pre> <p>Since we want to ensemble this solution with my LLM approach I should give preference to the multiple solvers approach to have the best accuracy on the first attempt.</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#submissions","title":"Submissions","text":"experiment test score qwen2-1.5b-instruct v1 4 2020 solution single attempt 24 qwen2-1.5b-instruct v1 with 2020 24 2020 solution 26 <p>Our current LLMs are not able to improve over the baseline 2020 solution. We need more accurate models to be able to create a successful ensemble.</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#conclusion","title":"Conclusion","text":"<p>I have developed the code to create an ensemble, but I need more accurate models to improve over the baseline.</p> <p>Making a submission with 2020 solution is slow, takes around 7 hours. Ideally this should run in the background.</p>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#next-steps","title":"Next steps","text":"<ul> <li>Icecuber solution is slow, ideally I would run it on the background while i do all the LLM stuff.</li> <li>I might use the LLM to choose which attempts to submit, f.e. measuring the log loss of each solution. Ideally the correct solution should have lower perplexity.</li> </ul>"},{"location":"modeling/Iteration_06_ensemble_with_2020_solutions/#todo","title":"TODO","text":"<ul> <li> Refactor 2020 solution code and add arguments, there should only be a source of truth for the paths</li> <li> Understand how the solution is generated and use that to my advantage when ensembling</li> <li> Evaluate the solutions for train and evaluation</li> <li> Better analysis of the generated solutions and see what is the best way to ensemble</li> </ul>"},{"location":"modeling/Iteration_07_training_data/","title":"Iteration 7. Training data","text":"<p>15-08-2024</p>"},{"location":"modeling/Iteration_07_training_data/#goal","title":"Goal","text":"<p>Try different configurations of training data and see how the validation loss is affected.</p>"},{"location":"modeling/Iteration_07_training_data/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_07_training_data/#development","title":"Development","text":""},{"location":"modeling/Iteration_07_training_data/#create-new-train-val-partition","title":"Create new train-val partition","text":"<p>On the notebook 005_create_new_data_partititions I have prepared a function that given a random seed creates a new train-val partition. It will take 100 random eval tasks for validation and the rest for training.</p>"},{"location":"modeling/Iteration_07_training_data/#prepare-arc-like-datasets-for-training","title":"Prepare arc-like datasets for training","text":"<p>There are some arc-like datasets that could be useful for fine-tuning an LLM. They are listed here</p>"},{"location":"modeling/Iteration_07_training_data/#results","title":"Results","text":""},{"location":"modeling/Iteration_07_training_data/#train-loss-is-reproducible-val-loss-isnt","title":"Train loss is reproducible, val loss isn't","text":"<p>If I run the same experiment multiple times I get very similar train loss, but validation loss could be different. This makes harder to observe improvements.</p> <p></p> <p>What are the sources of randomness?</p> <ul> <li>Data augmentation</li> <li>LoRA initialization</li> </ul> <p>Maybe cyclic learning rate schedule might allow to escape from poor local optima, but the problem might be just a generalization one (because training loss is good).</p>"},{"location":"modeling/Iteration_07_training_data/#what-are-the-best-datasets-for-fine-tuning","title":"What are the best datasets for fine-tuning?","text":"dataset tasks tasks variations best val loss best val step combo-v2 (new train + val n-1) 800 0.1145 6000 new train 700 0.1406 6000 combo-v1 (new train + ConceptARC + Mini-ARC) 0 0.1551 6000 val n-1 100 0.164 1600 train 400 0.1831 3100 RE-ARC 400 40000 0.25 1100 ConceptARC 176 528 0.2784 400 Mini-ARC 149 149 0.3 100 1D-ARC 901 901 0.34 100 <ul> <li>Although we have seen that validation losses are not reproducible we can see a big improvement   when using the new train set compared to the old one.</li> <li>External datasets do not get good validation losses and quickly overfit.</li> <li>Making a combo with new train and val n-1 is the best option.</li> </ul>"},{"location":"modeling/Iteration_07_training_data/#is-it-helpful-to-first-train-on-re-arc","title":"Is it helpful to first train on RE-ARC?","text":"<p>It is unclear that training in two stages (first with RE-ARC and second with ARC) is positive. Training loss is lower as expected because initialization is better, but validation loss ends up being worse. However we have seen that validation loss is volatile, so in other experiment might be better.</p>"},{"location":"modeling/Iteration_07_training_data/#what-is-the-best-strategy-for-test-time-fine-tuning","title":"What is the best strategy for test-time fine-tuning?","text":"<p>If we use a constant learning rate with warmup the model overfits. The step of overfit changes with the learning rate. A lower learning rate will start overfitting later.</p> <p></p> <p>By using a linear schedule we can get even better results at 1k steps.</p> <p></p> <ul> <li>I have also tried using a bigger batch size but did not improve.</li> <li>I have also tried using a combination of train and test data but did not improve either.</li> </ul> <p>Thus a linear schedule with learning rate 1e-5 and 1k steps seems to be a good option.</p>"},{"location":"modeling/Iteration_07_training_data/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_07_training_data/#next-steps","title":"Next steps","text":"<ul> <li> Unify training scripts</li> <li> Iterable for data augmentation will be much better</li> <li> Load the state of the optimizer when fine-tuning in multiple stages? https://chatgpt.com/c/ce6a4f9c-7a50-4c19-a6f3-83793fe6a11d</li> <li> There might be room for improvement if using a generator instead of a fixed dataset. F.e. better use of RE-ARC dataset</li> <li> I need much better results when doing test-time fine-tuning. I could iterate faster if I focus on a single example or a few examples.</li> <li> Try using beam search, does it improve the results? https://docs.vllm.ai/en/latest/dev/sampling_params.html</li> <li> It seems we are hitting a ceiling, probably we need a better data formatting or more data</li> </ul>"},{"location":"modeling/Iteration_07_training_data/#todo","title":"TODO","text":"<ul> <li> What is the effect of changing the train data? Keep the same train configuration and just change the data.<ul> <li> What if I use the train dataset and n-1 eval dataset?</li> <li> What if I only do test-time fine-tuning?</li> <li> Is it helpful to first train with re-arc?</li> <li> Use other arc-like datasets for training</li> <li> What is the effect of using more training data? Can I estimate how the loss will decrease if generating more tasks?</li> </ul> </li> <li> Check for more datasets on: https://docs.google.com/spreadsheets/d/1fR4cgjY1kNKN_dxiidBQbyT6Gv7_Ko7daKOjlYojwTY/edit?gid=658867951#gid=658867951</li> <li> If I can easily swap train and test on fine-tuning, don't do it when creating the n-1 dataset. That will make configuration easier.</li> <li> Why training with combo-v2 diverges? Try using batch size 32 instead of 16, other option is gradient clipping. Bfloat16 vs float16, explained on next iteration.</li> <li> Train 1.5B model with the best configuration</li> </ul>"},{"location":"modeling/Iteration_08_code_improvements/","title":"Iteration 8. Code improvements","text":"<p>17-08-2024</p>"},{"location":"modeling/Iteration_08_code_improvements/#goal","title":"Goal","text":"<p>Improve the code so I can experiment faster in the future.</p>"},{"location":"modeling/Iteration_08_code_improvements/#motivation","title":"Motivation","text":"<p>I have rushed on the previous weeks and now I have to stop, unify and improve the code before trying new approaches.</p>"},{"location":"modeling/Iteration_08_code_improvements/#development","title":"Development","text":""},{"location":"modeling/Iteration_08_code_improvements/#iterable-dataset","title":"Iterable dataset","text":"<p>Using an iterable dataset will give more control over the data generation. It will also speedup the start of the training, because currently all the data augmentation is done previously. Additionally RAM usage will be lower.</p> <p>Possibly useful links:</p> <ul> <li>https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable#creating-map-style-datasets-and-iterable-datasets</li> <li>https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable#eager-data-processing-and-lazy-data-processing</li> <li>https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support</li> <li>https://huggingface.co/docs/trl/en/sft_trainer#datasets</li> <li>Feature: IterableDataset support for SFTTrainer #1695</li> </ul> <p>In the SFTTrainer we smartly support datasets.IterableDataset in addition to other style datasets. This is useful if you are using large corpora that you do not want to save all to disk. The data will be tokenized and processed on the fly, even when packing is enabled.</p>"},{"location":"modeling/Iteration_08_code_improvements/#experiments","title":"Experiments","text":"<pre><code>train_dataset = train_dataset.to_iterable_dataset()\nTraceback (most recent call last):\n  File \"/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/fine-tuning.py\", line 754, in &lt;module&gt;\n    trainer = SFTTrainer(\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 373, in __init__\n    train_dataset = self._prepare_dataset(\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 519, in _prepare_dataset\n    return self._prepare_non_packed_dataloader(\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 587, in _prepare_non_packed_dataloader\n    tokenized_dataset = dataset.map(\nTypeError: IterableDataset.map() got an unexpected keyword argument 'num_proc'\n</code></pre> <p>This is the same error as shown in Feature: IterableDataset support for SFTTrainer #1695</p> <p>Apparently it is solved if I install directly from github:</p> <pre><code>pip install git+https://github.com/huggingface/trl.git\n\nifferent run name by setting the `TrainingArguments.run_name` parameter.\n  0%|                                                                                                                   | 0/100 [00:00&lt;?, ?it/s]Detected flash_attn version: 2.6.3\n  5%|\u2588\u2588\u2588\u2588\u2588\u258e                                                                                                     | 5/100 [00:18&lt;05:26,  3.44s/it]Traceback (most recent call last):\n  File \"/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/fine-tuning.py\", line 773, in &lt;module&gt;\n    trainer.train()\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 444, in train\n    output = super().train(*args, **kwargs)\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in train\n    return inner_training_loop(\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/transformers/trainer.py\", line 2236, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/accelerate/data_loader.py\", line 699, in __iter__\n    raise ValueError(\nValueError: Batch does not contain any data (`None`). At the end of all iterable data available before expected stop iteration.\nTraceback (most recent call last):\n  File \"/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/fine-tuning.py\", line 773, in &lt;module&gt;\n    trainer.train()\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 444, in train\n    output = super().train(*args, **kwargs)\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in train\n    return inner_training_loop(\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/transformers/trainer.py\", line 2236, in _inner_training_loop\n    for step, inputs in enumerate(epoch_iterator):\n  File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/accelerate/data_loader.py\", line 699, in __iter__\n    raise ValueError(\nValueError: Batch does not contain any data (`None`). At the end of all iterable data available before expected stop iteration.\n</code></pre> <p>It seems that the iterator finished, let's try to create an infinite dataset.</p> <pre><code>def my_generator(dataset):\n    while True:\n        for item in iter(dataset):\n            yield item\ntrain_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"dataset\": train_dataset})\n</code></pre> <p>This seems to be working!</p>"},{"location":"modeling/Iteration_08_code_improvements/#enabling-data-augmentation-on-a-generator","title":"Enabling data augmentation on a generator","text":"<p>Now that I'm able to train with an IterableDataset, I have to create a generator that uses data augmentation.</p> <p>I have made a first implementation and seems to be working at a smaller scale. I need to verify with a long train.</p>"},{"location":"modeling/Iteration_08_code_improvements/#divergence-in-trainings-caused-by-float16","title":"Divergence in trainings caused by float16","text":"<p>After experiment <code>12_combo-v2_Qwen2-0.5B-Instruct_lr1e-4_r32_12e3steps</code> on <code>August 17th, 2024 at 8:29:10 pm</code> I have experienced divergence when training with <code>combo-v2</code> or <code>train_rs7</code>. After 60-120 minutes the training suddenly diverges.</p> <p>I have tried without success:</p> <ul> <li>Increasing the batch size from 16 to 32</li> <li>Decreasing <code>max_grad_norm</code> from default 1.0 to 0.1</li> <li>Changing the optimizer from <code>paged_adamw_8bit</code> to <code>adamw_torch</code></li> <li>It also happens with the old-train dataset</li> </ul> <p>Currently I'm trying to train on the canonical train dataset to see if the problem arises.</p> <p>\u00bfMaybe it is related to updating the library? But the problem started yesterday when the library was not updated.</p> <p>I have run a train with the old script and it run smoothly, so it seems it is a problem of the new script.</p> <p>\u00bfCould it be related to <code>bloat16</code> and <code>float16</code>? YES! Switching to <code>float16</code> on my computer resulted on unstable trainings, I should be careful on Kaggle because there using <code>float16</code> is 4 times faster.</p>"},{"location":"modeling/Iteration_08_code_improvements/#unstable-validation-loss","title":"Unstable validation loss","text":"<p>In this situations the loss is unstable:</p> <ul> <li>batch 2, shuffle validation set at start</li> </ul> <p>In this situations the loss is stable:</p> <ul> <li>batch 2, do not shuffle validation set at start</li> <li>batch 1, wether I shuffle or not at start</li> </ul> <p>I believe it is something related to the batch. Because there are 94 validation prompts, so there is no prompt leave out. Shuffling will influence the pairs and the pairs are influencing the loss. Maybe is related to padding?</p> <p>I will be using a batch size of 1 from now on.</p>"},{"location":"modeling/Iteration_08_code_improvements/#scale-compute","title":"Scale compute","text":"<ul> <li>To run wandb on a server without login I have to setup the <code>WANDB_API_KEY</code> environment variable.</li> <li>CUDA version seems to be <code>12.1.105</code> and cudnn <code>8.9.2.26</code> when I look with <code>conda list | grep nvidia</code></li> <li>Python version is <code>3.10</code></li> <li>I need to create a <code>requirements.txt</code> file</li> <li><code>huggingface/transformers-pytorch-gpu:4.41.3</code> might be a good candidate docker image. Link to docker hub</li> </ul> <pre><code>#docker run --gpus all -v /mnt:/mnt -it huggingface/transformers-pytorch-gpu:4.41.3\ndocker run --gpus all -v /mnt:/mnt -it cuda-python:python3.10-cuda12\ncd /mnt/hdd0/MEGA/AI/22_Kaggle/arc24\n#/usr/bin/python3\npip3 install -r requirements.txt; python3 scripts/fine-tuning.py --help\npip3 install -r requirements.txt --index-url https://download.pytorch.org/whl/cu120\n\npip install -r requirements.txt; python scripts/fine-tuning.py --help\n</code></pre> <p>So far everything is working on my computer. I have been able to run a training successfully.</p> <p>Creating the docker images</p> <pre><code>docker build -t cuda-python:python3.10-cuda12 .\ndocker tag cuda-python:python3.10-cuda12 gbarbadillo/cuda-python:python3.10-cuda12\ndocker push gbarbadillo/cuda-python:python3.10-cuda12\n</code></pre> <p>I can train on a single A6000 GPU.</p>"},{"location":"modeling/Iteration_08_code_improvements/#results","title":"Results","text":""},{"location":"modeling/Iteration_08_code_improvements/#iterabledataset-results","title":"IterableDataset results","text":"<p>I have verified that when not using data augmentation the IterableDataset yields the same results as the Dataset.</p> <p>Then I have run a 1k steps experiment using data augmentation and the results suggest that there is no difference.</p> <p></p> <p>The train loss is almost the same. We see variations in the validation loss as in other experiments, but the iterable dataset is not worse so we can validate the implementation.</p>"},{"location":"modeling/Iteration_08_code_improvements/#load-optimizer","title":"Load optimizer","text":"<p>So far I have implemented a method to load the optimizer, but it seems to load the learning rate as well. TODO: I need to work deeper on that.</p>"},{"location":"modeling/Iteration_08_code_improvements/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_08_code_improvements/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_08_code_improvements/#todo","title":"TODO","text":"<ul> <li> Unify the train scripts</li> <li> Unify the evaluation scripts</li> <li> Refactor code such as data augmentation to remove duplications</li> <li> Use an iterable dataset to avoid doing all the augmentations before training. This will create   a better augmented distribution and give more control over the data.</li> <li> I need to validate that the iterable works equal or better, to do that I will use old train and val sets for better reproducibility.</li> <li> Better control over the prompt templates, I would like to try new approaches in the future</li> <li> Implement option to load the optimizer when fine-tuning</li> <li> Check if loading an optimizer is helpful for fine-tuning</li> <li> Problem with wandb rate limits:<ul> <li>https://docs.wandb.ai/guides/track/limits#rate-limits</li> <li>https://community.wandb.ai/c/w-b-support/36</li> <li>Solved without doing anything, seemed to be a wandb problem</li> </ul> </li> <li> Diverging trainings, should I decrease <code>max_grad_norm</code>?<ul> <li> Try decreasing <code>max_grad_norm</code></li> <li> Try increasing the batch size</li> <li> Maybe change the optimizer? <code>optim=\"paged_adamw_8bit\"</code></li> <li> It was for using float16 instead of bfloat16</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_09_improve_inference/","title":"Iteration 9. Improve inference","text":"<p>21-08-2024</p>"},{"location":"modeling/Iteration_09_improve_inference/#goal","title":"Goal","text":"<p>Can we improve the accuracy of the LLM by using a more advanced inference?</p>"},{"location":"modeling/Iteration_09_improve_inference/#motivation","title":"Motivation","text":"<p>MindsAI team uses AIRV (augment, inference, reverse augmentation, and vote)</p> <p>This simply means that they make multiple inferences for each task and then vote. Voting can be done at the grid level or we could vote by cell. I have to visualize the predictions to see if we could increase the accuracy by using some of this techniques.</p> <p>Another option to try to improve the accuracy is to use beam-search. Instead of using the naive approach of doing greedy generation we could try different options and create an answer that has an overall higher probability.</p>"},{"location":"modeling/Iteration_09_improve_inference/#development","title":"Development","text":"<p>It's a good opportunity to refactor the inference code and make it more flexible. I would save all the predictions in a file and then have a function to take a decision. I will have to create a notebook to visualize the predictions, probably sorted by some metric.</p> <p>So far I'm only doing geometric augmentations, but I could also do color swaps and change the order of the train samples. That will increase the compute used at inference, but I could probably optimize the inference speed.</p>"},{"location":"modeling/Iteration_09_improve_inference/#beam-search","title":"Beam search","text":"<ul> <li>What is Beam Search? Explaining The Beam Search Algorithm</li> <li>VLLM sampling params</li> </ul> <p>use_beam_search \u2013 Whether to use beam search instead of sampling. best_of \u2013 Number of output sequences that are generated from the prompt. From these best_of sequences, the top n sequences are returned. best_of must be greater than or equal to n. This is treated as the beam width when use_beam_search is True. By default, best_of is set to n.</p> <p>I have been playing with beam search and it generates a sequence with higher probability than simply greedy decoding.</p> <p></p> <pre><code>## Best of 1 (logprob: -51.21, runtime: 2.06s)\nAlexander the Great was a legendary conqueror who ruled over a vast empire that\nstretched from Greece to India. One of his most famous conquests was the city of\nTyre, which was located on the coast of Lebanon. Alexander had heard of Tyre's\nwealth and power, and he was determined to conquer it. He sent a fleet of ships\nto attack the city, and they were successful in capturing it. Alexander then\nspent several years building up the city and making it his own. However, Ty\n\n## Best of 2 (logprob: -37.66, runtime: 1.90s)\nAlexander the Great, the son of Philip II of Macedon, was one of the most famous\nconquerors in history. He was born in 356 BC and died in 323 BC, but his legacy\nlives on to this day. Alexander was known for his military prowess and his\nability to conquer vast territories. One of the most famous battles that\nAlexander fought was against the city of Tyre. Tyre was a powerful city-state in\nthe eastern Mediterranean, and it had been a\n\n## Best of 128 (logprob: -31.82, runtime: 12.52s)\nAlexander the Great was one of the most famous conquerors in history. He was\nborn in 356 BC and died in 323 BC, but his legacy lives on to this day. One of\nthe most famous battles he fought was the Battle of Tyre, which took place in\n332 BC. The city of Tyre was one of the most powerful cities in the world at the\ntime. It was located on the coast of the Mediterranean Sea and was known for its\n</code></pre> <p>Thus it seems reasonable that we could improve the accuracy of the predictions by using beam search.</p>"},{"location":"modeling/Iteration_09_improve_inference/#update-inference-script","title":"Update inference script","text":"<p>The table shows inference time for 100 evaluation tasks with <code>Qwen-0.5B</code> and 8 predictions per task.</p> implementation inference time speedup baseline 54m - batch all task prompts 7m 7.7 batch all prompts 1m50s 29.5 <p>By batching the prompts I have been able to speedup inference by ~30 times. This is a valuable lesson for the future: always batch the LLM predictions if we are not interested in latency but in throughput.</p> <p>This speedup opens the door to generating many more predictions per task and later do voting or other more advanced selection process.</p>"},{"location":"modeling/Iteration_09_improve_inference/#results","title":"Results","text":""},{"location":"modeling/Iteration_09_improve_inference/#beam-search-results","title":"Beam search results","text":"<p>This are the results of using <code>Qwen-0.5B</code> and making 8 predictions per task.</p> best_of pass_n accuracy correct_pixels correct_size runtime 1 12% 4.40% 77% 88% 7m 2 13.50% 5.90% 78.70% 90% 50m 4 13.50% 6% 77.50% 89% 1h30 <p>We can see that the results improve, but at the cost of much bigger runtime.</p> <p>GPU utilization is much lower when using beam-search. Likely related issue, maybe it is not optimized because they were considering to remove the beam search feature.</p>"},{"location":"modeling/Iteration_09_improve_inference/#increasing-the-number-of-predictions","title":"Increasing the number of predictions","text":""},{"location":"modeling/Iteration_09_improve_inference/#qwen-05b","title":"Qwen-0.5B","text":"<pre><code>Maximum number of predictions: 512\nNumber of predictions: 1 accuracy: 5.0% correct_pixels: 65.3%   correct_size: 81.0% pass_n: 5.0%    unanswered: 2.5%    \nNumber of predictions: 2 accuracy: 4.4% correct_pixels: 71.2%   correct_size: 85.6% pass_n: 6.7%    unanswered: 2.7%    \nNumber of predictions: 4 accuracy: 4.6% correct_pixels: 75.2%   correct_size: 88.3% pass_n: 10.1%   unanswered: 2.6%    \nNumber of predictions: 8 accuracy: 4.7% correct_pixels: 78.6%   correct_size: 90.0% pass_n: 13.4%   unanswered: 2.7%    \nNumber of predictions: 16 accuracy: 4.5%    correct_pixels: 81.2%   correct_size: 91.3% pass_n: 17.0%   unanswered: 2.7%    \nNumber of predictions: 32 accuracy: 4.5%    correct_pixels: 83.4%   correct_size: 92.8% pass_n: 19.8%   unanswered: 2.6%    \nNumber of predictions: 64 accuracy: 4.6%    correct_pixels: 85.3%   correct_size: 93.6% pass_n: 22.5%   unanswered: 2.6%    \nNumber of predictions: 128 accuracy: 4.5%   correct_pixels: 86.5%   correct_size: 94.3% pass_n: 23.8%   unanswered: 2.7%    \nNumber of predictions: 256 accuracy: 4.5%   correct_pixels: 87.4%   correct_size: 94.6% pass_n: 25.1%   unanswered: 2.7%    \nNumber of predictions: 512 accuracy: 4.5%   correct_pixels: 87.8%   correct_size: 95.0% pass_n: 26.0%   unanswered: 2.7%\n</code></pre> <p>We went from a model that on average only solves 6.7% of the tasks with 2 predictions to solving 26% by making 512 predictions. Would we be able to select the correct prediction among 512 options?</p>"},{"location":"modeling/Iteration_09_improve_inference/#qwen-15b","title":"Qwen-1.5B","text":"<pre><code>Maximum number of predictions: 128\nNumber of predictions: 1 accuracy: 5.0% correct_pixels: 67.5%   correct_size: 82.9% pass_n: 5.0%    unanswered: 3.2%    \nNumber of predictions: 2 accuracy: 4.9% correct_pixels: 72.8%   correct_size: 86.0% pass_n: 7.8%    unanswered: 3.3%    \nNumber of predictions: 4 accuracy: 5.0% correct_pixels: 76.3%   correct_size: 88.2% pass_n: 10.8%   unanswered: 3.4%    \nNumber of predictions: 8 accuracy: 5.0% correct_pixels: 79.1%   correct_size: 89.4% pass_n: 14.3%   unanswered: 3.5%    \nNumber of predictions: 16 accuracy: 5.1%    correct_pixels: 81.7%   correct_size: 90.7% pass_n: 17.5%   unanswered: 3.4%    \nNumber of predictions: 32 accuracy: 5.0%    correct_pixels: 83.5%   correct_size: 91.8% pass_n: 20.3%   unanswered: 3.4%    \nNumber of predictions: 64 accuracy: 5.0%    correct_pixels: 85.0%   correct_size: 92.9% pass_n: 22.5%   unanswered: 3.4%    \nNumber of predictions: 128 accuracy: 5.0%   correct_pixels: 86.0%   correct_size: 93.0% pass_n: 25.0%   unanswered: 3.4%\n</code></pre> <p>We see the same tendency, maybe more linear with this model.</p>"},{"location":"modeling/Iteration_09_improve_inference/#effect-of-the-temperature","title":"Effect of the temperature","text":"<p>Increasing the temperature makes the model more creative: pass_n increases, but overall accuracy decreases. Unanswered also increases, likely due to errors when generating the grids.</p>"},{"location":"modeling/Iteration_09_improve_inference/#how-good-is-voting","title":"How good is voting?","text":"model predictions pass_n random2 voting attempt_1 voting accuracy Qwen-0.5B 512 26.0% 6.7% 12.5% 7.5% 48% Qwen-0.5B T=0.8 128 31.5% 6.7% 9.5% 9.5% 30% Qwen-1.5B 128 25.0% 7.8% 10.5% 9.5% 42% <p>We can see that the voting selection mechanism has an accuracy between 30-50% to select the correct answer. It is much better than random, but at the same time we are leaving more than half of the correct answers. Could we use the model to select the best answer?</p>"},{"location":"modeling/Iteration_09_improve_inference/#submission-results","title":"Submission results","text":"experiment old LB score new LB score qwen2-0.5b-instruct/2 2% 6% qwen2-0.5b-instruct/2 + ttft 4% 5% qwen2-1.5b-instruct/2 4% 5% qwen2-1.5b-instruct/2 + ttft 4% 7%"},{"location":"modeling/Iteration_09_improve_inference/#how-does-the-output-size-affect-the-results","title":"How does the output size affect the results?","text":"<p>I don't see a clear relation between output size and results.</p>"},{"location":"modeling/Iteration_09_improve_inference/#conclusion","title":"Conclusion","text":"<p>By improving the inference speed and the answer selection we now are scoring 7 using LLMs instead of the previous best leaderboard score of 5.</p> <ul> <li>Beam search might be helpful, but it is not efficiently implemented in VLLM yet</li> <li>By batching the prompts I have been able to speedup inference by ~30 times</li> <li>If we increase the number of predictions we got a correct answer for more tasks, but the challenge is how to select the correct answer</li> <li>Voting can select the correct answer between 30-50% of the times for the current models. I guess that this accuracy would improve for better models</li> <li>It's possible to use the temperature to trade accuracy for pass_n, the model becomes more creative and can solve more tasks</li> </ul>"},{"location":"modeling/Iteration_09_improve_inference/#next-steps","title":"Next steps","text":"<ul> <li> Can I use the model to judge which predictions are the best? https://github.com/vllm-project/vllm/issues/5234</li> <li> The improvement of using test time fine-tuning is not that big. What if I fine-tune for each task independently?</li> <li> Could I try an imitation of beam-search by setting a non-zero temperature, creating multiple predictions for the same prompt and collecting only the one with the highest logprob? Prefix caching might help, but probably the generation is already doing that if requiring more than one response.</li> </ul>"},{"location":"modeling/Iteration_09_improve_inference/#todo","title":"TODO","text":"<ul> <li> Modify generation script to allow generating an arbitrary number of solutions</li> <li> Create a function to select the solution (by voting f.e.)</li> <li> Create a notebook to understand how beam search works, first using text</li> <li> Can I speedup inference? There is evidence that batching could make a great speedup.</li> <li> Can I speedup inference even more? Group all the prompts together</li> <li> Does beam-search increase the accuracy of the model?</li> <li> Does it help to add more data augmentations? (train samples reorder, color swap)</li> <li> Document how good the voting script is compared to random selection</li> <li> What is the effect of using T!=0?</li> <li> What is the influence of the shape in the accuracy?</li> <li> Add submission results</li> <li> Add conclusions</li> </ul>"},{"location":"modeling/Iteration_10_improve_selection/","title":"Iteration 10. Improve response selection","text":"<p>24-08-2024</p>"},{"location":"modeling/Iteration_10_improve_selection/#goal","title":"Goal","text":"<p>Can I use an LLM to improve the selection of responses over voting?</p>"},{"location":"modeling/Iteration_10_improve_selection/#motivation","title":"Motivation","text":"<p>On the previous iteration we have seen that voting is able to select the correct answer with an accuracy of 30-50%. If we can find a more accurate method that will make a direct improvement in system accuracy.</p> <p>We already know that using beam search is able to create better answers, but the implementation is not efficient and it is much slower than normal generation. My intuition is that we can use the model to estimate the likelihood of an answer once it has been created. And maybe that can be used to select the right answer.</p>"},{"location":"modeling/Iteration_10_improve_selection/#development","title":"Development","text":""},{"location":"modeling/Iteration_10_improve_selection/#measuring-likelihood-of-a-prompt","title":"Measuring likelihood of a prompt","text":"<p>I'm going to create a notebook to do a proof of concept of the idea using VLLM. If it works I will convert it to a script that can replace the current voting script.</p> <p>I have found that when requiring VLLM to compute the logprobs of the prompt it causes OOM error if the prompt length is not small. Thus we cannot use that feature. I'm going to try instead to use the logprob of the generated grids.</p> <p>Links:</p> <ul> <li>Github issue: Add efficient interface for evaluating probabilities of fixed prompt-completion pairs</li> <li>prompt_logprobs \u2013 Number of log probabilities to return per prompt token.</li> </ul>"},{"location":"modeling/Iteration_10_improve_selection/#results","title":"Results","text":""},{"location":"modeling/Iteration_10_improve_selection/#using-logprob-as-a-secondary-sorting-factor-is-the-best-option","title":"Using logprob as a secondary sorting factor is the best option","text":"naive aggregate combine voting and logprob number of predictions pass_n voting cumulative_logprob mean_cumulative_logprob cumulative_logprob mean_cumulative_logprob cumulative_logprob mean_cumulative_logprob 8 16.0% 10.5% 8.7% 10.2% 9.20% 9.70% 9.20% 10.20% 32 23.0% 12.5% 12.2% 13.3% 12.20% 12.20% 13.80% 13.80% 128 23.5% 12.5% 10.7% 10.7% 12.20% 11.20% 12.80% 12.80% <p>The logprobs of the predictions do not have better sorting capabilities than voting. However we can use voting to sort the predictions, and use the logprobs to solve the ties. That improves over random choosing when there is a tie.</p> <p>To find the best algorithm I used the metric below:</p> <pre><code>voting_mean_logprob mean_cumulative_logprob mean correct position: 2.2 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 27, 1, 3, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\nvoting_lower_bound_constant_std mean_cumulative_logprob mean correct position: 2.2 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 27, 1, 3, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n\nvoting_mean_logprob cumulative_logprob mean correct position: 2.3 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 29, 1, 4, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\nvoting_lower_bound_constant_std cumulative_logprob mean correct position: 2.3 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 29, 1, 4, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n\nthere is no difference here.\n\n\nvoting mean_cumulative_logprob mean correct position: 2.4 ([1, 7, 1, 0, 0, 3, 1, 0, 4, 35, 1, 4, 14, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\nvoting cumulative_logprob mean correct position: 2.4 ([1, 7, 1, 0, 0, 3, 1, 0, 4, 35, 1, 4, 14, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n\nlower_bound_constant_std cumulative_logprob mean correct position: 2.8      ([17, 5, 0, 0, 0, 2, 2, 1, 3, 24, 0, 16, 5, 0, 9, 0, 0, 0, 0, 0, 0, 7, 2, 3, 5, 0])\nlower_bound_constant_std mean_cumulative_logprob mean correct position: 2.8 ([14, 12, 0, 0, 0, 2, 2, 1, 3, 22, 0, 5, 5, 0, 9, 0, 0, 0, 0, 0, 0, 7, 2, 2, 5, 0])\n\n\nlower_bound mean_cumulative_logprob mean correct position: 3.0 ([15, 18, 0, 0, 0, 2, 2, 0, 3, 21, 0, 9, 3, 0, 9, 0, 0, 0, 0, 0, 1, 7, 5, 3, 5, 0])\nlower_bound cumulative_logprob mean correct position: 3.0 ([17, 11, 0, 0, 0, 2, 1, 0, 3, 24, 0, 17, 3, 0, 9, 0, 0, 0, 0, 0, 1, 7, 5, 3, 5, 0])\n\n\nmean_logprob cumulative_logprob mean correct position: 3.3 ([19, 6, 1, 0, 0, 1, 2, 5, 3, 22, 0, 17, 3, 0, 15, 2, 0, 0, 0, 0, 2, 11, 3, 3, 5, 0])\nmean_logprob mean_cumulative_logprob mean correct position: 3.3 ([20, 15, 1, 0, 0, 1, 2, 5, 3, 20, 0, 7, 3, 0, 15, 2, 0, 0, 0, 0, 2, 11, 3, 3, 5, 0])\n\nnaive mean_cumulative_logprob mean correct position: 4.3 ([82, 4, 1, 0, 0, 40, 3, 5, 101, 25, 0, 29, 15, 0, 14, 0, 0, 0, 3, 0, 0, 80, 1, 6, 42, 0])\nnaive cumulative_logprob mean correct position: 4.4 ([83, 5, 1, 0, 0, 40, 3, 5, 101, 27, 0, 86, 15, 0, 14, 0, 0, 0, 3, 0, 0, 79, 1, 6, 42, 0])\n</code></pre>"},{"location":"modeling/Iteration_10_improve_selection/#improvements-on-multiple-evaluations","title":"Improvements on multiple evaluations","text":"<p>I have done multiple evaluations as part of other iteration, the following plot shows how the new voting algorithm improves almost in all situations to the old voting.</p> <p></p>"},{"location":"modeling/Iteration_10_improve_selection/#conclusion","title":"Conclusion","text":"<p>We have developed a new grid selection algorithm that uses voting and logprobs as the metric to solve the ties. This method does not require extra computation (we use the logprobs from inference) and improves the results slightly.</p>"},{"location":"modeling/Iteration_10_improve_selection/#next-steps","title":"Next steps","text":"<p>I might develop a script to measure the likelihood of a response using torch, because VLLM is not optimized to do that. That may allow to improve the results.</p>"},{"location":"modeling/Iteration_10_improve_selection/#todo","title":"TODO","text":"<ul> <li> Create new script to select grids</li> </ul>"},{"location":"modeling/Iteration_11_pseudo_beam_search/","title":"Iteration 11. Pseudo beam-search","text":"<p>25-08-2024</p>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#goal","title":"Goal","text":"<p>Can I improve the accuracy of the predictions by using a pseudo-beam-search?</p>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#motivation","title":"Motivation","text":"<p>Beam-search has been probed to generate more accurate responses than greedy decoding. However it is not efficiently implemented on VLLM.</p> <p>My idea is to generate n responses for the same prompt and select the one with the highest logprob. This would be similar to beam-search, but the implementation would be much more efficient.</p>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#development","title":"Development","text":""},{"location":"modeling/Iteration_11_pseudo_beam_search/#results","title":"Results","text":""},{"location":"modeling/Iteration_11_pseudo_beam_search/#inference-speed","title":"Inference speed","text":"n runtime estimated runtime (min) 1 1m50 1.8 2 2m32 2.5 4 3m55 3.8 8 6m32 6.5 16 11m46 11.8 32 22m43 22.5 <p>The runtime increases linearly with <code>n</code>, however there is a constant time that makes that using <code>n=4</code> only twice the time as <code>n=1</code>.</p> <pre><code>python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --temperature=0.1 --output_filepath=submission_qwen05_x8_T01_n1.json --n=1\n</code></pre> <p>This table could work as a rule of thumb of the slowdown when using pseudo beam-search.</p> n slowdown 1 1 4 2 10 4 20 8"},{"location":"modeling/Iteration_11_pseudo_beam_search/#accuracy-effect","title":"Accuracy effect","text":"<p>On a previous iteration I was able to see improvements due to beam-search with just 8 predictions per task. Let's try do to the same. I will be using <code>n=20</code> and different temperatures.</p> <p></p> <p></p> <p>The tendency when increasing the temperature is completely different to the observed on previous experiments. But the improvement in accuracy is not clear.</p> <pre><code># estimated runtime 1h30 ~ 16*6\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T01.json --temperature=0.1\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T02.json --temperature=0.2\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T04.json --temperature=0.4\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T06.json --temperature=0.6\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T08.json --temperature=0.8\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --n=20 --output_filepath=submission_qwen05_x8_n20_T10.json --temperature=1.0\n\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T01.json --temperature=0.1\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T02.json --temperature=0.2\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T04.json --temperature=0.4\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T06.json --temperature=0.6\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T08.json --temperature=0.8\npython inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=16 --n=20 --output_filepath=submission_qwen05_x16_n20_T10.json --temperature=1.0\n</code></pre>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#full-experiment","title":"Full experiment","text":"<p>I have run an experiment for 4h49: <code>submission_qwen05_x128_n20_T08</code> and I'm comparing it to <code>submission_qwen15_x128</code>. The accuracy improves from 5% to 5.2% so the improvement is tiny, and the inference time has been close to 10 times. So it does not seem this is a promising path.</p>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#conclusion","title":"Conclusion","text":"<p>Accuracy improvement is tiny despite inference time being increased a lot. Is better to use compute in other ways.</p>"},{"location":"modeling/Iteration_11_pseudo_beam_search/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_11_pseudo_beam_search/#todo","title":"TODO","text":"<ul> <li> Modify inference script to support this<ul> <li> Are the outputs provided by LLM sorted by logprob, or I have to sort them myself? YES THEY ARE ALREADY SORTED</li> </ul> </li> <li> How does the inference speed changes when requesting more responses per prompt?</li> <li> Does the accuracy improves?</li> </ul>"},{"location":"modeling/Iteration_12_grid_representation/","title":"Iteration 12. Grid representation","text":"<p>26-08-2024</p>"},{"location":"modeling/Iteration_12_grid_representation/#goal","title":"Goal","text":"<p>Can I improve the accuracy of the models by using a better grid representation?</p>"},{"location":"modeling/Iteration_12_grid_representation/#motivation","title":"Motivation","text":"<p>Before going to generate synthetic data to learn the priors I want to know what is the best representation of the problem for an LLM. LLMs typically deal with 1d data, not 2d data. Thus trying different representations makes a lot of sense. This can have a big impact on the accuracy of the model.</p> <p>The grid representation should not use too many tokens, otherwise hardware requirements grow.</p>"},{"location":"modeling/Iteration_12_grid_representation/#development","title":"Development","text":""},{"location":"modeling/Iteration_12_grid_representation/#fine-tuning-script-refactor","title":"Fine-tuning script refactor","text":"<p>I need to refactor the fine-tuning script to make it very easy to try new representations and prompts. Code should be shared between train and evaluation.</p> <p>My idea will be to run a very short train with a fixed seed, refactor the code and verify that it still works the same way.</p>"},{"location":"modeling/Iteration_12_grid_representation/#finding-a-set-of-symbols-to-replace-the-numbers","title":"Finding a set of symbols to replace the numbers","text":"<p>Using the Llama and Qwen tokenizers I have been able to find a set of symbols that are unique (do not form part of other words in the vocabulary) Using this symbols the model should receive a representation that is equivalent to the current numbers one. But maybe the model can work better with that set of symbols.</p> <pre><code>selection = ['\u00f1', '\u00f2', '\u00f7', '\u00fb', '\u0105', '\u0107', '\u010f', '\u0119', '\u011a', '\u011e']\n</code></pre>"},{"location":"modeling/Iteration_12_grid_representation/#create-a-simple-evaluation-script","title":"Create a simple evaluation script","text":"<p>Currently I have a notebook to merge weights of the model and lora, a script to do inference and a notebook to evaluate. That works if I only have to evaluate a single model, but does not scale to evaluating many models.</p> <p>Thus I have to create either a script or a notebook that simply takes the path of the checkpoint that I want to evaluate, and does all the job.</p>"},{"location":"modeling/Iteration_12_grid_representation/#cosine-with-restarts-learning-rate-schedule","title":"Cosine with restarts learning rate schedule","text":"<p>I have updated the fine-tuning to support cosine with restarts learning rate scheduler.</p> <ul> <li>https://huggingface.co/docs/transformers/en/main_classes/trainer</li> <li>https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer_utils.py#L410</li> <li>https://huggingface.co/transformers/v4.2.2/_modules/transformers/optimization.html</li> </ul> <p>Maybe I could use another scheduler directly, that decreases the amplitude of the cosine restart over the train duration. The experiment with cosine learning rate seems to be increasing the learning rate to a too high value.</p> <ul> <li>https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR</li> <li>https://github.com/huggingface/transformers/blob/746104ba6f0514159d58dc2fb09c887d0e9d4863/src/transformers/trainer.py#L1249C22-L1249C34</li> <li>https://github.com/bitsandbytes-foundation/bitsandbytes/blob/main/bitsandbytes/optim/adamw.py</li> </ul> <p>4 cycles, 0.707, warmup ratio in the cycle. It seems I would need to give both the optimizer and scheduler as input to the train function.</p> <p>Study of how the Trainer works (source code). It has a method <code>self.create_optimizer_and_scheduler</code> that calls to <code>self.create_optimizer</code> and <code>self.create_scheduler</code>. This method is called from <code>self._inner_training_loop</code> that is itself called from the <code>self.train</code> method.</p> <ul> <li><code>self.train</code><ul> <li><code>self._inner_training_loop</code><ul> <li><code>self.create_optimizer_and_scheduler</code><ul> <li><code>self.create_optimizer</code></li> <li><code>self.create_scheduler</code><ul> <li><code>optimization.get_scheduler</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>I believe the simplest hack is to modify the <code>self.create_scheduler</code> function to return the scheduler I want.</p>"},{"location":"modeling/Iteration_12_grid_representation/#results","title":"Results","text":"<p>All the presented results are the mean metrics of doing 64 predictions per task.</p>"},{"location":"modeling/Iteration_12_grid_representation/#experiment-with-different-grid-encoders","title":"Experiment with different grid encoders","text":"<p>Wandb</p> row numbers grid shape other symbols accuracy correct_pixels correct_size unanswered 2.0% 58.0% 74.0% 2.2% x 2.3% 63.9% 81.5% 3.4% x 2.8% 62.3% 79.8% 2.8% x x 2.8% 66.3% 84.2% 2.8% x 1.9% 59.1% 75.5% 2.4% x x x 2.3% 66.7% 84.9% 4.1% <ul> <li>Using row numbers and grid shape increase accuracy, correct pixels and correct size</li> <li>There is no evidence that using alternative symbols instead of numbers gives better results.</li> </ul> <p>Thus the best encoder configuration for Qwen would be <code>GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))</code>.</p> <pre><code>GridCodeBlockEncoder(MinimalGridEncoder())\nGridCodeBlockEncoder(RowNumberEncoder(MinimalGridEncoder()))\nGridShapeEncoder(MinimalGridEncoder())\nGridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\n\nGridCodeBlockEncoder(ReplaceNumberEncoder(MinimalGridEncoder()))\nGridShapeEncoder(RowNumberEncoder(ReplaceNumberEncoder(MinimalGridEncoder())))\n</code></pre>"},{"location":"modeling/Iteration_12_grid_representation/#validation-loss-is-not-a-perfect-proxy","title":"Validation loss is not a perfect proxy","text":"<p>I have evaluated multiple checkpoints of a training, and the following plots show how the metrics change during the training.</p> <p></p> <p>We see an almost monotonic improvement during training, however the validation loss shows a different story.</p> <p></p> <p>Thus I should probably evaluate the last and the best checkpoint, and launch longer trainings because there might be room for improvement.</p> <p>Validation loss is useful when it decreases, but when it diverges it is no longer correlated with model accuracy</p>"},{"location":"modeling/Iteration_12_grid_representation/#experiments-training-on-arc-tasks","title":"Experiments training on ARC tasks","text":""},{"location":"modeling/Iteration_12_grid_representation/#longer-trainings","title":"Longer trainings","text":"<p>Since I have found that the validation loss was not a good metric, I'm going to train the models for longer.</p> <p></p> <p>The plot shows a clear disconnection between validation loss and the other metrics.</p>"},{"location":"modeling/Iteration_12_grid_representation/#optimal-learning-rate","title":"Optimal learning rate","text":"<p>I have tried increasing the default 1e-4 learning rate to see if I could get better results, without success.</p> <p></p> model learning rate train loss val loss accuracy correct_pixels correct_size pass_64 unanswered Qwen2-0.5B 1.00E-04 0.03 0.175 3.40% 67.50% 85.00% 18.00% 2.60% Qwen2-0.5B 2.00E-04 0.0319 0.175 2.90% 65.50% 82.70% 19.00% 3.60% Qwen2-0.5B 4.00E-04 0.043 0.152 2.80% 65.40% 83.00% 12.50% 3.10%"},{"location":"modeling/Iteration_12_grid_representation/#effect-of-lora-rank","title":"Effect of lora rank","text":"<ul> <li>Train loss decreases as the lora rank increases, as expected. Given more capacity the loss is reduced more.</li> <li>There seems to be a relation between accuracy and lora rank. We get higher accuracy by using higher ranks.</li> <li>The relation with the other metrics is unclear</li> </ul> model lora_rank train loss val loss accuracy correct_pixels correct_size pass_64 unanswered Qwen2-0.5B 16 0.0385 0.175 3.10% 66.50% 85.00% 19.50% 3.00% Qwen2-0.5B 32 0.0305 0.175 3.40% 67.50% 85.00% 18.00% 2.60% Qwen2-0.5B 64 0.0249 0.189 3.20% 65.40% 82.80% 15.00% 4.10% Qwen2-0.5B 128 0.021 0.1805 4% 67.30% 83.90% 18.00% 3.10% Qwen2-0.5B 256 0.0194 0.1856 4.50% 67.90% 84.60% 21.00% 2.90%"},{"location":"modeling/Iteration_12_grid_representation/#test-time-fine-tuning","title":"Test time fine-tuning","text":"<p>I have done a bunch of experiments with different learning rates and learning rate schedules. The best learning rate is the same as in training: 1e-4. And the best schedule is linear. I have tried also cosine with restarts, cyclic learning rate and constant learning rate but give worse results.</p>"},{"location":"modeling/Iteration_12_grid_representation/#optimal-training-steps","title":"Optimal training steps","text":"<p>We see that the accuracy increases when we fine-tune for longer with test-time fine-tuning. Train loss decreases and validation loss raises. The other metrics do not show a clear relation with the number of training steps.</p> steps train loss val loss accuracy correct_pixels correct_size pass_64 1000 0.044 0.17 6.70% 67.90% 83.30% 28.00% 2000 0.0248 0.215 7.80% 67.00% 81.60% 26.50% 4000 0.0066 0.275 8.70% 69.00% 83.80% 29.50% 8000 0.00167 0.346 9.50% 68.10% 82.10% 25.50% <p>Just by fine-tuning for longer we are able to increase accuracy by 3% (from 6% to 9%).</p>"},{"location":"modeling/Iteration_12_grid_representation/#removing-train-samples-to-fit-on-max-sequence-length","title":"Removing train samples to fit on max sequence length","text":"<p>As part of a following iteration I have implemented a feature that if a task is too long to fit into the <code>max_sequence_length</code>, it removes train samples until it is small enough.</p> <p>I wanted to find if it is helpful or not for fine-tuning the model. The effect is is unclear as the results below show. In one case it improves the results and in other it worsens them. Thus the effect is likely very small.</p> <pre><code># Experiment 1\n/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft/05_shape-and-number-new-model_Qwen2-0.5B-Instruct_lr1e-4_r32_4e3steps/checkpoint-4000/inference_x64.json\naccuracy: 8.7%  correct_pixels: 69.0%   max_correct_pixels: 82.6%   correct_size: 83.8% any_correct_size: 88.0% pass_64: 29.5%  unanswered: 3.0%    \n/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft/06_shape-and-number-new-model-rtstfmsl_Qwen2-0.5B-Instruct_lr1e-4_r32_4e3steps/checkpoint-4000/inference_x64.json\naccuracy: 8.7%  correct_pixels: 68.4%   max_correct_pixels: 81.7%   correct_size: 82.7% any_correct_size: 86.0% pass_64: 27.0%  unanswered: 2.9%\n\n# Experiment 2\n/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft/11_bigger-lora_Qwen2-0.5B-Instruct_lr1e-4_r256_4e3steps/checkpoint-4000/inference_x64.json\naccuracy: 10.4% correct_pixels: 71.0%   max_correct_pixels: 83.2%   correct_size: 84.1% any_correct_size: 87.5% pass_64: 28.0%  unanswered: 2.7%\n/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft/11_bigger-lora-remove-train-samples_Qwen2-0.5B-Instruct_lr1e-4_r256_4e3steps/checkpoint-4000/inference_x64.json\naccuracy: 10.8% correct_pixels: 71.1%   max_correct_pixels: 83.4%   correct_size: 83.8% any_correct_size: 87.5% pass_64: 32.5%  unanswered: 2.1%    \n\n# Experiment 3\n/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/04_row-number-and-grid-shape_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps/checkpoint-6000/inference.json\naccuracy: 2.8%  correct_pixels: 66.3%   max_correct_pixels: 82.2%   correct_size: 84.2% any_correct_size: 91.0% pass_n: 18.5%   unanswered: 2.8%\n/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/12_rtstfmsl_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps/checkpoint-6000/inference_x64.json\naccuracy: 3.0%  correct_pixels: 67.7%   max_correct_pixels: 83.1%   correct_size: 85.4% any_correct_size: 91.0% pass_n: 16.5%   unanswered: 3.0%\n</code></pre>"},{"location":"modeling/Iteration_12_grid_representation/#qwen2-05b-vs-qwen2-15b","title":"Qwen2-0.5B vs Qwen2-1.5B","text":""},{"location":"modeling/Iteration_12_grid_representation/#train-on-arc-tasks","title":"Train on ARC tasks","text":"model steps train loss val loss accuracy correct_pixels correct_size pass_64 pass_2 Qwen2-0.5B 24000 0.0157 0.217 4.00% 66.70% 84.10% 21.50% 11.20% Qwen2-1.5B 12000 0.015 0.188 6.20% 70.50% 87.60% 22.00% 13.30%"},{"location":"modeling/Iteration_12_grid_representation/#test-time-fine-tuning_1","title":"Test-time fine-tuning","text":"model steps train loss val loss accuracy correct_pixels correct_size pass_64 pass_2 Qwen2-0.5B 4000 0.0066 0.275 8.70% 69.00% 83.80% 29.50% 22.50% Qwen2-1.5B 4000 0.0015 0.323 12.90% 72.60% 85.30% 33.00% 23.20%"},{"location":"modeling/Iteration_12_grid_representation/#summary","title":"Summary","text":"<p>Qwen2-1.5B gets better results but the difference is small when we consider the pass_2 metric which is the most relevant for the challenge.</p> <p>When we train on ARC tasks we get a pass_2 of ~12% and this increases to 23% when doing test-time fine-tuning.</p>"},{"location":"modeling/Iteration_12_grid_representation/#submission-results","title":"Submission results","text":"<p>We have improved the score from 7 to 10. In addition we have been able to ensemble the model with the 2020 solution and achieve a score of 25 (24 + 1).</p>"},{"location":"modeling/Iteration_12_grid_representation/#conclusion","title":"Conclusion","text":"<p>The biggest finding of this iteration is that validation loss is only useful when it decreases, once it starts to diverge it is no longer useful. I have found that I can train for longer both on normal training and test-time fine-tuning and improve the results.</p> <p>Predicting the grid shape and writing row numbers also helps to improve the predictions, this has a small cost in outputting more tokens.</p> <p>It seems that using higher lora ranks gives more accurate models.</p> <p>Qwen2-1.5B gets better results than Qwen2-0.5B, but the difference in pass_2 metric is too small. That might explain that we didn't see clear differences in the leaderboard.</p> <p>We have been able to increase the LB score to 10, and using an ensemble to 25. This implies that there is a gap of around 10% between validation and leaderboard.</p>"},{"location":"modeling/Iteration_12_grid_representation/#next-steps","title":"Next steps","text":"<ul> <li>I might have to reconsider the role of lora ranking now that I know that validation loss is not a good proxy.   Run a series of experiments with different r. Maybe having a higher r could allow for faster ttft.</li> <li>Trainings are becoming too long, could I speedup them using libraries such as unsloth?</li> <li>Optimize the input prompt</li> <li>Does it make sense to fine-tune bigger models such as Phi-3, Llama or the 7B Qwen2?</li> </ul>"},{"location":"modeling/Iteration_12_grid_representation/#todo","title":"TODO","text":"<ul> <li> Does it help to predict the shape of the grid?</li> <li> Does it help to add row idx at the start of each line?</li> <li> Are the pixel symbols relevant? Or could I replace the number for other symbols?</li> <li> How useful is the validation loss?</li> <li> Train for longer, is validation loss really useful?<ul> <li> What is the optimal train steps?</li> <li> I'm using the best learning rate?</li> <li> Can I get better results using a different lora rank?</li> </ul> </li> <li> Test time fine-tuning, train with different number of steps<ul> <li> 1e-4 is the best learning rate</li> <li> So far the best results are obtained training for longer, I have trained up to 4k steps</li> <li> Do I get better results if I train for more than 4k steps?</li> <li> Can the model learn faster using cyclic learning rates? No</li> <li> Does it help to to remove train samples to fit training sequence length? First experiment gives worse results, but not sure if the differences are significative.</li> <li> Could I train faster by changing the batch size?</li> </ul> </li> <li> Qwen2-0.5B vs Qwen2-1.5B</li> <li> Do we get improvements in submission?</li> <li> If I make the same submission 3 times, what is the variability of the score? (Using a random seed)</li> </ul>"},{"location":"modeling/Iteration_13_single_task_ttft/","title":"Iteration 13. Single task test-time fine-tuning","text":"<p>29-08-2024</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#goal","title":"Goal","text":"<p>Can we improve the accuracy of the model if we fine-tune the model for each task independently?</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#motivation","title":"Motivation","text":"<p>Jack Cole says the following in the Machine Learning Street interview:</p> <p>If you train a model on the re-arc dataset you will get like 1% on the test set. But if you apply their techniques of active inference the score will increase to 23%</p> <p>This is not what I'm currently seeing. For example when I improved the inference script to be much faster and added voting, I get LB scores of 6 and 5 for Qwen2-0.5B and Qwen2-1.5B, when applying test-time fine-tuning the scores were 5 and 7. Thus my initial score is much better than theirs, but I don't experience that improvement when using test-time fine-tuning.</p> <p>On recent experiments with the evaluation dataset I could improve from 9% to 15%. Again not even close to what Jack Cole said. TODO: this is an ongoing work, maybe using longer trainings or higher learning rates could improve my results.</p> <p>Maybe the problem is that so far I have been fine-tuning in the whole test set. That might be suboptimal because the tasks could be contradictory. So maybe it's better to fine-tune for each task independently. Instead of fine-tuning for 1k steps on 100 tasks, fine-tune for 10 steps in each of the 100 tasks.</p> <p>Other possible explanations:</p> <ul> <li>It might also be the case that we need a stronger base model, but we leave that for future experiments.</li> <li>Maybe my test fine-tuning method is not as good as theirs</li> </ul>"},{"location":"modeling/Iteration_13_single_task_ttft/#development","title":"Development","text":"<p>The easier way to test this is to fork the Kaggle notebook and make the following modifications.</p> <ol> <li>Decompose the test file into single task files</li> <li>Fine-tune on each of those tasks, generating n saved models</li> <li>Make inference with all of the models, each on its task</li> <li>Concatenate all the predictions on a single file</li> </ol> <p>The drawback of this approach is that the warmup time of fine-tuning and inference will happen 100 times instead of just one. But I believe there is enough time in the submission to do that.</p> <p>Another possible problem is that if there is a single task, it might be the case that the training data is too long for training. I will have to think of how to solve that. Probably the best way is to randomly get rid of some of the inputs in that case. I could add that option to training. Otherwise the data generator will be stuck in an infinite loop. However I have found that if task has both inputs and outputs of 30x30, it might be the case that even with just one train sample we cannot fit the data in the <code>max_seq_len</code>. Thus I have to think of how to deal with those cases.</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#tasks-that-are-too-big-for-training","title":"Tasks that are too big for training","text":"<p>No prompt smaller than 4096 tokens for task 25094a63 No prompt smaller than 4096 tokens for task f9d67f8b No prompt smaller than 4096 tokens for task 981571dc No prompt smaller than 4096 tokens for task b9630600</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#vllm-freezes-randomly","title":"VLLM freezes randomly","text":"<p>On some submissions I have observed some timeouts that did not have sense, the configuration and model were the same as in other successful submissions that only took around 9 hours.</p> <p>When doing one of the experiments for this iteration I have finally see that the notebook was stuck at inference. Link to notebook run</p> <p>There is no message in the logs, it simply stops writing logs after 3.5 hours.</p> <p>To try to solve this I have added a timeout when running inference. If the execution takes more than the timeout it is stopped. I have set it to 5 minutes, but it seems it was too low and 3 submissions have timeout. I have inspected the inference times when doing 64 predictions, and just generating the output tokens took 3m for the worst case. So it makes sense that generating 128 predictions takes more than 5 minutes.</p> <p>I can hide VLLM logs with <code>export VLLM_LOGGING_LEVEL=ERROR</code></p>"},{"location":"modeling/Iteration_13_single_task_ttft/#parallelize-all-2020-solution","title":"Parallelize all 2020 solution","text":"<p>Currently I'm running icecuber solution alongside test-time fine-tuning. I cannot run at the same time icecuber and inference because they require more than 50% of RAM and there are crashes.</p> <p>I have prepared a notebook to measure dsl search resources usage and it turns out that sometimes it more than 60% of RAM. Thus we cannot parallelize with inference either.</p> <p></p> <p>I have also measured resources from icecuber to verify ram usage icecuber notebook It uses 80% of RAM sometimes in the execution.</p> <p></p> <p>In the other hand my solution has the following resource usage, this is an experiment with the validation set. For 3 hours it is doing the test-time fine-tuning and for 1 hour it makes inference.</p> <p></p> <p>It seems that the best solution would be to do all the 2020 solution while the test-time fine-tuning step.</p> <p>The easiest way would be to create a script that does both 2020 solution parts. Otherwise I should be waiting for one part to stop to run the other part and that would make the code more complex. TODO: maybe run dsl search at the same time as fine-tuning.</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#results","title":"Results","text":""},{"location":"modeling/Iteration_13_single_task_ttft/#experiment-about-optimal-number-of-splits","title":"Experiment about optimal number of splits","text":"<p>There is a clear positive relation between the number of splits and the accuracy of the model. However we see a decrease after 20 splits. My hypothesis is that if we were using a bigger compute budget we won't see that effect. The total number of training steps is 1000, so for 50 splits that means we will only change the model 20 times.</p> <p>We are able to reach an accuracy of 10.2% and a pass_2 of 23.7%. On the previous iteration I could only reach 9.4% accuracy fine-tuning for 8k steps, the best experiment with 1k steps reached 7% accuracy. I could reach a pass_2 of 22.7% fine-tuning for 4k steps, and the best experiment with 1k steps reached just 15%. This comparison really validates this single task test-time fine-tuning approach.</p> Click to see all the metrics <pre><code># For a compute budget of 1k steps\n# Aggregated metrics:\n1 split?\n\n2 splits, 3h12\naccuracy: 7.7%  correct_pixels: 69.8%   max_correct_pixels: 82.5%   correct_size: 84.4% any_correct_size: 87.0% pass_64: 25.5%  unanswered: 3.0%\naccuracy: 7.0%  correct_pixels: 71.3%   max_correct_pixels: 74.7%   correct_size: 85.0% any_correct_size: 86.0% pass_2: 14.0%   unanswered: 1.0%\n\n4 splits, 3h18\naccuracy: 9.3%  correct_pixels: 70.9%   max_correct_pixels: 84.5%   correct_size: 84.7% any_correct_size: 89.5% pass_64: 27.5%  unanswered: 3.8%\naccuracy: 11.9% correct_pixels: 73.3%   max_correct_pixels: 78.6%   correct_size: 85.9% any_correct_size: 87.9% pass_2: 23.7%   unanswered: 0.0%\n\n10 splits, 3h22\naccuracy: 9.9%  correct_pixels: 70.6%   max_correct_pixels: 82.1%   correct_size: 84.1% any_correct_size: 87.0% pass_64: 28.5%  unanswered: 2.9%\naccuracy: 11.7% correct_pixels: 74.2%   max_correct_pixels: 78.2%   correct_size: 86.2% any_correct_size: 86.7% pass_2: 23.5%   unanswered: 0.5%\n\n20 splits, 3h34\naccuracy: 10.2% correct_pixels: 71.0%   max_correct_pixels: 84.1%   correct_size: 84.5% any_correct_size: 89.0% pass_64: 30.0%  unanswered: 3.5%\naccuracy: 11.2% correct_pixels: 75.2%   max_correct_pixels: 80.8%   correct_size: 86.7% any_correct_size: 88.8% pass_2: 22.4%   unanswered: 0.5%\n\n50 splits, 4h12\naccuracy: 9.4%  correct_pixels: 71.2%   max_correct_pixels: 81.8%   correct_size: 85.4% any_correct_size: 88.0% pass_64: 27.0%  unanswered: 2.8%\naccuracy: 9.2%  correct_pixels: 75.7%   max_correct_pixels: 79.1%   correct_size: 88.3% any_correct_size: 88.8% pass_2: 18.4%   unanswered: 0.0%\n\n100 splits, 5h10\naccuracy: 8.9%  correct_pixels: 70.0%   max_correct_pixels: 82.4%   correct_size: 84.6% any_correct_size: 88.0% pass_64: 28.0%  unanswered: 3.1%\naccuracy: 9.7%  correct_pixels: 73.5%   max_correct_pixels: 76.9%   correct_size: 87.2% any_correct_size: 87.8% pass_2: 19.4%   unanswered: 0.0%\n\n50 splits, constant lr, 4h5\naccuracy: 10.4% correct_pixels: 70.3%   max_correct_pixels: 82.9%   correct_size: 83.9% any_correct_size: 87.0% pass_64: 29.5%  unanswered: 4.0%\naccuracy: 10.0% correct_pixels: 72.2%   max_correct_pixels: 76.3%   correct_size: 85.0% any_correct_size: 87.0% pass_2: 20.0%   unanswered: 1.0%\n</code></pre>"},{"location":"modeling/Iteration_13_single_task_ttft/#optimization-for-50-splits","title":"Optimization for 50 splits","text":"Click to see all the metrics <pre><code>qwen2-0.5b-instruct/5\n\n50 splits, 1000 steps, linear lr 1e-4, 4h12\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=194795197\naccuracy: 9.4%  correct_pixels: 71.2%   max_correct_pixels: 81.8%   correct_size: 85.4% any_correct_size: 88.0% pass_64: 27.0%  unanswered: 2.8%\naccuracy: 9.2%  correct_pixels: 75.7%   max_correct_pixels: 79.1%   correct_size: 88.3% any_correct_size: 88.8% pass_2: 18.4%   unanswered: 0.0%\n\n50 splits, 1000 steps, constant lr 1e-4 bs16, 4h5\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=194876119\naccuracy: 10.4% correct_pixels: 70.3%   max_correct_pixels: 82.9%   correct_size: 83.9% any_correct_size: 87.0% pass_64: 29.5%  unanswered: 4.0%\naccuracy: 10.0% correct_pixels: 72.2%   max_correct_pixels: 76.3%   correct_size: 85.0% any_correct_size: 87.0% pass_2: 20.0%   unanswered: 1.0%\n\n\n\nval qwen2-0.5b/5 50split 2k_step_bs8 1e-4_lr_cte (halve batch size to 8 and duplicate steps to 2000), 4h14, 3h8 fine-tuning (1h inference, so around 1-2 minutes per split)\naccuracy: 9.9%  correct_pixels: 69.0%   max_correct_pixels: 80.8%   correct_size: 82.0% any_correct_size: 84.5% pass_n: 33.5%   unanswered: 3.2%\naccuracy: 8.9%  correct_pixels: 72.8%   max_correct_pixels: 76.1%   correct_size: 85.2% any_correct_size: 85.7% pass_2: 17.9%   unanswered: 0.0%\n\nhalve again batch size and learning rate, 80 steps per training\nval qwen2-0.5b/5 50split 4k_step_bs4 5e-5_lr_cte, 4h20, 3h11 fine-tuning\naccuracy: 11.9% correct_pixels: 70.4%   max_correct_pixels: 83.7%   correct_size: 83.2% any_correct_size: 87.5% pass_n: 32.5%   unanswered: 3.5%    \naccuracy: 10.7% correct_pixels: 73.4%   max_correct_pixels: 78.9%   correct_size: 85.5% any_correct_size: 87.8% pass_2: 21.4%   unanswered: 0.8%\n\n160 steps per training\nval qwen2-0.5b/5 50split 8k_step_bs2 2e-5_lr_cte, 4h23, 3h15 fine-tuning\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=195770629\naccuracy: 11.1% correct_pixels: 71.4%   max_correct_pixels: 84.5%   correct_size: 85.0% any_correct_size: 88.0% pass_n: 33.5%   unanswered: 3.3%\naccuracy: 11.2% correct_pixels: 74.9%   max_correct_pixels: 79.7%   correct_size: 86.7% any_correct_size: 88.8% pass_2: 22.4%   unanswered: 0.5%\n\n\nval qwen2-0.5b/5 50split 8k_step_bs2 2e-5_lr_lin, 4h24, 3h16 fine-tuning, 1h10 inference\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=195800246\naccuracy: 10.9% correct_pixels: 71.2%   max_correct_pixels: 84.1%   correct_size: 84.9% any_correct_size: 88.0% pass_n: 35.0%   unanswered: 3.1%\naccuracy: 12.1% correct_pixels: 73.7%   max_correct_pixels: 79.4%   correct_size: 85.6% any_correct_size: 87.9% pass_2: 24.2%   unanswered: 1.3%\n\nval qwen2-0.5b/5 50split 8k_step_bs2 4e-5_lr_lin, 4h22\naccuracy: 11.9% correct_pixels: 71.5%   max_correct_pixels: 83.1%   correct_size: 84.7% any_correct_size: 87.0% pass_n: 38.0%   unanswered: 3.3%\naccuracy: 11.7% correct_pixels: 73.2%   max_correct_pixels: 78.5%   correct_size: 85.2% any_correct_size: 87.8% pass_2: 23.5%   unanswered: 1.0%\n\n# Go down to batch size 1, and use 100 splits\nval qwen2-0.5b/5 100split 16k_step_bs1 2e-5_lr_lin, 5h44, 3h55 fine-tuning 1h50 inference\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=196574355\naccuracy: 11.8% correct_pixels: 71.6%   max_correct_pixels: 84.6%   correct_size: 84.9% any_correct_size: 89.5% pass_n: 36.0%   unanswered: 3.0%\naccuracy: 11.5% correct_pixels: 75.9%   max_correct_pixels: 81.2%   correct_size: 87.2% any_correct_size: 88.8% pass_n: 23.0%   unanswered: 1.0%\n\n# Increase max_seq_len to 5120, 5h42\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=196575579\naccuracy: 12.2% correct_pixels: 72.1%   max_correct_pixels: 84.9%   correct_size: 85.2% any_correct_size: 88.5% pass_n: 34.0%   unanswered: 2.9%\naccuracy: 12.2% correct_pixels: 74.7%   max_correct_pixels: 81.8%   correct_size: 87.0% any_correct_size: 89.8% pass_n: 24.5%   unanswered: 1.3%\n\nval qwen2-0.5b/5 100split step16k_bs1 4e-5lr_lin\nhttps://www.kaggle.com/code/ironbar/single-task-test-time-fine-tuning-for-arc24?scriptVersionId=196610194\naccuracy: 12.5% correct_pixels: 72.0%   max_correct_pixels: 84.1%   correct_size: 84.6% any_correct_size: 88.5% pass_n: 37.0%   unanswered: 3.0%\naccuracy: 11.0% correct_pixels: 75.3%   max_correct_pixels: 79.4%   correct_size: 86.7% any_correct_size: 87.8% pass_n: 21.9%   unanswered: 0.5%37\n</code></pre> splits steps batch size lr lr schedule accuracy pass_64 vote_2 runtime 2 1000 16 1.00E-04 linear 7.70% 25.50% 14.0% 3h12 4 1000 16 1.00E-04 linear 9.30% 27.50% 23.7% 3h18 10 1000 16 1.00E-04 linear 9.90% 28.50% 23.5% 3h22 20 1000 16 1.00E-04 linear 10.20% 30.00% 22.4% 3h34 50 1000 16 1.00E-04 linear 9.40% 27.00% 18.4% 4h12 100 1000 16 1.00E-04 linear 8.90% 28.00% 19.4% 5h10 50 1000 16 1.00E-04 constant 10.40% 29.50% 20.0% 4h5 50 2000 8 1.00E-04 constant 9.90% 33.50% 17.9% 4h14 50 4000 4 5.00E-05 constant 11.90% 32.50% 21.4% 4h20 50 8000 2 2.00E-05 constant 11.10% 33.50% 22.4% 4h23 50 8000 2 2.00E-05 linear 10.90% 35.00% 24.2% 4h24 50 8000 2 4.00E-05 linear 11.90% 38.00% 23.5% 4h22 100 16000 1 2.00E-05 linear 11.80% 36.00% 23.0% 5h44 100 16000 1 2.00E-05 linear 12.20% 34.00% 24.5% 5h42 100 16000 1 4.00E-05 linear 12.50% 37.00% 21.9% 5h46 <p>Using 100 splits, batch size 1 and linear learning rate schedule we have improved the accuracy from 7.7% to 12.5% and vote_2 from 14% to 21.9%.</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#conclusion","title":"Conclusion","text":"<p>New best score of 28! using an ensemble</p>"},{"location":"modeling/Iteration_13_single_task_ttft/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_13_single_task_ttft/#todo","title":"TODO","text":"<ul> <li> Add the option to the train script to remove train samples to fit training sequence length</li> <li> I have found that sometimes a task would not fit into max seq length. How to detect that case and what to do?</li> <li> Measure the effect of using the feature above in a normal training. Done on previous iteration, unclear effect.</li> <li> Optimize the parametrization of single task ttft (stttft) (learning rate and steps) Does it improve over the baseline?</li> <li> Try with constant learning rate schedule, might be better for short fine-tunings.</li> <li> Can I improve the leaderboard score?</li> <li> Add logging to better analyze the problems, reduce verbosity<ul> <li> Can I hide VLLM loggings? Yes: export VLLM_LOGGING_LEVEL=ERROR</li> </ul> </li> <li> Better think of the timeout feature. Example of the feature working successfully: https://www.kaggle.com/code/ironbar/v2-single-task-test-time-fine-tuning-for-arc24?scriptVersionId=196707552</li> <li> Can I optimize the submission speed?<ul> <li> Maybe reduce VLLM RAM usage. https://docs.vllm.ai/en/latest/automatic_prefix_caching/apc.html</li> <li> Maybe use unsloth and change to single P100 GPU.</li> </ul> </li> <li> Can I increase the max_seq_len? That might preventing me from training on some tasks.</li> <li> Parallelize all 2020 solution so it does not add any extra time, how much RAM uses the second approach?</li> </ul>"},{"location":"modeling/Iteration_14_speedup_training/","title":"Iteration 14. Speedup training with unsloth","text":"<p>01-09-2024</p>"},{"location":"modeling/Iteration_14_speedup_training/#goal","title":"Goal","text":"<p>Can we speedup training using unsloth?</p>"},{"location":"modeling/Iteration_14_speedup_training/#motivation","title":"Motivation","text":"<p>On previous iterations I have tried using different learning rate schedules to speedup the training, or changing the batch size without success. Unsloth library might be an easy way to speedup training.</p> <p>If I'm able to speedup training I will be able to train faster, but at the same time I could do longer test-time fine-tuning when making a submission.</p> <p>Hopefully this will be a very fast iteration.</p>"},{"location":"modeling/Iteration_14_speedup_training/#development","title":"Development","text":""},{"location":"modeling/Iteration_14_speedup_training/#install-unsloth","title":"Install unsloth","text":"<p>I'm going to create a new local conda environment following their instructions</p> <p>I have to decrease the python version to 3.10, with 3.11 could not find a compatible distribution of xformers.</p> <pre><code>conda create --name arc-unsloth \\\n    python=3.10 \\\n    pytorch-cuda=12.1 \\\n    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers\nconda activate arc-unsloth\n\npython -c \"import torch; print(torch.cuda.is_available())\"\n\n\npip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\npip install --no-deps trl peft accelerate bitsandbytes\npip install wandb termcolor\n</code></pre> <p>I have accidentally removed conda when trying to clean my base environment :(</p> <p>Regenerating the arc original environment:</p> <pre><code>conda create -n arc pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y\nconda activate arc\npip install -r requirements.txt\npip install flash-attn --no-build-isolation\npip install vllm\n</code></pre>"},{"location":"modeling/Iteration_14_speedup_training/#fine-tuning-script","title":"Fine-tuning script","text":"<p>On a first step I'm going to duplicate the fine-tuning script and adapt it to use unsloth. Once I validate that it works and it's faster I will look for the way of having a single script.</p>"},{"location":"modeling/Iteration_14_speedup_training/#results","title":"Results","text":""},{"location":"modeling/Iteration_14_speedup_training/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_14_speedup_training/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_14_speedup_training/#todo","title":"TODO","text":"<ul> <li> It only supports 1 gpu! https://github.com/unslothai/unsloth/issues/547</li> </ul>"},{"location":"modeling/Iteration_15_study_data_scaling/","title":"Iteration 15. Study how well this method scales with data","text":"<p>01-09-2024</p>"},{"location":"modeling/Iteration_15_study_data_scaling/#goal","title":"Goal","text":"<p>How the model accuracy scales with the number of training tasks?</p>"},{"location":"modeling/Iteration_15_study_data_scaling/#motivation","title":"Motivation","text":"<p>Before taking a decision about the next steps, I want to know how well the current method scales with the available training data.</p>"},{"location":"modeling/Iteration_15_study_data_scaling/#development","title":"Development","text":"<p>The idea is to compare trainings that use the same number of steps (same compute) but use different training data. I'm going to add an option to the fine-tuning script to subsample the train data.</p> <p>I already have baseline results without subsampling. I'm going to try the following values: <code>[0.8, 0.6, 0.4, 0.2, 0.1]</code></p>"},{"location":"modeling/Iteration_15_study_data_scaling/#results","title":"Results","text":"<p>Accuracy seems to improve linearly when scaling the data. F.e. having 1400 tasks for training should yield an accuracy of 5%.</p> training tasks accuracy 700 2.80% 1400 5.60% 2800 11.20% 5000 20.00% 10000 40.00% 21250 85.00% <p>In the unlikely even that the trend continues \"forever\", it would be enough to generate 21k tasks to achieve the 500k reward.</p>"},{"location":"modeling/Iteration_15_study_data_scaling/#conclusion","title":"Conclusion","text":"<p>If we had access to more data with the same quality as the ARC tasks, it is very likely that we could improve the accuracy of our model.</p>"},{"location":"modeling/Iteration_15_study_data_scaling/#next-steps","title":"Next steps","text":"<ul> <li>Revisit the study about external data</li> <li>Generate new data for training</li> </ul>"},{"location":"modeling/Iteration_16_next_steps/","title":"Iteration 16. Plan next steps","text":"<p>02-09-2024</p>"},{"location":"modeling/Iteration_16_next_steps/#goal","title":"Goal","text":"<p>Analyze current progress and plan the next steps.</p>"},{"location":"modeling/Iteration_16_next_steps/#current-progress","title":"Current progress","text":"<p>So far I have been replicating the MindsAI approach. Probably the main difference is that I fine-tune an LLM and I believe they train a model from zero. They have been working in the problem for 2 years, so they have more knowledge and likely more data to train on.</p> <p>In summary the approach is:</p> <ol> <li>Fine-tune an LLM on ARC tasks. I use data augmentation to generate more training tasks.</li> <li>Test-time fine-tuning on the private test set</li> <li>AIRV (Augment, Inference, Reverse augmentation and Vote)</li> </ol> <p>Ensembling this approach with the 2020 baseline I have been able to reach a score of 28 on LB, position 11 at the time of doing the submission. Only 15 people had beaten the baseline during the 3 months of competition.</p> <p>I believe there is room for improvement with the current approach. F.e. I already know that using more training data will likely result in a more accurate model.</p> <p></p>"},{"location":"modeling/Iteration_16_next_steps/#rethinking-the-challenge","title":"Rethinking the challenge","text":""},{"location":"modeling/Iteration_16_next_steps/#representation-is-the-key-to-abstraction","title":"Representation is the key to abstraction","text":"<p>How can we learn from few high-dimensionality data?</p> <p>I believe that the only way to do that is having the right representation of the data, that lies on a smaller dimension manifold that allows to learn from few data.</p> <p>Thus the key is to learn a good representation of the ARC task space. There might be an additional difficulty to find the right representation, f.e. humans typically have to search the right perspective of the data to be able to solve a problem. But I will make the hypothesis that finding the right perspective is a simpler problem, or that we could sample the model to find that perspective.</p> <p></p> <p>If the data has 1 dimension, it is possible to learn (fit a curve) on it</p>"},{"location":"modeling/Iteration_16_next_steps/#how-can-we-learn-a-good-representation-of-the-arc-problems","title":"How can we learn a good representation of the ARC problems?","text":"<p>If we teach the models to do tasks that require a good representation/understanding of the ARC problems it is likely that the models will develop the right representations.</p> <p>Which tasks can be useful to learn good representations?</p> <ul> <li><code>examples + input -&gt; output</code>. This is the current task that the model is learning</li> <li><code>inputs -&gt; input</code>. Generating new inputs requires to understand the distribution of the grids. It could also be done with the outputs, that should also follow some distribution.</li> <li><code>examples -&gt; code</code>. This is the approach used by Ryan Greenblat with GPT-4o</li> <li><code>code + input -&gt; output</code>. This is equivalent to the first task, but instead of giving examples as input, it gives the code definition of the problem.</li> <li><code>code -&gt; inputs</code>. Each input to a task follows some distribution, given a description of the   distribution the model should be able to generate samples of that distribution.</li> <li><code>inputs -&gt; code</code>. We could also do the opposite task, given some inputs write code to generate that distribution.</li> </ul> <p>I have the intuition that if a model learns to do all the tasks in the list will generalize better than a model that only knows how to do one of the tasks. The representation of the ARC grids and problems could be shared among all the tasks.</p> examples + input -&gt; output examples -&gt; code code + input -&gt; output inputs -&gt; input code -&gt; inputs inputs -&gt; code <p>Disentangling the inputs and the outputs is a good way to learn the correct representation, otherwise it's possible to simple memorize the task. This could be done by reusing the same input distribution for different tasks.</p> <p>The code might be just pseudo-code, f.e. functions with good names and interface that are not implemented. There are some problems that require perception to detect the objects. In those cases I can teach the model to use some non implemented function, and on inference if I detect that the model tries to use that function, use the model instead to generate the output instead of relying on code.</p> <p>Having a model that can generate new inputs given a distribution or given code could be very useful to expand the size of the training data.</p>"},{"location":"modeling/Iteration_16_next_steps/#how-can-we-represent-a-task","title":"How can we represent a task","text":"<ul> <li>Examples. This is the format of the ARC challenge.</li> <li>Code. We can use python code to implement the task. Code is an unambiguous way to represent the task.</li> <li>Natural language. We can also use natural language to represent the task, but natural language is ambiguous   and I don't feel is a good approach.</li> </ul> <p>Abstraction: going from examples to code Synthesis: going from code to examples</p> <p>If I write python code to synthesize new examples, I could reuse that code to teach a model to go from examples to code.</p>"},{"location":"modeling/Iteration_16_next_steps/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_16_next_steps/#more-data-augmentation","title":"More data augmentation","text":"<p>I have realized that I can apply even more data augmentation to the training tasks. Currently I'm applying geometric augmentations and color swapping both to the inputs and to the outputs.</p> <p>But I have realized that I can use reversible augmentations (those that preserve the original information) to create new tasks. They will be different tasks but totally valid.</p> <p>This work because we can create new tasks by concatenating different transformations. If we create code to synthesize new tasks, one way to generate a great number of tasks quickly is to compose different transformations to create new tasks. So given an existing ARC task we can create a new one by applying a new transformation.</p> <p>We could apply reversible augmentations on the inputs or in the outputs. F.e. we could apply:</p> <ul> <li>Padding, f.e. adding a black or grey border</li> <li>Reflections</li> <li>Upscaling</li> <li>Geometric transformations</li> </ul> <p>This approach will make the training tasks more difficult. It is likely that using this augmentations will improve the current approach. I could start by simply doing an additional geometric transformation on the tasks randomly.</p>"},{"location":"modeling/Iteration_16_next_steps/#multi-task-approach","title":"Multi-task approach","text":"<p>I would like to test the hypothesis that learning multiple tasks results in a better model. The easiest way to test that is to train a model to both do the ARC challenges and to generate new inputs. This experiment doesn't need new data, so we could test the hypothesis quickly.</p>"},{"location":"modeling/Iteration_16_next_steps/#examples-to-code-approach","title":"Examples to code approach","text":"<p>The next step would be to write python code to solve the training tasks. It should be written carefully to have the smallest domain language possible.</p> <p>The great advantage of using code is that we can verify the solutions. That is a huge advantage over the current approach that generates solutions that can't be verified.</p> <p>Ryan generates around 8k solutions. My guess is that I could generate 1k solutions in the most optimistic setup. Thus fine-tuning is critical to have a higher quality model that doesn't need so many predictions. He uses a 30k token prompt, fine-tuning will remove that necessity. Revision is very important, having access to the output of the program and updating the solution. He used GPT4-o, I don't have access to such a powerful model, but I have the power of fine-tuning an small model.</p>"},{"location":"modeling/Iteration_16_next_steps/#omni-arc-model","title":"Omni-ARC model","text":"<p>The final step would be to train a model to do all the tasks. That would require writing also code to generate the input distributions to the tasks. I believe this approach has a great chance of winning the ARC challenge.</p> <p></p>"},{"location":"modeling/Iteration_16_next_steps/#todo","title":"TODO","text":"<ul> <li> Add images to make the understanding of the ideas easier</li> </ul>"},{"location":"modeling/Iteration_17_external_data/","title":"Iteration 17. Revisit external training data","text":"<p>02-09-2024</p>"},{"location":"modeling/Iteration_17_external_data/#goal","title":"Goal","text":"<p>Add external data to the current training data and see if it improves.</p>"},{"location":"modeling/Iteration_17_external_data/#motivation","title":"Motivation","text":"<p>We have made an experiment showing that the model should scale very well when adding new tasks to training. Additionally we have discovered that validation loss is not a good proxy. Thus we have to revisit the experiments we did with external data.</p>"},{"location":"modeling/Iteration_17_external_data/#development","title":"Development","text":"<p>The baseline will be the model trained for 6k steps just on the original ARC tasks. We will run experiments adding different data and evaluate to see if there is improvement.</p>"},{"location":"modeling/Iteration_17_external_data/#reverse-arc-dataset","title":"Reverse ARC dataset","text":"<p>Let's loop over all the ARC train tasks and collect the task ids of the tasks that could be reversed (going from the output to the input).</p> <p>One way to do that would be to use the website and manually copy the ids of the tasks that could be reversed. However that involves many manual steps. I would rather create a simple jupyter notebook interface to do that. That could be the base for future work in task generation.</p>"},{"location":"modeling/Iteration_17_external_data/#is-inference-deterministic","title":"Is inference deterministic?","text":"<p>I have the feeling that when running inference twice (due to errors or bugs) the results were exactly the same. That was not my intention, so I want to verify it and fix it.</p> <p>On a first step I'm going to run inference two times with the same number of predictions. I have verified that the predictions are exactly the same, thus I'm going to check if I'm fixing the random seed. Inference should be deterministic since the temperature is 0, but I'm doing data augmentation which I would like to be random.</p> <p>VLLM was setting the random seed when creating the LLM, thus I have made a small modification to manually set my seed after that.</p>"},{"location":"modeling/Iteration_17_external_data/#more-external-datasets","title":"More external datasets","text":"<p>External datasets are listed here</p> <ul> <li>ConceptARC looks nice</li> </ul>"},{"location":"modeling/Iteration_17_external_data/#results","title":"Results","text":""},{"location":"modeling/Iteration_17_external_data/#uncertainty-in-the-evaluation","title":"Uncertainty in the evaluation","text":"<p>The evaluation has some uncertainty, and the training process another. I need to characterize both of them so I can take informed decisions. To characterize uncertainty on evaluation I have run the same evaluation without fixing the random seed, 64 predictions for each task were made (as I have been doing lately). I did the same experiment with two different models.</p> experiment accuracy correct_pixels correct_size pass_64 unanswered pass_2 1 2.60% 68.83% 87.36% 14.50% 2.27% 6.63% 2 2.68% 69.30% 87.77% 14.00% 2.17% 9.18% 3 2.68% 68.82% 87.45% 15.00% 2.22% 9.18% 4 2.72% 68.92% 87.53% 17.50% 2.18% 8.67% 5 2.69% 69.17% 87.58% 19.00% 2.23% 6.63% mean 2.67% 69.01% 87.54% 16.00% 2.22% 8.06% std 0.04% 0.22% 0.15% 2.15% 0.04% 1.32% experiment accuracy correct_pixels correct_size pass_64 unanswered pass_2 1 2.57% 68.44% 86.52% 13.00% 2.44% 7.65% 2 2.70% 68.82% 86.81% 18.00% 2.18% 6.63% 3 2.40% 68.48% 86.60% 13.50% 2.39% 7.65% 4 2.56% 68.54% 86.33% 15.50% 2.41% 6.63% 5 2.50% 68.42% 86.36% 15.50% 2.36% 7.65% mean 2.55% 68.54% 86.52% 15.10% 2.35% 7.24% std 0.11% 0.16% 0.20% 1.98% 0.10% 0.56% <ul> <li>It seems that the evaluation has low uncertainty. The difference between runs of the same model are small.</li> <li>The only metric that is noisier is <code>pass_64</code> and <code>pass_2</code>, as expected because there are only 100 tasks, and it aggregates all the predictions for each task.</li> <li><code>pass_2</code> is less noisy, probably because it is the result of voting, and pass_64 aggregates all the tasks and takes the best</li> <li>Thus we can trust this metrics, we could measure small differences between models if the training is reliable</li> </ul>"},{"location":"modeling/Iteration_17_external_data/#variability-on-model-training","title":"Variability on model training","text":""},{"location":"modeling/Iteration_17_external_data/#using-new-partition-700-train-100-val","title":"Using new partition (700 train, 100 val)","text":"experiment accuracy correct_pixels correct_size pass_64 unanswered vote_2 1 2.69% 69.17% 87.58% 19.00% 2.23% 6.63% 2 2.77% 68.72% 86.72% 20.50% 2.21% 8.16% 3 3.55% 69.82% 89.27% 17.50% 2.27% 12.76% mean 3.00% 69.24% 87.85% 19.00% 2.24% 9.18% std 0.48% 0.55% 1.30% 1.50% 0.03% 3.19% experiment accuracy correct_pixels correct_size pass_64 unanswered vote_2 1 2.98% 67.67% 85.41% 16.50% 3.05% 1.02% 2 2.50% 68.42% 86.36% 15.50% 2.36% 7.65% mean 2.74% 68.05% 85.88% 16.00% 2.70% 4.34% std 0.34% 0.52% 0.67% 0.71% 0.49% 4.69% <p>The variability due to model training is much bigger. In fact this variability will likely make the results very difficult to compare unless there is a big difference between them</p> <p>How could I reduce the variability to be able to measure small differences between experiments? I have to take in mind that train loss does not show significative differences between the runs. Thus it does not seem to be a problem with training convergence. I believe the problem is related to the differences between training and validation, it is a generalization issue.</p> <ul> <li>Increasing the validation set will reduce the variability, I could go back to the original train and validation sets.   I have already created an iteration to train models for submission, so in this iteration I could focus   on improving the original validation set. If it works I will use that training configuration and train   on all the data for submission.</li> <li>Training for longer might result in more stable predictions</li> <li>Cyclic learning rates might also improve convergence, but it doesn't seem to be the problem here.</li> <li>If variability is inevitable, the only solution will be to run multiple trainings and average the results.</li> </ul>"},{"location":"modeling/Iteration_17_external_data/#using-original-partition-400-train-400-eval","title":"Using original partition (400 train, 400 eval)","text":"<p>I'm going to run multiple trainings with the original partition and measure the variability of the evaluation.</p> <p>The following table shows the std of 3 runs for different experiments. 32 predictions were made for each task for evaluation.</p> experiment accuracy correct_pixels correct_size pass_n unanswered n vote_1 vote_2 01_baseline_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps_10240msl 0.16% 0.17% 0.17% 0.56% 0.20% 0.00% 0.76% 0.24% 02_RE-ARC_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps_10240msl 0.06% 0.10% 0.07% 0.45% 0.09% 0.00% 0.57% 0.44% 03_MINI-ARC_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps_10240msl 0.12% 0.22% 0.39% 1.23% 0.13% 0.00% 0.13% 0.29% 04_ConceptARC_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps_10240msl 0.21% 0.35% 0.29% 0.89% 0.06% 0.00% 0.32% 0.77% 05_all_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps_10240msl 0.17% 0.25% 0.45% 0.26% 0.02% 0.00% 0.60% 0.38% <p>On average the std for accuracy is 0.14%, it was 0.41% when using the new train-val partition. Thus by changing the validation strategy we have reduced the variability 3 times.</p>"},{"location":"modeling/Iteration_17_external_data/#what-is-the-best-data-for-training","title":"What is the best data for training?","text":"external dataset accuracy correct_pixels correct_size pass_n vote_2 reversed-ARC 2.53% 65.67% 84.65% 12.62% 7.32% ConceptARC 3.36% 66.01% 85.01% 14.29% 8.33% all 3.45% 67.11% 86.29% 14.75% 10.45% - 3.82% 66.21% 84.85% 15.33% 9.70% MINI-ARC 4.02% 66.81% 85.46% 15.29% 10.19% RE-ARC + MINI-ARC + ConceptARC 4.21% 68.09% 87.07% 16.12% 10.61% RE-ARC 4.89% 69.07% 87.99% 18.25% 12.62% RE-ARC + MINI-ARC 5.16% 69.21% 87.72% 18.12% 11.74% <ul> <li>Using this datasets is beneficial: RE-ARC, MINI-ARC</li> <li>Using this datasets worsens the accuracy: ConceptARC, reversed-ARC</li> </ul> <p>When I was creating reversed-ARC I noticed that the difficulty was lower than the original ARC dataset. There were more trivial tasks and the tasks were easier.</p> <p>I find more intriguing that using ConceptARC is not helpful. I should investigate this in the future before creating my own data.</p>"},{"location":"modeling/Iteration_17_external_data/#submission-results","title":"Submission results","text":"<p>Adding RE-ARC to the training datasets improved LB score of a single model from 11 to 14. If I train for longer I get to a score of 16.</p> <p>TODO: what if I add MINI-ARC</p>"},{"location":"modeling/Iteration_17_external_data/#conclusion","title":"Conclusion","text":"<p>To reduce validation metrics variability is better to use the whole evaluation set (instead of using the new partitions that used 700 examples for training and 100 for validation).</p> <p>Using RE-ARC and MINI-ARC improve validation accuracy.</p>"},{"location":"modeling/Iteration_17_external_data/#next-steps","title":"Next steps","text":"<ul> <li>Try to understand why using ConceptARC for training is not helpful.</li> </ul>"},{"location":"modeling/Iteration_17_external_data/#todo","title":"TODO","text":"<ul> <li> Create a dataset with reverse ARC tasks. All tasks that are invertible could be reversed and used for training.</li> <li> Implement a new option on training that uses a dataset without test tasks. It has a configuration with the number of train tasks and randomly makes new tasks selecting train and test samples.</li> <li> Prepare RE-ARC dataset for the new format</li> <li> Once we have the best configuration, try increasing <code>max_seq_len</code>. Some training samples do not fit in 4096</li> <li> Are VLLM results deterministic? Why?</li> <li> What is the uncertainty in my estimation of accuracy? I need to know which differences are significative and which aren't before drawing conclusions.</li> <li> Does the submission improve whe adding MINI-ARC</li> <li> Add submission results to conclusions</li> </ul>"},{"location":"modeling/Iteration_18_submission_models/","title":"Iteration 18. Train models for submission","text":"<p>03-09-2024</p>"},{"location":"modeling/Iteration_18_submission_models/#goal","title":"Goal","text":"<p>Train a set of models with the submission in mind. This implies using all the available data for training and skipping the validation step.</p>"},{"location":"modeling/Iteration_18_submission_models/#motivation","title":"Motivation","text":"<p>It is likely that training in all the available data would bring small improvements on leaderboard. Solving an extra problem could be the difference between winning a prize or not.</p>"},{"location":"modeling/Iteration_18_submission_models/#development","title":"Development","text":"<p>12k training steps could be a good duration when using the original ARC dataset. It is likely that when adding new classes I could train for longer and get better results.</p> <p>Initially I will be using the whole ARC dataset. That way I will have an additional 100 evaluation tasks that could boost the leaderboard score. I will be adding new data once I verify that it is beneficial.</p> <p>Using a higher lora rank than 32 might give better results, but I still have to verify it.</p> <p>Use the bigger <code>max_seq_len</code> possible, because there will be long problems in the test set. I have found that I can use a <code>max_seq_len</code> of 10240 without any problem, and very little slowdown because each prompt is processed independently, so they are not padded.</p>"},{"location":"modeling/Iteration_18_submission_models/#results","title":"Results","text":""},{"location":"modeling/Iteration_18_submission_models/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_18_submission_models/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_18_submission_models/#todo","title":"TODO","text":"<ul> <li> Do we see improvements in LB score when increasing the <code>max_seq_len</code>?</li> <li> Do we see improvements when increasing LoRA rank? 128 better than 32, 512 is better than 128? Whole model?</li> <li> Verify that I can use a 512 lora model in the submission without additional changes</li> <li> Do we see improvements when training with RE-ARC?</li> <li> Is it beneficial to use a bigger number of predictions? F.e. 256</li> </ul>"},{"location":"modeling/Iteration_20_bigger_models/","title":"Iteration 20. Bigger models","text":"<p>04-09-2024</p>"},{"location":"modeling/Iteration_20_bigger_models/#goal","title":"Goal","text":"<p>Study the tendency of using bigger models.</p>"},{"location":"modeling/Iteration_20_bigger_models/#motivation","title":"Motivation","text":"<p>It seems that using bigger models gives better results, I want to dig deeper into that trend.</p>"},{"location":"modeling/Iteration_20_bigger_models/#development","title":"Development","text":"<p>I can fine-tune <code>Qwen2-7B</code> using 2xA6000 GPUs (80GB of VRAM). I have tried fine-tuning <code>Qwen2-72B</code> on 8xA6000 but it gave OOM error. I only could fine-tune when using int4 quantization, but it was terribly slow, 700s per batch.</p>"},{"location":"modeling/Iteration_20_bigger_models/#results","title":"Results","text":""},{"location":"modeling/Iteration_20_bigger_models/#train-on-arc-tasks","title":"Train on ARC tasks","text":"model train steps pass_2 Qwen2-0.5B 6000 7.10% Qwen2-1.5B 6000 12.20% Qwen2-7B 6000 20.40% <p>Clearly the bigger models generalize better for the same number of training steps.</p> <p></p> <p>It seems that the accuracy improves linearly with the log of the parameters. If the trend continues we will get a pass_2 accuracy of 50% if fine-tuning GPT4.</p> parameters pass_2 estimation 0.5 7.10% 1.5 12.58% 7 20.27% 72 31.91% 2000 48.50% Click to see more detailed results  | model      | train steps | train loss | val loss | accuracy | correct_pixels | correct_size | pass_64 | unanswered | pass_2 | |------------|-------------|------------|----------|----------|----------------|--------------|---------|------------|--------| | Qwen2-0.5B | 6000        | 0.053      | 0.16     | 2.80%    | 66.30%         | 84.20%       | 18.50%  | 2.80%      | 7.10%  | | Qwen2-1.5B | 6000        | 0.0302     | 0.154    | 5.30%    | 69.60%         | 87.50%       | 26.00%  | 2.80%      | 12.20% | | Qwen2-7B   | 6000        | 0.0135     | 0.129    | 7.30%    | 71.30%         | 87.70%       | 27.00%  | 3.20%      | 20.40% |  | model      | train steps | train loss | val loss | accuracy | correct_pixels | correct_size | pass_64 | unanswered | pass_2 | |------------|-------------|------------|----------|----------|----------------|--------------|---------|------------|--------| | Qwen2-0.5B | 3000        | 0.075      | 0.157    | 1.60%    | 66.30%         | 85.10%       | 11.50%  | 3.40%      | 7.10%  | | Qwen2-1.5B | 3000        | 0.0505     | 0.138    | 3.70%    | 68.70%         | 86.50%       | 19.50%  | 3.10%      | 11.20% | | Qwen2-7B   | 3000        | 0.026      | 0.114    | 6.70%    | 70.60%         | 87.00%       | 32.00%  | 3.50%      | 17.30% |"},{"location":"modeling/Iteration_20_bigger_models/#test-time-fine-tuning","title":"Test-time fine-tuning","text":"<p>Already done a comparison on grid representation iteration</p>"},{"location":"modeling/Iteration_20_bigger_models/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_20_bigger_models/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_20_bigger_models/#todo","title":"TODO","text":"<ul> <li> Can I make a submission with Qwen2-7B? Could I make test-time fine-tuning using quantization?</li> <li> Would Qwen2-7B improve if training for 12k or 24k steps?</li> <li> Are there competitive models in the 14B-30B ballpark to try?</li> </ul>"},{"location":"modeling/Iteration_21_more_data_augmentation/","title":"Iteration 21. More data augmentation","text":"<p>06-09-2024</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#goal","title":"Goal","text":"<p>Does using more data augmentation improve the accuracy of the model?</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#motivation","title":"Motivation","text":"<p>When planning about the next steps I identified the opportunity of using more data augmentation.</p> <p>The idea is to apply extra transformations independently to the input and/or the output. Previously the same transformation was applied to the input and output.</p> <p>I believe this extra augmentation should be applied only on the first training, when fine-tuning on ARC tasks. This extra augmentation will hopefully induce a better representation in the model. But when we are doing test-time fine-tuning we want to learn the task at hand, so applying extra augmentation isn't likely to be helpful (at least when we have a small compute budget).</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#development","title":"Development","text":""},{"location":"modeling/Iteration_21_more_data_augmentation/#results","title":"Results","text":""},{"location":"modeling/Iteration_21_more_data_augmentation/#general-results","title":"General results","text":"task data augmentation probability accuracy correct_pixels correct_size pass_n vote_2 0% 5.93% 69.74% 87.60% 21.00% 14.02% 25% 6.96% 70.46% 88.27% 21.50% 15.15% 50% 7.02% 70.87% 88.77% 21.62% 13.51% 75% 6.49% 69.82% 88.96% 20.50% 15.03% 100% 6.37% 69.21% 89.06% 20.88% 13.32% <p>All experiments that use task data augmentation improve accuracy over the baseline that does not use it. Probably the best configuration is to use it 50% of the time.</p> <p></p> <p>Train and validation metrics show very clearly the effect of using this new data augmentation: train loss gets worse and validation loss improves.</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#ablation-study","title":"Ablation study","text":"<p>I used 4 types of task data augmentation: geometric, padding, upscale and mirror. Let's remove them one by one and measure the effect.</p> removed data augmentation accuracy correct_pixels correct_size pass_n vote_2 - 6.37% 69.21% 89.06% 20.88% 13.32% geometric 6.91% 69.88% 88.32% 22.88% 15.15% padding 6.43% 69.22% 88.30% 18.62% 12.34% upscale 6.01% 68.54% 88.67% 18.62% 13.01% mirror 5.82% 68.34% 88.75% 18.75% 13.01% <p>Removing mirror and upscale have the greatest drop in performance, while removing geometric transformation results on improved accuracy. I can try new configurations using this knowledge to try to improve the results (<code>1244</code>).</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#tune-augmentation-weights","title":"Tune augmentation weights","text":"task data augmentation probability accuracy correct_pixels correct_size pass_n vote_2 0% 5.93% 69.74% 87.60% 21.00% 14.02% 50% weights=1111 7.02% 70.87% 88.77% 21.62% 13.51% 50% weights=1244 7.09% 71.12% 89.22% 22.38% 15.03% <p>If I tune the weights of the different augmentations it seems that I can update slightly the accuracy of the model. However the improvement is small, maybe the difference is not significative.</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#submission-results","title":"Submission results","text":"<p>The same training with task augmentation scored 19 on LB and without just 14 (v15 vs v17). There is uncertainty on this scores but it seems that the improvement is clear.</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#conclusion","title":"Conclusion","text":"<p>We observed an improvement in model accuracy on validation and also an improvement in leaderboard score when using more data augmentation.</p>"},{"location":"modeling/Iteration_21_more_data_augmentation/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_21_more_data_augmentation/#todo","title":"TODO","text":"<ul> <li> Implement new data transformations, visualize them on a notebook</li> <li> Add parameters to train configuration to allow to control this new data augmentation</li> <li> Run trainings and see if the accuracy improves</li> <li> Submission</li> </ul>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/","title":"Iteration 22. Learning the inputs distribution","text":"<p>10-09-2024</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#goal","title":"Goal","text":"<p>Is it helpful to learn to generate new inputs in addition to learn to solve the tasks?</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#motivation","title":"Motivation","text":"<p>My intuition is that learning a good representation of the input is the key to solve the challenge. The model will learn the representation by doing tasks that require having a good representation.</p> <p>One of those tasks is to generate new inputs for a task. The advantage of this approach is that we don't need to generate new data, we already have it. We just have to make a better use of it.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#development","title":"Development","text":"Click to see all the metrics <pre><code># Verify that inference does not change\nreset; rm -r /mnt/hdd0/Kaggle/arc24/evaluations/20240907_more_data_augmentation/04_100-augmentation-1110_Qwen2-0.5B-Instruct_lr1e-4_r32_12e3steps_10240msl/checkpoint-12000/inference_x008*; python easy_inference_and_evaluation.py /mnt/hdd0/Kaggle/arc24/models/20240907_more_data_augmentation/04_100-augmentation-1110_Qwen2-0.5B-Instruct_lr1e-4_r32_12e3steps_10240msl/checkpoint-12000 --predictions_per_task 8\n\n# Baseline results\naccuracy: 3.2%  correct_pixels: 68.8%   max_correct_pixels: 77.4%       correct_size: 90.1%     any_correct_size: 91.0% pass_n: 9.5%    unanswered: 2.0%\naccuracy: 3.8%  correct_pixels: 69.7%   max_correct_pixels: 74.7%       correct_size: 90.8%     any_correct_size: 92.3% pass_n: 7.7%    unanswered: 1.5%\n\n# Fix tiny difference between train and inference\naccuracy: 3.3%  correct_pixels: 68.9%   max_correct_pixels: 78.2%       correct_size: 90.2%     any_correct_size: 92.0% pass_n: 10.5%   unanswered: 2.0%\naccuracy: 3.8%  correct_pixels: 69.7%   max_correct_pixels: 74.5%       correct_size: 90.8%     any_correct_size: 92.3% pass_n: 7.7%    unanswered: 1.5%\n\n# try with prompts v1 (the improvement is very likely chance, but shows we could train with these shorter prompts)\naccuracy: 3.2%  correct_pixels: 69.0%   max_correct_pixels: 78.2%       correct_size: 90.3%     any_correct_size: 92.0% pass_n: 10.5%   unanswered: 2.0%\naccuracy: 4.3%  correct_pixels: 69.1%   max_correct_pixels: 74.5%       correct_size: 90.3%     any_correct_size: 92.3% pass_n: 8.7%    unanswered: 2.0%\n\n# Verify that fine-tuning works\npython fine-tuning.py \\\n--model_path=Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v0 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-like_datasets/MINI-ARC.json output-from-examples-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_prompt/01_v0 \\\n--max_steps=1 \\\n--logging_steps=1 \\\n--verbose \\\n--no-resume_from_checkpoint\n\npython fine-tuning.py \\\n--model_path=Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-like_datasets/MINI-ARC.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_prompt/02_v1 \\\n--max_steps=1 \\\n--logging_steps=1 \\\n--verbose \\\n--no-resume_from_checkpoint\n\n# this should break because there is no v2 version\npython fine-tuning.py \\\n--model_path=Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json predict-output-v2 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-like_datasets/MINI-ARC.json predict-output-v2 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json predict-output-v2 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_prompt/03_v2 \\\n--max_steps=1 \\\n--logging_steps=1 \\\n--verbose \\\n--no-resume_from_checkpoint\n\npython fine-tuning.py \\\n--model_path=Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-like_datasets/MINI-ARC.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_prompt/03_v2 \\\n--max_steps=1 \\\n--logging_steps=1 \\\n--verbose \\\n--no-resume_from_checkpoint \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json input-from-inputs-v0\n\n\npython fine-tuning.py \\\n--model_path Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json input-from-inputs-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json input-from-inputs-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/01_baseline \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--verbose \\\n--remove_train_samples_to_fit_max_seq_len \\\n--max_seq_len 8192\n\npython fine-tuning.py \\\n--model_path Qwen/Qwen2-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-outputs-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-outputs-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/02_output-from-outputs \\\n--max_steps 1000 \\\n--eval_steps 100 \\\n--logging_steps 10 \\\n--verbose \\\n--remove_train_samples_to_fit_max_seq_len \\\n--max_seq_len 8192\n\npython fine-tuning.py \\\n--model_path Qwen/Qwen2-0.5B-Instruct \\\n--adapter_path /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/01_baseline/checkpoint-1000 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json input-from-inputs-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json input-from-inputs-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/03_input-from-inputs-continuation \\\n--max_steps 3000 \\\n--eval_steps 100 \\\n--logging_steps 10 \\\n--verbose \\\n--remove_train_samples_to_fit_max_seq_len \\\n--max_seq_len 8192\n\npython fine-tuning.py \\\n--model_path Qwen/Qwen2-0.5B-Instruct \\\n--adapter_path /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/03_input-from-inputs-continuation/checkpoint-3000 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json input-from-inputs-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json input-from-inputs-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/04_input-from-inputs-continuation \\\n--max_steps 8000 \\\n--eval_steps 100 \\\n--logging_steps 10 \\\n--verbose \\\n--remove_train_samples_to_fit_max_seq_len \\\n--max_seq_len 8192\n</code></pre>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#weighted-loss-function","title":"Weighted loss function","text":"<ul> <li>https://stackoverflow.com/questions/71581197/what-is-the-loss-function-used-in-trainer-from-the-transformers-library-of-huggi</li> <li>https://chatgpt.com/c/66e022b7-731c-8012-ab70-646a31741fe2</li> <li>https://huggingface.co/docs/transformers/en/main_classes/trainer</li> <li>https://huggingface.co/docs/trl/en/sft_trainer</li> <li>https://github.com/huggingface/transformers/blob/v4.44.2/src/transformers/trainer.py#L3353</li> </ul>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#results","title":"Results","text":""},{"location":"modeling/Iteration_22_learning_inputs_distribution/#newly-generated-inputs","title":"Newly generated inputs","text":"<p>I have done a quick experiments with a model trained for 1k steps. It seemed that need more training time. The images below are from a model trained for 40k steps both to learn the input distribution and to generate outputs.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The generations are not perfect, but they are promising. It might be possible to expand the evaluation set in a similar fashion to RE-ARC but without the need to write new python code.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#is-it-helpful-to-learn-to-predict-inputs","title":"Is it helpful to learn to predict inputs?","text":"<p>This experiment is different than training different datasets. Here we are training the model to do new tasks. We can think of the new tasks as some sort of regularization. Thus I believe that the fair comparison is comparing models that have been trained for the same number of steps on the task of interest (predict the output given examples). This implies that models that were trained on multiple tasks will be trained for a higher number of steps, because some of the steps will be used to learn the other tasks.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#5k-steps-results","title":"5k steps results","text":"new tasks accuracy correct_pixels correct_size pass_n vote_2 - 4.61% 68.54% 87.34% 17.25% 10.73% 5k inputs 5.36% 69.40% 88.38% 19.75% 13.13% 5k inputs, 5k outputs 5.13% 68.34% 87.18% 19.75% 12.37% 2.5k inputs 2.5k outputs 4.68% 68.55% 87.66% 17.38% 11.99% <p>We see a clear improvement when trained for 5k additional steps learning the inputs distribution. Learning the output distribution does not seem to be as helpful as learning the inputs.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#10k-steps-results","title":"10k steps results","text":"new tasks accuracy correct_pixels correct_size pass_n vote_2 - 5.71% 69.61% 88.11% 20.00% 13.51% pretrained on inputs 5.82% 69.46% 87.66% 20.12% 13.89% 10k inputs 6.44% 69.60% 87.25% 22.25% 15.40% <p>When training for 10k steps we see again that using additional 10k steps learning the input distribution is helpful. Using a model pretrained on inputs does not give significative improvements, train metrics show that it learns faster but it seems that the previous knowledge is forget during the training.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#comparison-with-task-augmentation","title":"Comparison with task augmentation","text":"<p>We don't have a direct apples to apples comparison because one experiment was done for 12k steps and the other for 10k steps.</p> training steps (k) lora rank task augmentation train on inputs accuracy correct_pixels correct_size pass_n vote_2 10 32 FALSE FALSE 5.71% 69.61% 88.11% 20.00% 13.51% 12 32 FALSE FALSE 5.93% 69.74% 87.60% 21.00% 14.02% 10 32 FALSE TRUE 6.44% 69.60% 87.25% 22.25% 15.40% 12 32 TRUE FALSE 7.02% 70.87% 88.77% 21.62% 13.51% 10 32 TRUE TRUE 7.10% 70.52% 88.58% 21.88% 14.52% 10 128 TRUE TRUE 8.46% 71.40% 88.55% 25.50% 16.92% <p>But it seems that task augmentation had a bigger effect on accuracy than learning the inputs. Combining both is beneficial, and using a higher lora rank also improves the results.</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#submission-results","title":"Submission results","text":"<p>Submission v18 that didn't learn input distribution scored 15, while submission v19 that learned input distribution scored 18. There is uncertainty in the leaderboard scores but it seems that it improves. TODO:</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#conclusion","title":"Conclusion","text":"<p>We have probed that the model can learn to generate new inputs and that this helps in the main task of solving the ARC puzzles. However the effect seems to be smaller than using task augmentation (see previous iteration).</p>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#next-steps","title":"Next steps","text":"<ul> <li>Create a expanded version of the evaluation set, similar to RE-ARC, but using my model instead of   writing new python code.</li> </ul>"},{"location":"modeling/Iteration_22_learning_inputs_distribution/#todo","title":"TODO","text":"<ul> <li> Refactor prompt code to remove duplications, verify that inference results do not change.</li> <li> Refactor the code to allow using different prompts</li> <li> Update fine-tune script to support a more complex configuration for train data (filepath and prompt)</li> <li> Create prompt template for input prediction</li> <li> Create prompt template for output prediction</li> <li> Quick experiments to validate implementation</li> <li> Long experiments to see if the model improves</li> <li> Visualize some of the new inputs for the typical first training tasks</li> <li> My biggest concern is that the loss might be higher for this task, since it is an open problem.   In the other hand predicting the output was deterministic. The concern had sense, but the results   do not suggest it is a real problem.</li> <li> Update notebook code source with the new code</li> <li> Is it helpful to use the weights of a model trained to predict inputs as a start point</li> <li> Submission results</li> </ul>"},{"location":"modeling/Iteration_23_more_external_data/","title":"Iteration 23. More external data","text":"<p>12-09-2024</p>"},{"location":"modeling/Iteration_23_more_external_data/#goal","title":"Goal","text":"<p>Is there more external data that can improve the accuracy of the model?</p>"},{"location":"modeling/Iteration_23_more_external_data/#motivation","title":"Motivation","text":"<p>Using RE-ARC allowed to improve LB score from 10 to 16. Data is really important.</p> <p>Recently I have noticed that Simon Strandgaard (kaggle) is creating data for ARC challenge. I want to explore that data and also search for additional datasets.</p>"},{"location":"modeling/Iteration_23_more_external_data/#development","title":"Development","text":"<p>So far I have used RE-ARC, ConceptARC, 1D-ARC and MINI-ARC. Only RE-ARC and MINI-ARC showed significative improvements on evaluation metrics.</p>"},{"location":"modeling/Iteration_23_more_external_data/#sources-of-information","title":"Sources of information","text":"<ul> <li>Simon ARC lab (neoeye), apparently host the code to generate datasets for ARC tasks.<ul> <li>Neoeye tama dataset</li> <li>Simon datasets on huggingface</li> <li>Dataset viewer</li> <li>Multiple datasets for ARC<ul> <li>ARC_synthetic_extend At a first glance it seems that they have only changed the colors, this is not useful because we can do it with data augmentation.</li> <li>IPARC. Simon says it is very hard and I agree, I have checked some examples and were hard to understand.</li> <li>PQA: Perceptual Question Answering This dataset looks interesting.</li> <li>ARC Community The tasks are hard to understand.</li> <li>ARC dataset diva The arc-dataset-diva is focused on tiny tasks, where 2 pixels goes in, some transformation happens, and 2 pixels comes out. Probably too small, like the 1D-ARC dataset.</li> <li>dbigham ARC tasks, 21 tasks. some of them have uncertainty on the test sample.</li> <li>synth_riddles I don't like them, I don't understand some of them.</li> </ul> </li> </ul> </li> <li>Small datasets available on Kaggle<ul> <li>nosound's 9 hand crafted ARC tasks</li> <li>Andy Penrose's 5 tasks</li> </ul> </li> <li>ARC Public resources Google Sheet<ul> <li>Language-complete Abstraction and Reasoning Corpus (LARC) I could use this   dataset to test if using language definition of the tasks is useful. A following step would be   to use code.</li> <li>ARC gym: a data generation framework for the Abstraction &amp; Reasoning Corpus</li> </ul> </li> </ul> <p>There is one weird thing, why simon does not have its own data on the viewer?</p>"},{"location":"modeling/Iteration_23_more_external_data/#results","title":"Results","text":""},{"location":"modeling/Iteration_23_more_external_data/#add-more-external-datasets","title":"Add more external datasets","text":"experiment accuracy correct_pixels correct_size pass_32 vote_2 baseline 7.62% 70.89% 88.64% 23.25% 15.91% add new datasets 7.86% 71.21% 88.74% 23.00% 15.66% add neoneye tama 7.46% 71.33% 89.12% 22.50% 15.66% add MINI-ARC 7.62% 71.41% 89.21% 26.62% 17.05% remove neoneye tama 7.38% 71.22% 88.86% 24.75% 16.92% <p>The differences between experiments are small and probably not significative, let's make a brief summary of the added datasets:</p> <ul> <li>neoneye's tama: 50 tasks with 100 variations each</li> <li>PQA: 7 different tasks with lots of variations</li> <li>MINI-ARC: 149 tasks with 4.5 samples per task</li> <li>Kaggle: 14 tasks with 3.8 samples per task</li> </ul>"},{"location":"modeling/Iteration_23_more_external_data/#pretrain-on-datasets-with-more-samples-per-task","title":"Pretrain on datasets with more samples per task","text":"<p>Some of the datasets such as RE-ARC, PQA or neoneye's tama have a lot of samples for each task. Thus it might have sense to first pre-train the model on those datasets and on a second stage use all the available data.</p> experiment accuracy correct_pixels correct_size pass_32 vote_2 pretrain on big datasets + normal train 10.11% 72.79% 89.06% 28.25% 20.20% double length train 9.93% 71.73% 87.80% 26.88% 17.30% <p>We see a small improvement, it might not be significative.</p>"},{"location":"modeling/Iteration_23_more_external_data/#conclusion","title":"Conclusion","text":"<p>We have tried adding new external datasets to train. Results are not conclusive, it is not clear if adding this new datasets improves the validation scores.</p>"},{"location":"modeling/Iteration_23_more_external_data/#next-steps","title":"Next steps","text":"<ul> <li> Language-complete Abstraction and Reasoning Corpus (LARC)</li> </ul>"},{"location":"modeling/Iteration_23_more_external_data/#todo","title":"TODO","text":"<ul> <li> PQA: Perceptual Question Answering<ul> <li> Read the paper</li> <li> The dataset is big, how to deal with it? Can I group all the common tasks together?</li> <li> Check the colors</li> </ul> </li> <li> Visualize Simon datasets https://github.com/neoneye/simon-arc-lab. I have been looking at the code, but I don't see how to decode the datasets. I believe he only works with RLE encoded data.<ul> <li> https://github.com/neoneye/simon-arc-lab/blob/main/simon_arc_lab/rle/deserialize.py</li> </ul> </li> <li> Create a small dataset combining the 2 existing small kaggle datasets</li> <li> ARC gym</li> <li> Could it have sense to pretrain only on the datasets that have a lot of variation like RE-ARC and PQA?</li> <li> Does neoneye tama improve accuracy?</li> </ul>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/","title":"Iteration 24. Overfit to the train set","text":"<p>14-09-2024</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#goal","title":"Goal","text":"<p>If I want to solve the private test set I first have to solve the train set.</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#motivation","title":"Motivation","text":"<p>I have evaluated the the evaluation dataset, which I use to train the models for submission and the accuracy was just 18%, and vote_2 solved only 29.5% of the problems. MindsAI team is solving 46% of the hidden test problems, and I cannot solve more than 30% of the training problems.</p> <p>If I can train a model to learn well the training data, I could use it to generate more training samples like the RE-ARC dataset.</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#development","title":"Development","text":""},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#implement-the-option-to-do-full-model-fine-tuning","title":"Implement the option to do full-model fine-tuning","text":"Click to see bash commands <pre><code>python fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--no-use_lora \\\n--learning_rate 2e-5 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_full_fine-tuning/01_baseline-no-lora_lr2e-5 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--learning_rate 1e-4 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_full_fine-tuning/02_lora-r-32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n</code></pre>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#multi-gpu-training","title":"Multi-gpu training","text":""},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#local-experiments","title":"Local experiments","text":"<p>I'm training models for 100k steps and it seems it's going to take around 100 hours. If I can speedup this time using multiple gpus it would be very helpful to enable faster experimentation.</p> <p>I'm going to do local experiments with one and two gpus to see if I can benefit from faster training.</p> Click to see bash commands <pre><code>export CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--learning_rate 1e-4 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240918_debug_multi-gpu/01_baseline-1-gpu \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=50 \\\n--logging_steps=5 \\\n--random_seed=7 \\\n--n_gpus=1 \\\n--device_map None \\\n--no-verbose \\\n--batch_size=16\n\nunset CUDA_VISIBLE_DEVICES\naccelerate launch fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--learning_rate 1e-4 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240918_debug_multi-gpu/02-2-gpus-accelerate \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=50 \\\n--logging_steps=5 \\\n--random_seed=7 \\\n--n_gpus=2 \\\n--device_map None \\\n--no-verbose \\\n--batch_size=16\n</code></pre> <p>The training loss is exactly the same, but the training time is 89s when using 2 gpus vs 154 seconds when using a single gpu.</p> <p>There were some problems with doble logging when using accelerate that have been solved by updating the code. The same code can still be used with a single gpu without any problem.</p> <ul> <li>There is a logger by accelerate, https://huggingface.co/docs/accelerate/en/package_reference/logging</li> <li>And the problem with wandb is likely my restart implementation. I don't remember why I did that.</li> <li>https://docs.wandb.ai/guides/integrations/accelerate/</li> </ul>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#cluster-experiments","title":"Cluster experiments","text":"<p>I have to:</p> <ul> <li>update the submit script to allow using accelerate</li> <li>see how to configure accelerate (if needed)</li> </ul> <p>I have been unable to train with 4 gpus, it seemed that VRAM memory is not enough. Even when using gradient checkpointing I get OOM errors. Link that says that accelerate uses more VRAM</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#results","title":"Results","text":""},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#start-point-what-is-the-accuracy-of-my-best-models-on-the-train-datasets","title":"Start point: what is the accuracy of my best models on the train datasets?","text":"model dataset accuracy correct_pixels correct_size pass_32 vote_2 v18 training 30.96% 78.82% 90.55% 58.79% 49.29% v19 training 31.68% 78.83% 90.76% 59.13% 50.38% v18 evaluation 17.05% 76.69% 90.57% 40.88% 29.80% v19 evaluation 17.91% 76.76% 90.53% 38.25% 29.55% <ul> <li>Accuracy of the models is very similar, despite v19 being trained for double steps (although it was trained also in the task of generating inputs)</li> <li>Train accuracy is clearly higher than evaluation accuracy. This could be because we are using RE-ARC   for training and also because the training dataset is easier.</li> <li>If I want to save time in the task of overfitting, I should probably just train on the task of predicting outputs. Then once   I have discovered the secret of overfitting I could train again with multiple tasks.</li> <li>I believe this metrics shows that either we are using underparametrized models or we are undertraining.   We should be able to learn to solve all the training tasks.</li> </ul>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#increase-model-capacity-and-train-duration","title":"Increase model capacity and train duration","text":"<p>We are exploring two new axis in this experiment: increase the model capacity and train for much longer.</p> <p>After increasing the model capacity and training 5 times longer, still the model does not overfit to the train dataset.</p> <p></p> <p>We can see that the metrics improve when increasing the training duration, but the rate of improvement is slow. There is no sign of stopping, so it is possible that training for 1e6 steps will bring additional improvements.</p> <p>Also it is unclear if using full fine-tune is better than using LoRA, depends on the train duration.</p> <p>Let's try to enumerate the facts:</p> <ul> <li>Training the model for longer improves the accuracy on the training data, but the rate of improvement   is surprisingly slow.</li> <li>We have trained a model for 100k steps on a dataset of 1221 tasks. We used batch size 16 so the model   saw a total of 1.6M of samples during training. Each task was seen 1300 times, but considering that   we use task augmentation each original task was seen 650 times. During training we randomly swap   between the... Aha moment where I discovered the bug, go down to conclusions</li> </ul>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#fails-on-very-simple-tasks","title":"Fails on very simple tasks","text":"<p>F.e. on 1cf80156, 0b148d64, 1f85a75f... Maybe what they have in common is that the input is \"big\".</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#1cf80156","title":"1cf80156","text":"<p>This tasks is really simple, just crop the object. But it does not do it correctly.</p> <p></p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#0b148d64","title":"0b148d64","text":"<p>Surprisingly it adds an extra black line, or removes part of the object...</p> <p></p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#1f85a75f","title":"1f85a75f","text":"<p>Another pretty simple crop task where it fails to recreate the object.</p> <p></p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#does-changing-the-temperature-improve-the-accuracy","title":"Does changing the temperature improve the accuracy?","text":"<p>Maybe using temperature 0 could be preventing to find the right solution, I'm going to try using different temperatures.</p> temperature accuracy pass_32 vote_2 0.0 47.53% 76.00% 68.42% 0.1 47.64% 75.50% 67.92% 0.2 47.48% 76.25% 67.42% 0.5 47.25% 76.50% 68.92% 1.0 46.17% 76.50% 69.92% <p>The temperature almost does not affect the predictions on the training set. So clearly this is not the problem.</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#check-prompts","title":"Check prompts","text":"<p>Let's check some sample prompts of the easy tasks that the model is failing: 1cf80156, 0b148d64, 1f85a75f</p> <p>I don't see any problem with the prompts or the responses. My guess is that the model has trouble when working with big inputs, f.e. a 15x17 grid. It consistently fails to predict the correct output shape, and then it does what it can to create an output with the wrong shape.</p> <p>I could go back to a simpler encoder that did not predict the output shape, but the model has to make the decision of what the output shape is. Delaying that decision to the drawing of the grid does not seem like a better option, because in that case it has to draw the correct pixels and the shape at the same time.</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#conclusion","title":"Conclusion","text":"<p>I have found a bug in data augmentation that resulted on not using any of the test samples for training. The surprisingly low accuracy on the training datasets arises because we evaluate on the test samples from the train dataset but we were not training on those samples. So the model was not able to generalize to new samples from the task, or the model could not learn the correct task from fewer samples.</p> <p>This also explains why the validation loss diverged despite training on the validation dataset.</p> <p>Training for longer improves the results, there is no sign of stopping in this trend.</p>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#next-steps","title":"Next steps","text":"<ul> <li>Maybe I should train a smaller lora when doing test-time fine-tuning</li> <li>Fix the data augmentation bug</li> </ul>"},{"location":"modeling/Iteration_24_overfit_to_the_train_set/#todo","title":"TODO","text":"<ul> <li> Modify the evaluation script to give better names for the output files. This will allow me to evaluate   both the train and evaluation datasets without overwriting the files.</li> <li> Check if I can do a full fine-tuning instead of LoRA</li> <li> Can I speedup the training by using multiple gpus or unsloth? Yes, with multiple gpus</li> <li> Verify the data augmentation bug: it seems I was never using the test sample as test!! If that is true relaunch all training duration experiments.</li> </ul>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/","title":"Iteration 25. Decouple fine-tuning and test-time fine-tuning","text":"<p>15-09-2024</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#goal","title":"Goal","text":"<p>Can I improve the LB score by decoupling fine-tuning and test-time fine-tuning?</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#motivation","title":"Motivation","text":"<p>My current approach uses LoRA to fine-tune an LLM to do ARC tasks. Then at test-time I fine-tune the same LoRA to adapt to the test tasks.</p> <p>It might be the case that using the same lora rank for both stages is not optimal. On the first training we use around 1k tasks and the training is very long. On the second step we have just 100 tasks and the training is very short. Moreover it seems that fine-tuning the model for each task independently might be the better option. It is unlikely that we need the same capacity to learn 1k tasks (training) as to learn 1 task (test).</p> <p>Moreover I might try to do a full fine-tuning of the model, and in that case I would need to do a different test-time fine-tuning. That makes worth to investigate the option of decoupling fine-tuning and test-time fine-tuning.</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#development","title":"Development","text":""},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#lora-documentation","title":"LoRA documentation","text":"<p>If I want to adapt to each task, maybe it has more sense to train a small LoRA for each task instead of retraining the whole LoRA with r=128.</p> <ul> <li>https://huggingface.co/docs/peft/main/en/developer_guides/lora#pissa</li> <li>https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig Maybe <code>pissa_niter_16</code> is a good option?</li> </ul>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#local-experiment-design","title":"Local experiment design","text":"<p>I'm going to fine-tune a model on a single task or a few tasks. I will try different LoRa initializations to see the effect.</p> Click to see bash commands <pre><code>python merge_lora.py --base_model_path /home/gbarbadillo/data/Qwen2-0.5B-Instruct --lora_path /mnt/hdd0/Kaggle/arc24/models/20240910_predict_inputs/10_task-augmentation-and-input-from-inputs-v0_Qwen2-0.5B-Instruct_lr1e-4_r128_2e4steps_10240msl/checkpoint-20000 --output_path /home/gbarbadillo/data/Qwen2-0.5B-arc\n\njq 'to_entries | .[:5] | from_entries' /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1.json &gt; /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json\n\npython fine-tuning.py \\\n--model_path=Qwen/Qwen2-0.5B-Instruct \\\n--adapter_path /mnt/hdd0/Kaggle/arc24/models/20240910_predict_inputs/10_task-augmentation-and-input-from-inputs-v0_Qwen2-0.5B-Instruct_lr1e-4_r128_2e4steps_10240msl/checkpoint-20000 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/01_baseline-from-adapter \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/02_LoRA-32-default-initialization \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--lora_weight_initialization pissa \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/03_LoRA-32-pissa \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 32 \\\n--lora_weight_initialization default \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/03_LoRA-32-default \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 8 \\\n--lora_weight_initialization default \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/03_LoRA-08-default \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 2 \\\n--lora_weight_initialization default \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/03_LoRA-02-default \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 4e-5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 2 \\\n--lora_weight_initialization default \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/04_LoRA-02-default_lr1e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5 \\\n--learning_rate 1e-4\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 2 \\\n--lora_weight_initialization default \\\n--learning_rate 2e-4 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/04_LoRA-02-default_lr2e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 1 \\\n--lora_weight_initialization default \\\n--learning_rate 2e-4 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/04_LoRA-01-default_lr2e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 1 \\\n--lora_weight_initialization default \\\n--learning_rate 4e-4 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/04_LoRA-01-default_lr4e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 1 \\\n--lora_weight_initialization pissa \\\n--learning_rate 4e-4 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/05_LoRA-01-pissa_lr4e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n\npython fine-tuning.py \\\n--model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc \\\n--lora_r 1 \\\n--lora_weight_initialization olora \\\n--learning_rate 4e-4 \\\n--warmup_ratio 0.1 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240915_debug_LoRA_initialization/05_LoRA-01-olora_lr4e-4 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7_n-1_small.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps=500 \\\n--logging_steps=10 \\\n--random_seed=7 \\\n--batch_size=5\n</code></pre>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#kaggle-experiment-design","title":"Kaggle experiment design","text":"<p>My idea is to run a few trainings in a few validation tasks with different LoRA configurations. All the trainings will use batch size 1 by default. I will have a look at the training metrics and also to the evaluation.</p> <p>TODO: I would like to visualize the training loss in a plot</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#results","title":"Results","text":""},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#weight-initialization","title":"Weight initialization","text":"<p>I have tried <code>olora</code> and <code>pissa</code> weight initializations and I haven't observed improvements in training convergence over the default initialization. In fact I have seen gradient explosions. Thus I recommend to keep using the default initialization.</p> <p>Wandb experiments</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#using-a-new-lora","title":"Using a new LoRA","text":"<p>I have verified both locally and on Kaggle that I can reach the same training loss with a new LoRA than simply retraining the base LoRA. Moreover I have been able to reach similar losses using a much smaller LoRA rank (1 vs 32).</p> <p>I have made 9 submissions with different lora ranks and learning rates without being able to reach the same LB score as the baseline. Maybe when using a new LoRA we need to train for longer, one way of achieving that would be to do a common warmup using all the data, then fine-tune for each task independently.</p> <p>All the submissions shown below use linear learning rate schedule, batch size 1 and <code>qwen2-0.5b/19</code> (this model uses a LoRA rank of 128).</p> lora_r batch size learning rate LB score 128 1 2E-05 20 1 1 5E-05 16 1 1 1E-04 17 1 1 2E-04 16 32 1 2E-05 17 32 1 4E-05 18 32 1 8E-05 14 128 1 1E-05 19 128 1 2E-05 18 128 1 4E-05 16 <p>None of the experiments achieves the same LB score as the baseline that directly modifies the original LoRA. Results are close but not equal.</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#using-a-new-lora-with-warmup","title":"Using a new LoRA with warmup","text":"<p>I made two submissions that scored 15 on leaderboard. Link1 Link2</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#validation-results","title":"Validation results","text":"<p>TODO: can I reach the same results as the baseline?</p> <p>TODO: what if I use my best model instead of the old <code>qwen2-0.5b-instruct/5</code>?</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#submission-results","title":"Submission results","text":"<p>The best results so far have been obtained using a full fine-tuned model and training new LoRAs for each test task.</p>"},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_25_decouple_ft_and_ttft/#todo","title":"TODO","text":"<ul> <li> Create a notebook to do experiments: https://www.kaggle.com/code/ironbar/v2-single-task-test-time-fine-tuning-for-arc24?scriptVersionId=196655009<ul> <li> Add functionality to visualize training loss, that will allow to compare the different configurations</li> </ul> </li> <li> Add functionality to train script to select LoRA initialization</li> <li> Run local experiments to understand the effect of LoRA initialization</li> <li> Can I get the same or better results when using a new LoRA for test-time fine-tuning?</li> <li> Maybe warming up the adapter by training in all the tasks could be useful. F.e. train with bs=16 for 100 steps.</li> </ul>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/","title":"Iteration 26. Dawn of Omni-ARC","text":"<p>16-09-2024</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#goal","title":"Goal","text":"<ul> <li>Create a new dataset with python code that solves the ARC training tasks.</li> <li>Create a set of primitive functions that are used to solve many tasks (DSL)</li> <li>Create new tasks inspired by the training tasks and using the primitive functions</li> <li>Learn more about the ARC tasks</li> </ul>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#motivation","title":"Motivation","text":"<p>To learn a good representation of the data I believe that I should take the Omni-ARC approach: learn many tasks related to ARC. The most promising task would be to write code that implements the ARC tasks. It will allow to verify if it runs on the training samples giving a great advantage over voting.</p> <p>This is going to be a very experimental iteration, thus I have many goals because I'm not sure what will be the optimal approach to do this.</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#development","title":"Development","text":"<ul> <li>The code should be able to cope with data augmentation, and I should test that</li> <li>I'm going to create new modules to store the code. The library <code>inspect</code> is useful to retrieve the source code.</li> </ul>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#thoughts-when-implementing-the-tasks","title":"Thoughts when implementing the tasks","text":"<ul> <li>I need to implement an object class with properties such as size, position and methods such as is_line, is_rectangle, move...</li> <li>Sometimes we need to ignore the colors when dealing with objects, other times we have to   consider that different colors implies different objects</li> <li>After visualizing 50 tasks it seems that I can write easily code for 30 of them, so 60%</li> <li>There are many growing patterns that are probably better defined by example than by code.</li> <li>Many tasks involve grids</li> <li>If I make the assumption that grids are numpy arrays code will be simpler, I would simply have to write a wrapper around the tasks to be convert to list again</li> <li>Applying data augmentation to the source code might be more difficult than I believe. For example on <code>task_007bbfb7</code> there are some axis used that should not change. However <code>task_0520fde</code> uses axis and should be changed if there are rotations. Maybe I can solve it simply by adding some special comments that will be removed afterwards.</li> <li>Some implementations like <code>task_05269061</code> are hard to modify for geometric augmentation</li> </ul>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#specification","title":"Specification","text":"<p>I want to write a python library that generates Abstraction and Reasoning Corpus (ARC) tasks. A task will have some input grids, the python code that does the transformation of the task and some output grids. Given this data a model could learn to generate the code given the inputs and the outputs.</p> <p>This are the main ideas behind the library:</p> <ul> <li>Focus on running code. At first I thought about writing pseudocode and using non-implemented functions.   But now I realize that the greatest value from this approach is being able solve the tasks using python   code. Thus I need running code. Moreover having running code enables to do automated testing, and   we don't have to deal with data augmentation issues. The data augmentation will be applied just   to the inputs, not to the function.</li> <li>As many different tasks as possible. Experiments suggest that is better to have many different tasks   than to have many samples from the same task.</li> <li>High quality tested code. Having clean code with the right level of abstraction will be crucial   to be agile and speedup the writing of new tasks. Using automated tests will enable refactoring and   agile development.</li> <li>Small but flexible domain specific language. This is much easier to maintain and expand that having   a ton of specific functions. Moreover it will be easier to learn for the model if the number of   primitive functions is small.</li> </ul>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#task-components","title":"Task components","text":"<p>A task will always have the following components:</p> <ul> <li>Input grids</li> <li>Output grids</li> <li>Code</li> </ul> <p>There are infinite ways to implement the task generator, but the output should follow the definition above.</p> <p>The tasks are implemented using python dict. The grids are lists and the code will simply be a string.</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#implementation-requirements","title":"Implementation requirements","text":"<p>The input grids for a task could come from an already created dataset, or there could be a function that generates new input grids. When defining/implementing a new task we should specify what the input is.</p> <p>The input grids might undergo some data augmentation until they become the final inputs for the task. This should also be specified when defining the task. This is more likely to happen when we are not using a generator function, if the inputs come from a dataset using data augmentation will increase the variability of the samples.</p> <p>There might be some python function that implements the core task, but we can create new tasks by composing multiple tasks. This should be specified when defining the task, some kind of pipeline of functions. But to train the model we need to have all the code combined in a single function. So we need some method to take a pipeline of functions and create a single function with all the code from those functions. Ideally we could create multiple tasks very fast by specifying some core task and listing all the possible tasks that could be used to compose new tasks.</p> <p>There would be some task register where I store all the created tasks, that can be used later to generate training data.</p> <p>Internally I will work with numpy arrays, that makes the code easier. A wrapper will convert the list to array at start and viceversa at the end.</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#new-repo","title":"New repo","text":"<p>I have decided that I have to create a new repo to host all the code to generate tasks with code. A good name for the repo would be omni-arc, since it will enable the training of the omni-arc models.</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#conclusion","title":"Conclusion","text":"<p>Following work will be done at: https://github.com/ironbar/omni-arc</p>"},{"location":"modeling/Iteration_26_dawn_of_omni_arc/#todo","title":"TODO","text":"<ul> <li> Have a look at RE-ARC, maybe I can reuse it.</li> <li> How difficult is to be data augmentation resistant? If it is difficult I should better concentrate on creating new tasks.</li> </ul>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/","title":"Iteration 27. Fix data augmentation bug","text":"<p>22-09-2024</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#goal","title":"Goal","text":"<p>Fix the data augmentation bug and see how the results improve after the fix.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#motivation","title":"Motivation","text":"<p>On Iteration 24: Overfit to the train set I discovered that I was always swapping the train and test samples, thus I was never using the test samples for training.</p> <p>This reduces the training data, thus it's very likely that after fixing the bug the resulting trained models will be stronger.</p> <p>I have also realized that this improvement is likely to be bigger in test-time fine-tuning because we are training with n-1 samples per task. Losing one sample per task is more critical when the tasks have less samples to learn from. So hopefully I will see a noticeable improvement on the accuracy of the test-time fine-tuned model.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#development","title":"Development","text":""},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#results","title":"Results","text":""},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#training-results","title":"Training results","text":"<p>I have repeated the experiments to overfit to the train set.</p> <p></p> <p>These results are much better after fixing the bug in data augmentation. It is likely that training for 200k steps would result in completely learning the training data.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#test-time-fine-tuning-results","title":"Test-time fine-tuning results","text":"<p>I have to repeat some test-time fine-tuning experiments and see if the validation score improves. This is the baseline experiment and this is the repetition after the bugfix.</p> bugfix accuracy pass_64 vote_2 FALSE 12.50% 37.00% 21.9% TRUE 13.70% 35.50% 32.10% <p>We see an improvement in accuracy and a dramatic improvement in vote_2 metric. After the bugfix the drop between pass_64 and vote_2 is very small.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#submission-results","title":"Submission results","text":"submission Single LB score Ensemble LB score with bug 21 33 after bugfix 25 35 <p>Great improvement after fixing the bug on the submission, which uses test-time fine-tuning. Those experiments were done with v2-single-task-test-time-fine-tuning notebook.</p> <p>Another experiment with single-task-test-time-fine-tuning notebook improved the ensemble score from 32 to 34.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#overfit-to-train-and-learn-the-input-distribution","title":"Overfit to train and learn the input distribution","text":"<p>All the curves have the same tendency, wether they were trained to learn the input distribution or not. Thus this probes that we can learn the two tasks without any problem, even when the task losses are very different.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#conclusion","title":"Conclusion","text":"<p>After fixing the data augmentation bug I can completely learn the train dataset. I have also probed that I can learn at the same time the input distribution and to solve ARC tasks.</p> <p>Moreover I observed huge improvements in validation score when doing test-time fine-tuning.</p> <p>On leaderboard we have seen improvements, but not as big as the seen on validation.</p>"},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_27_fix_data_augmentation_bug/#todo","title":"TODO","text":"<ul> <li> Can I overfit to the train set and still learn the inputs distribution? Yes</li> <li> Design an experiment to verify the improvement on test-time fine-tuning</li> <li> Update with the results of the fine-tuned LoRA.</li> </ul>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/","title":"Iteration 28. Optimal train duration and capacity","text":"<p>22-09-2024</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#goal","title":"Goal","text":"<p>What is the optimal train duration and capacity to improve the validation score? And for the leaderboard score?</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#motivation","title":"Motivation","text":"<p>Previous experiments suggest that training for longer might improve generalization. I want to validate that on the evaluation set.</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#results","title":"Results","text":""},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#optimal-train-duration-for-full-fine-tune","title":"Optimal train duration for full fine-tune","text":"<p>All metrics improve when training for longer, we don't see signs of plateau on this experiment. Continual training might be a good option for this challenge.</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#does-lora-achieve-better-generalization","title":"Does LoRA achieve better generalization?","text":"<p>These experiments are for 40k steps, batch size 16.</p> experiment accuracy pass_32 vote_2 vote_1 full fine-tuning 12.25% 31.13% 22.85% 18.69% LoRA 32 11.10% 30.25% 22.85% 19.07% LoRA 128 12.73% 32.25% 22.47% 19.19% <p>There is no evidence that full fine-tuning generalizes better than LoRA.  Moreover considering that so far using a pretrained LoRA for TTFT gives better results it is likely that training a LoRA will give better results</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#conclusion","title":"Conclusion","text":"<p>We have verified that training for longer improves generalization. No sign of plateau was observed after training for 80k steps. Thus it's likely that when training to do two tasks we could train further than 160k steps and still see improvements.</p> <p>So far there is no evidence that full fine-tuning is better than LoRA. I need to validate this on the leaderboard.</p>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#next-steps","title":"Next steps","text":"<ul> <li>Train new models for submission: full fine-tuning and also LoRA and compare the results on the LB</li> </ul>"},{"location":"modeling/Iteration_28_optimal_train_duration_and_capacity/#todo","title":"TODO","text":"<ul> <li> Determine the optimal training steps for a full model fine-tune</li> <li> Do the same for different LoRAs</li> <li> Does the optimal training steps double when also learning to predict input from inputs?</li> <li> Do the results hold for the leaderboard? How to account for the extra training data?</li> </ul>"},{"location":"modeling/Iteration_29_qwen25/","title":"Iteration 29. Qwen 2.5","text":"<p>25-09-2024</p>"},{"location":"modeling/Iteration_29_qwen25/#goal","title":"Goal","text":"<p>A new release of Qwen was announced yesterday: Qwen 2.5, does it improve the accuracy on ARC?</p>"},{"location":"modeling/Iteration_29_qwen25/#motivation","title":"Motivation","text":"<p>Simply swapping the model might bring improvements for free!</p>"},{"location":"modeling/Iteration_29_qwen25/#development","title":"Development","text":"<ul> <li>https://qwenlm.github.io/blog/qwen2.5-llm/</li> <li>https://qwenlm.github.io/blog/qwen2.5-llm/#qwen25-05b15b3b-performance</li> <li>https://qwenlm.github.io/blog/qwen2.5-llm/#qwen25-05b15b-instruct-performance</li> </ul> <p>The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of 18 trillion tokens.</p> <p>Whe looking at benchmarks we see a noticeable improvement between Qwen 2 and 2.5.</p>"},{"location":"modeling/Iteration_29_qwen25/#results","title":"Results","text":""},{"location":"modeling/Iteration_29_qwen25/#validation-results-when-training-different-models-for-10k-steps","title":"Validation results when training different models for 10k steps","text":"model accuracy pass_32 vote_2 Qwen2-0.5B 8.24% 26.50% 15.91% Qwen2-0.5B-Instruct 8.25% 26.75% 15.91% Qwen2.5-0.5B 9.37% 26.75% 18.31% Qwen2.5-0.5B-Instruct 8.98% 26.00% 17.93% <p>Both versions of <code>Qwen2.5</code> achieve better results than <code>Qwen2-0.5B-Instruct</code>.</p>"},{"location":"modeling/Iteration_29_qwen25/#why-models-that-arent-instruct-are-slower","title":"Why models that aren't instruct are slower?","text":"<p>Inference with Qwen2.5B took 4920.6132 seconds, compared to the typical 1809.6193.  Why?</p> <p>Inspecting the responses I have found that the non-instruct versions do repeat the prediction multiple times. E.g.</p> <pre><code>&lt;|im_start|&gt;assistant\n### Output\n\n```grid shape: 6x6\n1 595959\n2 181818\n3 959595\n4 818181\n5 595959\n6 181818\n```\nAssistant\n### Output\n\n```grid shape: 6x6\n1 595959\n2 181818\n3 959595\n4 818181\n5 595959\n6 181818\n```\nAssistant\n...\n</code></pre>"},{"location":"modeling/Iteration_29_qwen25/#conclusion","title":"Conclusion","text":"<p>We have observed improvements when replacing Qwen2 by Qwen2.5. The most promising model is the non-instruct version but there is a problem at inference: it does not stop predicting. Until that problem is not solved I will use Qwen2.5</p>"},{"location":"modeling/Iteration_29_qwen25/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_29_qwen25/#todo","title":"TODO","text":"<ul> <li> Do the same experiment just changing the base model and compare the validation results</li> <li> Could I use Qwen-2.5-0.5B and avoid having repetitions in the prediction?<ul> <li> Check the training data.</li> <li> Check the tokenizer</li> <li> Local experiments</li> <li> Maybe adding some new stopword to VLLM could be a quick fix</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_30_optimal_number_predictions/","title":"Iteration 30. Optimal number of predictions","text":"<p>24-09-2024</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#goal","title":"Goal","text":"<p>What is the optimal number of predictions in a submission?</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#motivation","title":"Motivation","text":"<p>If I can reduce the number of predictions I might increase the test-time fine-tuning duration.</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#development","title":"Development","text":"<p>The experiment is simple: I will take a few models and make inference with them with a different number of predictions. Finally I will plot the accuracy vs the number of predictions.</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#results","title":"Results","text":""},{"location":"modeling/Iteration_30_optimal_number_predictions/#experimental-results","title":"Experimental results","text":"<p>The analysis of the data is not easy: for <code>pass_n</code> it is very clear that it increases when the number of predictions increases. But for the other metrics is not clear, it seems that increasing the number of predictions has a positive effect, but it is small and the metrics are noisy.</p> <p>The following table shows the number of predictions that got the best value for each metric.</p> training steps pass_n vote_2 vote_1 5000 48 48 64 10000 48 24 24 20000 64 32 48 40000 48 64 16 80000 64 24 48 <p>On <code>pass_n</code> it is clear that all the experiments get the best score on either 48 or 64 which are the biggest number of predictions tested. But we see greater variance in the vote metrics. I believe we can say that 8 predictions is not enough, but maybe 32 is enough and increasing the number of predictions does not lead to higher accuracy.</p> <p>Idea: we don't want to generalize, we want to maximize LB score. A smaller number of predictions will increase volatility and the chance of a lucky submission.</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#simulated-results","title":"Simulated results","text":"<p>I could take the results of all the previous experiments, and simulate new inferences by sampling. That way I could model the distribution of expected scores. Moreover I will have a total of 192 predictions, so I could extend the analysis to a higher number of predictions.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>We see that on average increasing the number of predictions has a positive effect in all the metrics. It is unclear that using a smaller number of predictions can lead to better and lucky results.</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#conclusion","title":"Conclusion","text":"<p>We have verified that increasing the number of predictions has a positive effect in all the relevant metrics. Thus reducing the number of predictions to increase the duration of test-time fine-tuning is a compromise that should be studied for each case.</p>"},{"location":"modeling/Iteration_30_optimal_number_predictions/#next-steps","title":"Next steps","text":"<ul> <li>Early stopping on TTFT will also free time to extend the training duration</li> <li>It's likely that I can create more efficient training and inference scripts for submission, where I have   to train and make inference with 100 models (instead of 1 that was the original use case)</li> </ul>"},{"location":"modeling/Iteration_31_train_submission_models/","title":"Iteration 31. Train new submission models","text":"<p>25-09-2024</p>"},{"location":"modeling/Iteration_31_train_submission_models/#goal","title":"Goal","text":"<p>Use all the learnings from the previous iterations to train a new generation of submission models.</p> <p>Do we get better scores in the leaderboard just for training for longer?</p>"},{"location":"modeling/Iteration_31_train_submission_models/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_31_train_submission_models/#development","title":"Development","text":"<p>This is the setup that should give the best results</p> <ul> <li>Qwen2.5-0.5B-Instruct, I'm using the instruct version because of the problems with the non-instruct version   on inference.</li> <li>Train as long as possible, do continual training (train the same model multiple times). I will first train for 40k steps, then maybe for 80k and continue with the retrainings until we don't see improvements on LB</li> <li>All datasets</li> <li>2 tasks</li> <li>LoRA seems to generalize better, verify it on the leaderboard</li> </ul>"},{"location":"modeling/Iteration_31_train_submission_models/#resuming-the-trainings-is-problematic","title":"Resuming the trainings is problematic","text":"<p>I have found that resuming multi-gpu trainings can be problematic. It seems that the script uses the generator to generate samples until the checkpoint and that takes a lot of time and many times it crashes. If it happens again I should take a look at the code and configuration of the training.</p> <p>I have added a parameter <code>ignore_data_skip</code> that should speedup the training startup, but then I believe the problem is related to GPU VRAM. The two trainings that could not be resumed where the full fine-tuning and the bigger LoRA that used a rank of 128. Those two experiments are the ones that use more VRAM memory. The training appears to start much faster but fails without a clear error message.</p>"},{"location":"modeling/Iteration_31_train_submission_models/#weirdly-low-submission-scores","title":"Weirdly low submission scores","text":"<p>When doing submissions with my best model <code>qwen25-0.5b/8</code> so far some of the submissions score very low (considering that the best submission got 40 on LB)</p> <p>I believe that the most plausible explanation for a successful submission with low score is that the fine-tuning failed, thus the base model was used. I'm going to try to reduce the <code>max_seq_len</code>. Inference should not be affected because I merge the models, so the problem must be on fine-tuning. And the problem has been manifested only on LoRA with 128 rank, which demands more memory.</p> <p>I have run the notebook on 10 validation samples and there is no sign of problems: notebook</p> <p>Maybe doing first the trainings with smaller datasets can alleviate the problem: notebook</p> <p>I have downloaded the model and verified that the md5sum of the files is correct. So corruption of the files does not seem to be the problem.</p> <p>I have repeated the submission and it scored 37, so maybe there is nothing wrong.</p>"},{"location":"modeling/Iteration_31_train_submission_models/#results","title":"Results","text":"<p>Wandb dashboard</p>"},{"location":"modeling/Iteration_31_train_submission_models/#leaderboard-results","title":"Leaderboard results","text":"<p>By training the models for longer I have been able to improve the LB score from 36 to 40. Learnings:</p> <ul> <li>Models trained with LoRA score higher than full fine-tuning. (37 vs 40). My guess is that it is better   for test-time fine-tuning to fine-tune an already trained LoRA than to fine-tune a new LoRA.</li> <li>LoRA with rank 32 scores lower than rank 64 or 128 (37 vs 40)</li> <li>Training for 400k steps did not show improvements over training for 200k steps.</li> </ul>"},{"location":"modeling/Iteration_31_train_submission_models/#validation-experiments-about-learning-rate-and-lora-rank-for-test-time-fine-tuning-a-new-lora","title":"Validation experiments about learning rate and LoRA rank for test-time fine-tuning a new LoRA","text":""},{"location":"modeling/Iteration_31_train_submission_models/#conclusion","title":"Conclusion","text":"<p>On this iteration we have improved the LB score from 36 to 40, reaching the second position in the challenge.</p> <p>The best results are obtained using LoRA with rank 64 or 128 for 200k steps.</p>"},{"location":"modeling/Iteration_31_train_submission_models/#next-steps","title":"Next steps","text":"<ul> <li>Single training for 200k steps</li> <li>Could I make it work with the non-instruct Qwen version?</li> </ul>"},{"location":"modeling/Iteration_31_train_submission_models/#todo","title":"TODO","text":"<ul> <li> Using small LoRAs might open the door again to use 4 gpus for training. If training for longer is the key, this would be helpful.</li> <li> What is the best learning rate for TTFT? It might change with the lora rank</li> <li> Understand the recent weird low scores on <code>qwen25-0.5b/8</code></li> </ul>"},{"location":"modeling/Iteration_32_llama_32/","title":"Iteration 32. Llama 3.2","text":"<p>26-09-2024</p>"},{"location":"modeling/Iteration_32_llama_32/#goal","title":"Goal","text":"<p>Try the new Llama 3.2 1B model</p>"},{"location":"modeling/Iteration_32_llama_32/#motivation","title":"Motivation","text":"<p>Meta has released a new Llama 3.2 1B model. The size is twice the size of Qwen-0.5B but I believe it is worth trying.</p>"},{"location":"modeling/Iteration_32_llama_32/#development","title":"Development","text":"<p>I have to explore what is the best way to encode the grids for this Llama model.</p> <ul> <li>https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</li> </ul>"},{"location":"modeling/Iteration_32_llama_32/#tokenizer-analysis","title":"Tokenizer analysis","text":"<p>Exploring how the tokenizer works is crucial to select a good grid encoding. We want to use an encoding that uses different symbols for each cell in the grid and at the same time that it is able to represent the grid with the minimum number of tokens possible.</p> <p>Llama 3.2 1B uses the same tokenizer as Llama 3.1 7B. This implies that it has words for each number from 0 to 999. In contrast Qwen uses a different word for each number, making it easier to represent the grids. One solution is to repeat each number 3 times to encode the grid for Llama.</p>"},{"location":"modeling/Iteration_32_llama_32/#local-trainings","title":"Local trainings","text":"Click to see bash commands <pre><code>python fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Llama-3.2-1B-Instruct \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(RepeatNumberEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240926_debug_Llama32/01_baseline \\\n--max_seq_len 2048 \\\n--device_map None \\\n--max_steps 500 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240926_debug_Llama32/02_qwen_25 \\\n--max_seq_len 2048 \\\n--device_map None \\\n--max_steps 500 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2-0.5B-Instruct \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240926_debug_Llama32/03_qwen_2 \\\n--max_seq_len 2048 \\\n--device_map None \\\n--max_steps 500 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n</code></pre> <p>I had to decrease the max_seq_len to 2048 to be able to train on my computer. Each step was taking around 9 seconds. For Qwen2.5 it takes around 6 seconds per step. The token length distribution is the same despite using different grid encoders. Qwen2 has the same speed as Qwen2.5</p> <p>So the Llama-1B model is 50% slower at training than Qwen-0.5B and is more memory hungry.</p>"},{"location":"modeling/Iteration_32_llama_32/#results","title":"Results","text":"model accuracy pass_32 vote_2 vote_1 Qwen2-0.5B 8.24% 26.50% 15.91% 13.01% Qwen2-0.5B-Instruct 8.25% 26.75% 15.91% 13.01% Qwen2.5-0.5B 9.37% 26.75% 18.31% 15.28% Qwen2.5-0.5B-Instruct 8.98% 26.00% 17.93% 14.27% Llama-3.2-1B 10.25% 29.00% 19.88% 14.88% <p>Validation results show an improvement when fine-tuning Llama for ARC tasks over Qwen2.5 and Qwen2.</p> <p>Inference took 2473 seconds, compared to the 1809 seconds that takes inference with Qwen. Inference is 37% slower.</p>"},{"location":"modeling/Iteration_32_llama_32/#conclusion","title":"Conclusion","text":"<p>Llama-3.2-1B gets better validation results than Qwen but at the cost of being 50% slower on training and 37% slower on inference. It is unclear if it is worth it. I would rather try using a model smaller than Qwen-0.5B to have more room for test-time fine-tuning and inference.</p>"},{"location":"modeling/Iteration_32_llama_32/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_32_llama_32/#todo","title":"TODO","text":"<ul> <li> Read about the new release. https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct</li> <li> What is the best way to encode the grids? Check the tokenizer. Maybe create a notebook to do this inspection, it will be useful in the future if more small models are released.</li> <li> What is the optimal learning rate?</li> <li> How is the training and inference speed compared to Qwen2.5-0.5B? Do I have to reduce the number of predictions or the duration of the test-time fine-tuning?</li> </ul>"},{"location":"modeling/Iteration_33_back_to_smollm/","title":"Iteration 33. Back to SmolLM","text":"<p>28-09-2024</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#goal","title":"Goal","text":"<p>Train with SmolLM models to see if we can reach similar accuracy to Qwen but with faster models.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#motivation","title":"Motivation","text":"<p>I have recently tried the new Llama 3.2 1B and it was better than Qwen but slower. I have the intuition that a small model trained for longer could reach the same accuracy as a bigger model. But this smaller model could be test-time fine-tuned for more steps or do more predictions.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#development","title":"Development","text":"<p>In the SmolLM blog they say the following:</p> <p>For all three models we use embedding tying and a context length of 2048 tokens. This context length can be further extended with some long context fine-tuning.</p> <p>Let's see if we can really train the models with a bigger context length and they work well at inference.</p> <p>I'm going to go directly for the smaller model <code>SmolLM-135M-Instruct</code> because there is a 360M parameter model but that is very close to Qwen's 500M.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#tokenizer-analysis","title":"Tokenizer analysis","text":""},{"location":"modeling/Iteration_33_back_to_smollm/#local-experiments","title":"Local experiments","text":"Click to see bash commands <pre><code># baseline, 492 seconds, 4.9 seconds/it\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM/01_baseline \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n\n# Try to increase per_device_train_batch_size but get OOM\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM/02_bs2 \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4 \\\n--per_device_train_batch_size 2\n\n# train on a single gpu, 338s, this uses ~21GB of VRAM, 3.3 seconds per iteration\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM/03_1gpu \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n\n# Reduce the msl to 2048, now it only uses 7GB of VRAM, 294s, 2.9 seconds per iteration\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM/04_1gpu_2048msl \\\n--max_seq_len 2048 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4\n\n# 186 seconds, 1.8 seconds per step\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--lora_r 32 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM/05_1gpu_2048msl_pdbs2 \\\n--max_seq_len 2048 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 1e-4 \\\n--per_device_train_batch_size 2\n</code></pre> <p>It is training at a speed of 1.8 seconds per step on a single GPU and with a max_seq_len of 2048. For reference Qwen trained at 6 seconds per step and Llama at 9 when being trained on 2 gpus. So potentially we are looking at a speedup of 6-7. If we are able to train SmolLM to a similar accuracy to Qwen this would be game changing.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#how-to-increase-the-context-length","title":"How to increase the context length","text":"<ul> <li>https://huggingface.co/togethercomputer/LLaMA-2-7B-32K/discussions/8</li> <li>ChatGPT suggestions to increase the context length</li> <li>More educated information from ChatGPT</li> <li>https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models</li> <li>https://blog.eleuther.ai/rotary-embeddings/</li> <li>Is it true that context window can be safely doubled without repercussions? #7206</li> <li>https://huggingface.co/docs/transformers/main/model_doc/llama2#transformers.LlamaConfig.rope_scaling</li> <li>Configuration samples:<ul> <li>Qwen2.5B, 32k context, theta 1000000</li> <li>Phi-3-mini-4k-instruct, 4k context, theta 10000.0</li> <li>Phi-3-mini-128k-instruct, 131k context, theta 10000.0, uses longrope</li> <li>SmolLM-135M-Instruct, 2k context, theta 10000.0</li> <li>Llama-3.1-8B-Instruct, 131k context, theta 500000, original context 8k but uses llama3 rope_scaling</li> </ul> </li> <li>https://wandb.ai/byyoung3/ml-news/reports/Scaling-Llama-2-to-32k-Tokens-With-LongLora--Vmlldzo1NzU4OTk2</li> </ul> <p>It seems that theta determines the original context. If a longer context is needed it seems that all the people use the rope_scaling.</p> Click to see bash commands <pre><code># train on a single gpu, 338s, this uses ~21GB of VRAM, 3.3 seconds per iteration\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/01_baseline-full-fine-tuning \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 4e-4\n\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/02_change-model-config \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--learning_rate 4e-4\n\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/03_change-model-config-longer \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 1000 \\\n--warmup_ratio 1e-1 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--random_seed 7 \\\n--learning_rate 4e-4\n\nexport CUDA_VISIBLE_DEVICES=1\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/04_longer-baseline \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 1000 \\\n--warmup_ratio 1e-1 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--random_seed 7 \\\n--learning_rate 4e-4\n\nexport CUDA_VISIBLE_DEVICES=0\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/05_rope-scaling-02 \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 1000 \\\n--warmup_ratio 1e-1 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--random_seed 7 \\\n--learning_rate 4e-4\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--n_gpus 1 \\\n--no-use_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20240928_debug_SmolLM_context_window/07_linear-rope-scaling-2-update-tokenizer \\\n--max_seq_len 10240 \\\n--device_map None \\\n--max_steps 1000 \\\n--warmup_ratio 1e-1 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--verbose \\\n--random_seed 7 \\\n--learning_rate 4e-4\n</code></pre>"},{"location":"modeling/Iteration_33_back_to_smollm/#amd-llama-135m","title":"AMD-Llama-135m","text":"<p>This model encodes each number independently, just like SmolLM.</p> <p>It is not an instruct model and it does not have a chat template.</p> <p>https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-adding-and-editing-chat-templates</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#current-problems","title":"Current problems","text":"<p><code>model_max_length</code> in tokenizer, does not seem to be saved correctly, I have manually fixed it.</p> <p>It makes very long predictions, just like non-instruct Qwen models.</p> <p>The baseline model that was simply fine-tuned does not work with VLLM because the following error:</p> <p>ValueError: User-specified max_model_len (10240) is greater than the derived max_model_len (max_position_embeddings=2048 or model_max_length=None in model's config.json). This may lead to incorrect model outputs or CUDA errors. To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1</p> <p>If I modify the model configuration to increase the max_model_len it works, but it seems to be predicting special tokens all the time because the predictions appear to be empty, but it takes 3599s to do the predictions. If I modify the inference script to just use 2048 it does the same thing with the predictions but faster. Thus it appears that without modifications the model cannot work correctly.</p> <p>I have manually modified the number of gpus to 1.   File \"/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/vllm/config.py\", line 337, in verify_with_parallel_config     raise ValueError( ValueError: Total number of attention heads (9) must be divisible by tensor parallel size (2).</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#results","title":"Results","text":""},{"location":"modeling/Iteration_33_back_to_smollm/#training-metrics","title":"Training metrics","text":"<p>Wandb metrics</p> <p>As a reference when training Qwen or Llama for 10k steps I could reach a train and validation loss around 0.08. Training for 80k steps could reduce the train loss to 0.03, but the validation loss did not improve.</p> <p>Training SmolLM reaches a min validation loss of 0.10 and 0.058 train loss.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#validation-metrics","title":"Validation metrics","text":"model training steps accuracy pass_32 vote_2 Qwen2-0.5B 10000 8.24% 26.50% 15.91% Qwen2-0.5B-Instruct 10000 8.25% 26.75% 15.91% Qwen2-0.5B-Instruct 80000 13.78% 33.88% 23.11% Qwen2.5-0.5B 10000 9.37% 26.75% 18.31% Qwen2.5-0.5B-Instruct 10000 8.98% 26.00% 17.93% Llama-3.2-1B 10000 10.25% 29.00% 19.88% SmolLM-135M-Instruct 40000 8.40% 23.00% 16.54% SmolLM-135M-Instruct 140000 8.58% 24.12% 16.88% <p>Despite training for much longer I have not been able to reach the accuracy of Qwen models that are trained just for 10k steps.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#inference-speed","title":"Inference speed","text":"<p>Inference takes 1231 seconds on a single GPU, by comparison Qwen takes 1809 using two GPUs.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#conclusion","title":"Conclusion","text":"<p>The smaller SmolLM model trains 6-7 times faster than Qwen and inference is 3 times faster.</p> <p>However I have not been able to reach similar validation accuracy to Qwen despite training for much longer.</p>"},{"location":"modeling/Iteration_33_back_to_smollm/#next-steps","title":"Next steps","text":"<ul> <li>After fixing the problem with the tokenizer, I could now train Qwen2.5 non-instruct model and have fast inference.</li> <li>I could revisit this iteration and try with other small models at the end of the challenge once the data is fixed and I just have to try different models. I might try different position encoding variations.</li> </ul>"},{"location":"modeling/Iteration_33_back_to_smollm/#todo","title":"TODO","text":"<ul> <li> What is the speedup when training?</li> <li> Train a model for 10k steps to find what is the optimal learning rate -&gt; 8e-4</li> <li> Does the evaluation return comparable metrics to Qwen?</li> <li> What is the speedup at inference?</li> <li> Try to get the same metrics as Qwen by training for much longer, f.e. 160k steps</li> <li> Another small LLM, it also has 2048 context window: https://huggingface.co/amd/AMD-Llama-135m</li> </ul>"},{"location":"modeling/Iteration_34_developing_omni_arc/","title":"Iteration 34. Developing Omni-ARC","text":"<p>29-09-2024</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#goal","title":"Goal","text":"<p>Implement a first version of Omni-ARC and validate if the approach is promising:</p> <ul> <li>Can we improve the generalization of the current approach by learning to predict code to do the tasks? Similar to the improvement that we got when learning the inputs distribution.</li> </ul>"},{"location":"modeling/Iteration_34_developing_omni_arc/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_34_developing_omni_arc/#development","title":"Development","text":""},{"location":"modeling/Iteration_34_developing_omni_arc/#the-right-level-of-abstraction","title":"The right level of abstraction","text":"<p>With the right level of abstraction writing code to solve the training tasks is very easy and fast. I have implemented almost 100 training tasks in less than 2 days. Just with very basic primitive functions like detect and draw objects is possible to solve a lot of tasks.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#repeated-or-very-similar-tasks","title":"Repeated or very similar tasks","text":"<p>On the training set I have detected some tasks that are exactly the same and other tasks are just variations of the same task. Maybe the dataset is not as big as I thought.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#first-version-of-omni-arc","title":"First version of Omni-ARC","text":"<p>I have implemented nearly 100 training tasks (25% of the tasks). I believe this is enough to make a training and see the effect it has on the model.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#local-experiments","title":"Local experiments","text":"<p>Let's make a quick training runs to verify that I can train using omni-arc dataset.</p> Click to see bash commands <pre><code># baseline\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/01_baseline \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n\n# use omni-arc with output-from-examples-v1\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/02_omni-arc_output-from-examples-v1 \\\n--train_datasets omni-arc-100 output-from-examples-v1 \\\n--val_dataset omni-arc-100 output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n\n# code from examples\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/02_omni-arc_code-from-examples-v0 \\\n--train_datasets omni-arc-100 code-from-examples-v0 \\\n--val_dataset omni-arc-100 code-from-examples-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n\n# output from code\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/02_omni-arc_output-from-code-v0 \\\n--train_datasets omni-arc-100 output-from-code-v0 \\\n--val_dataset omni-arc-100 output-from-code-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n\n# all together\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/02_omni-arc_all \\\n--train_datasets omni-arc-100 output-from-code-v0 \\\n--train_datasets omni-arc-100 code-from-examples-v0 \\\n--train_datasets omni-arc-100 output-from-examples-v1 \\\n--train_datasets omni-arc-100 input-from-inputs-v0 \\\n--val_dataset omni-arc-100 output-from-code-v0 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 1000 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n\n# prompt refinement\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241005_omni-arc/03_omni-arc_code-from-examples-v1 \\\n--train_datasets omni-arc-100 code-from-examples-v1 \\\n--val_dataset omni-arc-100 code-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 10 \\\n--random_seed 7 \\\n--batch_size 5 \\\n--learning_rate 4e-5 \\\n--verbose\n</code></pre>"},{"location":"modeling/Iteration_34_developing_omni_arc/#experiment-design","title":"Experiment design","text":"<p>We have two dimensions to test:</p> <ol> <li>Which tasks are useful?</li> <li>How we should weight omni-arc dataset against all the other datasets?</li> </ol> <p>The training duration should be increased proportionally to the new tasks.</p> <p>The baseline has 1200 tasks.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#problem-with-non-instruct-models","title":"Problem with non-instruct models","text":"<p>I thought that I had solved the problem with never-ending predictions from non-instruct models by modifying the tokenizer at the beginning of the train. However that is not true and I have had to relaunch all the trainings using an instruct version of Qwen. I might revisit this on the future.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#had-to-update-transformers-to-4451","title":"Had to update transformers to 4.45.1","text":"<p>I had to add scipy to the docker and that updated transformers library. When doing inference on my PC there was an error when loading the model so I had to update transformers to 4.45.1</p> <p>It's possible that I will have to update the Kaggle environment as well.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#verified-that-kaggle-and-github-dataset-are-identical","title":"Verified that Kaggle and Github dataset are identical","text":"<p>Notebook</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#results","title":"Results","text":""},{"location":"modeling/Iteration_34_developing_omni_arc/#local-experiments-training-metrics","title":"Local experiments training metrics","text":"<p>I have only trained for 1k steps. It seems that the difficulty from easier to harder is:</p> <ul> <li>code from examples</li> <li>output from examples</li> <li>output from code</li> </ul> <p>Training speed was not affected by using omni-arc. In fact it was faster but this could be due to the tasks being smaller.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#validation-results","title":"Validation results","text":"<p>It is unclear if training on new tasks like <code>code-from-examples</code> and <code>output-from-code</code> has a positive effect on the initial task of <code>output-from-examples</code>. We added an additional 100 training samples to the initial 1200 samples. The weight indicates how frequently we sample omni-arc versus the other datasets.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#conclusion","title":"Conclusion","text":"<p>We have implemented a first version of omni-arc and trained multiple models with it. The effect on the initial task of <code>output-from-examples</code> is unclear.</p>"},{"location":"modeling/Iteration_34_developing_omni_arc/#next-steps","title":"Next steps","text":"<ul> <li>Solve the problem with never ending predictions from non-instruct models. I would like to use Qwen2.5 base model   for the challenge.</li> <li>Can we solve some of the evaluation tasks using generated code?</li> </ul>"},{"location":"modeling/Iteration_34_developing_omni_arc/#todo","title":"TODO","text":"<ul> <li> Implement first version of omni-arc</li> <li> Add new prompt templates</li> <li> Update fine-tuning script to support omni-arc dataset</li> <li> Is training speed affected by using omni-arc? I believe generation is fast enough to be done real-time</li> <li> Clone omni-arc repo in the cluster and add the path to the PYTHONPATH</li> <li> Refine the prompts using ChatGPT</li> <li> Experiment to see if learning 3 tasks is better than learning two tasks. The baseline learns output-from-examples and input-from-inputs, the new experiment also learns code-from-examples. 10k steps per task.</li> </ul>"},{"location":"modeling/Iteration_35_optimize_batch_size/","title":"Iteration 35. Optimize batch size","text":"<p>07-10-2024</p>"},{"location":"modeling/Iteration_35_optimize_batch_size/#goal","title":"Goal","text":"<p>Can I train faster if I optimize the batch size?</p>"},{"location":"modeling/Iteration_35_optimize_batch_size/#motivation","title":"Motivation","text":"<p>So far I have mainly trained using a batch size of 16. However when learning 3 tasks I have observed sudden spikes in the training loss. This might be caused by the \"small\" batch size.</p> <p>In this experiment I'm going to increase the batch size and see how the training and validation metrics are affected.</p> <p>Since all the tasks are supposed to be independent and different (I already know that this is not true and are repeated tasks on the training dataset), increasing the batch size might help to learn better.</p>"},{"location":"modeling/Iteration_35_optimize_batch_size/#results","title":"Results","text":"<p>All experiments were done with LoRA rank 128. The only change was the batch size and learning rate, the number of training steps was adjusted to the batch size so all the trainings used the same number of samples.</p> <p></p> <p>The results show that we can get the same or better results when increasing the batch size over the default 16.</p>"},{"location":"modeling/Iteration_35_optimize_batch_size/#conclusion","title":"Conclusion","text":"<p>We can increase the batch size and get same or better results. Thus when training omni-arc on multiple tasks it might be worth to use a bigger batch size.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/","title":"Iteration 36. Solving evaluation tasks with code","text":"<p>09-10-2024</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#goal","title":"Goal","text":"<p>Can we solve the evaluation tasks by predicting code that implements the tasks?</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#motivation","title":"Motivation","text":"<p>On Iteration 34 I trained models on omni-arc tasks. It was unclear if the approach of <code>output-from-examples</code> benefited from training the model to do multiple tasks.</p> <p>However if I can predict python code that could be game-changer because I can verify the python code with the train samples.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#development","title":"Development","text":"<p>First models were trained with 100 training tasks, second model with close to 150. Coverage of the training dataset is important because it's likely correlated with coverage of the evaluation and test dataset.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#first-steps-with-inference","title":"First steps with inference","text":"Click to see bash commands <pre><code># baseline\npython inference.py \\\n--model_path  /mnt/hdd0/Kaggle/arc24/models/20241006_omniarc_validation/02_omni-arc-400-code-from-examples-Qwen2.5-0.5B-Instruct_lr5e-5_14000steps_2gpus_8192msl/checkpoint-14000 \\\n--prompt_version code-from-examples-v0 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 8 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/first_predictions/checkpoint-14000/inference_evaluation_x008.json \\\n--verbose\n\npython inference.py \\\n--model_path  /mnt/hdd0/Kaggle/arc24/models/20241006_omniarc_validation/02_omni-arc-400-code-from-examples-Qwen2.5-0.5B-Instruct_lr5e-5_14000steps_2gpus_8192msl/checkpoint-14000 \\\n--prompt_version code-from-examples-v0 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/first_predictions/checkpoint-14000/inference_evaluation_x032.json\n\npython merge_lora.py --base_model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct --lora_path /mnt/hdd0/MEGA/projects/temp/20241006_omniarc_validation/05_omni-arc-400-code-from-examples-v1-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_7000steps_2gpus_8192msl/checkpoint-7000 --output_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 8 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x008.json \\\n--verbose\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x032.json\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--temperature 0.5 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x032_t5e-1.json\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--temperature 0.7 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x032_t7e-1.json\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--temperature 0.9 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x032_t9e-1.json\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 32 \\\n--temperature 1 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x032_t1.json\n\npython inference.py \\\n--model_path  /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct-omni-arc \\\n--prompt_version code-from-examples-v1 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 128 \\\n--temperature 0.7 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/second_model/checkpoint-7000/inference_evaluation_x128_t7e-1.json\n\n\npython inference.py \\\n--model_path  /mnt/hdd0/MEGA/projects/temp/20241006_omniarc_validation/03_omni-arc-800-all-code-Qwen2.5-0.5B-Instruct_lr5e-5_26000steps_2gpus_8192msl/checkpoint-26000 \\\n--prompt_version code-from-examples-v0 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json \\\n--predictions_per_task 8 \\\n--temperature 0.7 \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/debug/third_model/checkpoint-26000/inference_evaluation_x008_t7e-1.json\n\n</code></pre> <p>The model is generating valid python code, I have to improve the inference script to check that the code is correct and create the output. Add timeouts for safety.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#results","title":"Results","text":""},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#preliminary-results","title":"Preliminary results","text":"<p>I solve 4% of the tasks from the evaluation dataset. All predictions seem to be correct because they are validated against the train dataset. When using temperature 0 there does not seem to be any favorable scaling law.</p> <p>Made up to 132 predictions, but the accuracy improves very slowly. The output from examples approach had a very different dynamic.</p> model accuracy pass_n vote_1 unanswered 1 2.41% 3.75% 3.75% 97.59% 2 3.00% 4.00% 4.00% 96.94% 3 2.98% 4.75% 4.75% 97.02% <p>On the best case we are able to solve close to 5% of the evaluation tasks. The most relevant aspect is that all the predictions are correct.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#token-distribution-of-omni-arc-code","title":"Token distribution of omni-arc code","text":"<p>We can see that the code is much smaller than predicting the whole grid which can have up to 1000 tokens, but the code is 200 tokens at maximum, 5 times smaller.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#how-does-the-method-scale-with-the-number-of-predictions","title":"How does the method scale with the number of predictions?","text":"<p>This first models do not scale well with the number of predictions, the improvement is very slow.</p> <p>As a reference we can compare it to a experiment from Iteration 30</p> <p></p> <p>My hypothesis is that the dataset is small and the model has not learned correctly yet.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#first-submissions","title":"First submissions","text":"<p>I have made a first submission with model: <code>20241006_omniarc_validation/05_omni-arc-400-code-from-examples-v1-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_7000steps_2gpus_8192msl/checkpoint-7000</code> and it solved 1 of the private test set tasks. It's a humble beginning, but if I can make it work this could be game-changing.</p> <p>I have also tried using test-time fine-tuning but then it did not solve any of the tasks.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#conclusion","title":"Conclusion","text":"<p>We have been able to solve new tasks by generating python code. The first evaluation solves close to 5% of the evaluation tasks. The most relevant thing is that all the predictions are either correct or empty. Thus this approach is very good for ensembling.</p>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#next-steps","title":"Next steps","text":"<ul> <li> If the code approach does not pay off, we could try to train a model to verify the solutions. Given two solutions to the problem select the correct one.</li> </ul>"},{"location":"modeling/Iteration_36_solving_evaluation_tasks_with_code/#todo","title":"TODO","text":"<ul> <li> How to execute the code safely and with timeouts? Check AIMO competition. This should be added   to the omni-arc repo, because all the dsl functions are there.</li> <li> How does the method scale with compute? Validation should allow to scale well.</li> <li> Verify that everything is working fine by looking at the predictions. -&gt; It is working fine, but the model is pretty bad at guessing the transformation. Probably more variability at the inputs is needed.</li> <li> What is the token distribution of the functions that implement the training tasks?</li> <li> Fix problems with evaluation metrics</li> <li> Fix problem with inference returning an error code</li> </ul>"},{"location":"modeling/Iteration_37_optimize_code_generation/","title":"Iteration 37. Optimize code generation","text":"<p>10-10-2024</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#goal","title":"Goal","text":"<p>We have verified that we can can solve ARC tasks by generating python code. Let's try to understand the dynamics of this new training and optimize the hyperparameters.</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#motivation","title":"Motivation","text":"<p>Solving tasks with code works, but can I optimize and improve the accuracy of the model?</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#development","title":"Development","text":""},{"location":"modeling/Iteration_37_optimize_code_generation/#results","title":"Results","text":""},{"location":"modeling/Iteration_37_optimize_code_generation/#how-does-the-training-steps-affect-the-accuracy","title":"How does the training steps affect the accuracy?","text":"<p>We don't see a clear relation between the training steps and model accuracy for this version of omni-arc.</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#is-it-helpful-to-learn-to-do-other-tasks","title":"Is it helpful to learn to do other tasks?","text":"<p>The mean pass_n of the experiments that only learn one task is 3.1%, while the experiments that learn multiple tasks is 4.1%. So clearly in this experiment learning multiple tasks is beneficial.</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#is-it-helpful-to-use-a-temperature-different-than-0","title":"Is it helpful to use a temperature different than 0?","text":"<p>There is great uncertainty in the results, so the best way to study the tendency is to compute the mean value for all the experiments.</p> <p></p> <p>The improvement is not huge, but we get better results on average when using a temperature of 0.7</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#is-there-any-difference-between-prompts","title":"Is there any difference between prompts?","text":"prompt_version experiment 1 pass_n experiment 2 pass_n 0 0.035 0.0475 1 0.0425 0.0425 2 0.0325 0.0425 <p>There isn't a clear winner.</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#what-if-i-train-on-omni-arc-just-to-create-the-output-grid","title":"What if I train on omni-arc just to create the output grid?","text":"experiment pass_n vote_2 vote_1 code-from-examples 5.00% 5.00% 5.00% output-from-examples 9.25% 7.50% 6.13% <p>In the best case we solve 5% of the evaluation tasks with the code approach. If we train on the same data but we predict the grids directly we can get almost double <code>pass_n</code>, but <code>vote_1</code> is much closer.</p> <p>So maybe current performance is good for the amount and quality of data we have.</p>"},{"location":"modeling/Iteration_37_optimize_code_generation/#conclusion","title":"Conclusion","text":"<p>We have not been able to improve over the previous iteration: we only solve 5% of the evaluation tasks.</p> <p>However we have gained the following learnings:</p> <ul> <li>It is beneficial to train to learn multiple tasks, not just <code>code-from-examples</code></li> <li>Using a temperature of 0.7 is beneficial</li> <li>There isn't a clear relation between training steps and model accuracy</li> <li>The choose of prompts does not seem to be very relevant</li> </ul>"},{"location":"modeling/Iteration_37_optimize_code_generation/#next-steps","title":"Next steps","text":"<ul> <li>Try with bigger models. If test-time fine-tuning is not necessary we might benefit from using bigger or coding models. F.e.<ul> <li>https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct</li> <li>https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct</li> </ul> </li> <li>Improve the omni-arc dataset:<ul> <li>Add more tasks to increase coverage</li> <li>Add more training inputs to have more variability (can I reuse re-arc for this?)</li> <li>Add task variations</li> <li>Add task to learn to use the primitives</li> </ul> </li> <li>Does test-time fine-tuning help to generate better code?</li> </ul>"},{"location":"modeling/Iteration_37_optimize_code_generation/#todo","title":"TODO","text":"<ul> <li> How does the training steps affect the accuracy? -&gt; Run trainings with different training lenght, just using code data</li> <li> What is the best prompt? Is there any difference?</li> <li> Is it helpful to learn to do other tasks?</li> <li> What if I train on omniarc just on the default task?</li> </ul>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/","title":"Iteration 38. Make non-instruct models great again","text":"<p>11-10-2024</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#goal","title":"Goal","text":"<p>Modify the training of non-instruct models so they learn to stop the response at inference.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#motivation","title":"Motivation","text":"<p>I have made experiment with non-instruct models at the past, but they do not stop the response at inference and thus inference times are higher because they repeat the response over and over.</p> <p>I have evidence that non-instruct models might give better results, but I have to find the way to train them correctly.</p> <p>There must be an easy way to fix this.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#development","title":"Development","text":""},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#experiment-design","title":"Experiment design","text":"<p>My idea is to fine-tune Qwen2.5-0.5B on a tiny dataset of just 5 samples. I will choose the smaller samples from the ARC tasks to train faster and have smaller VRAM requirements.</p> <p>Then I will make inference and see if the model stops the responses or not.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#updating-transformers-and-accelerate","title":"Updating transformers and accelerate","text":"<p>The python environment is currently a little big unstable due to the installations I did for omni-arc.</p> <p>I had to update both transformers and accelerate to make the training script work again on my computer.</p> <pre><code>pip install --upgrade transformers accelerate\n</code></pre>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#trainings","title":"Trainings","text":"Click to see bash commands <pre><code># baseline\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/01_baseline \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/01_baseline_accelerate \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct \\\n--device_map None \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/02_baseline_instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--no-use_lora \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/03_full-fine-tune \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--no-use_lora \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/04_full-fine-tune_change_pad_token \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--no-use_lora \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/05_full-fine-tune_change_pad_token_fix_tokenizer_bug \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 32 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/06_final_experiment_with_lora \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 100 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\naccelerate launch fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/07_final_experiment_with_lora_longer \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 500 \\\n--logging_steps 10 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--verbose\n\nfor checkpoint_folder in /mnt/hdd0/Kaggle/arc24/models/20241011_non-instruct_models/*/checkpoint-100; do\n    python easy_inference_and_evaluation.py \"${checkpoint_folder}\" --dataset_path /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json --predictions_per_task 8\ndone\n</code></pre> <p>Training for 100 steps takes 7:46 minutes without accelerate. With accelerate just a little bit more than 2 minutes.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#results","title":"Results","text":"<p>I have made two improvements to the existing code:</p> <ol> <li>Qwen tokenizer does not need to be resized, I just needed to change the <code>eos_token</code> to be the same    as the instruct model.</li> <li>I have added the tokenizer to the train function, that way it is saved in the checkpoint.</li> </ol> <p>The problem was that the original tokenizer was being saved instead of the modified one, thus at inference there was a discrepancy between the model and the tokenizer.</p> <p>I have been able to fine-tune the non-instruct version without any problem and make inference correctly.</p> <p>However it seems that LoRA is not enough for the non-instruct version, if I want to use it I have to fully fine-tune the model.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#conclusion","title":"Conclusion","text":"<p>I can now use the non-instruct models, but I have to fully fine-tune them. I cannot just use LoRA. Thus it is unclear if this will be useful.</p>"},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_38_make_non_instruct_models_great_again/#todo","title":"TODO","text":"<ul> <li> Create a small dataset for training and validation</li> <li> Train, make inference and verify if it works</li> </ul>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/","title":"Iteration 39. Reduce VLLM RAM usage","text":"<p>11-10-2024</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#goal","title":"Goal","text":"<p>If I can reduce the RAM usage from VLLM ensembling would be easier in the Kaggle submission.</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#motivation","title":"Motivation","text":"<p>Currently I cannot parallelize everything in the submission pipeline because VLLM uses 50% of RAM and the 2020 solution sometimes demands more than 50%.</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#development","title":"Development","text":"<p>I have been playing with VLLM parameters and <code>swap_space</code> seems to be the one with the biggest effect on RAM usage. In the documentation it says: </p> <p>CPU swap space size (GiB) per GPU.</p> <pre><code>export checkpoint_folder=/mnt/hdd0/Kaggle/arc24/models/20241007_batch_size/01_bs16_lr5e-5_Qwen2.5-0.5B-Instruct_10000steps_2gpus_8192msl/checkpoint-10000\nrm /mnt/hdd0/Kaggle/arc24/evaluations/20241007_batch_size/01_bs16_lr5e-5_Qwen2.5-0.5B-Instruct_10000steps_2gpus_8192msl/checkpoint-10000/inference_evaluation_x009.json\npython easy_inference_and_evaluation.py \"${checkpoint_folder}\" --dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json --predictions_per_task 9\n</code></pre>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#results","title":"Results","text":""},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#local-results","title":"Local results","text":"swap_space RAM usage inference time (s) 4 16 530 2 9.7 563 1 5.5 514 0 1.1 508 <p>We have reached an enormous VRAM decrease without a significant effect on inference time nor in accuracy. This results were obtained on my PC, I should repeat the experiments on Kaggle.</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#kaggle-results","title":"Kaggle results","text":"swap_space RAM usage inference time (s) 4 50% 84 2 32% 79 1 22% 79 0 12% 78 <p>We see the same trend, we can decrease the RAM usage by a lot while the inference time almost does not change.</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#remember-icecuber-ram-usage","title":"Remember icecuber RAM usage","text":"<p>On this notebook we can see that sometimes icecuber uses up to 80% of the RAM. So if we want to run it on parallel with VLLM inference we will have to use no <code>swap_space</code>.</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#conclusion","title":"Conclusion","text":"<p>We have found that we can decrease the RAM usage of VLLM dramatically apparently without any effect on accuracy nor on speed (at least for ARC inference.)</p>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#next-steps","title":"Next steps","text":"<ul> <li>Create a new notebook for Omni-ARC inference that runs all 2020 solution in parallel. On a first step it tries to solve the tasks using code. Then it does test-time fine-tuning on the remaining tasks.</li> </ul>"},{"location":"modeling/Iteration_39_reduce_vllm_ram_usage/#todo","title":"TODO","text":"<ul> <li>[ ]</li> </ul>"},{"location":"modeling/Iteration_40_try_coding_models/","title":"Iteration 40. Try coding models","text":"<p>12-10-2024</p>"},{"location":"modeling/Iteration_40_try_coding_models/#goal","title":"Goal","text":"<p>Can we improve the results of coding generation by using models that are specialized in code?</p>"},{"location":"modeling/Iteration_40_try_coding_models/#motivation","title":"Motivation","text":"<p>I have the intuition that using a model that has been specialized in coding can give better results than a simple instruct model.</p>"},{"location":"modeling/Iteration_40_try_coding_models/#development","title":"Development","text":"<p>Qwen already has coding models, so it would be very convenient if I can use those models:</p> <ul> <li>https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct</li> <li>https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct</li> </ul>"},{"location":"modeling/Iteration_40_try_coding_models/#analyze-tokenizers","title":"Analyze tokenizers","text":"<p>Both coder models use the same tokenizer as the Qwen non-coder models, so I don't have to use a different grid encoder. I can run the exact same training with these models. The only difference might be the VRAM requirements.</p>"},{"location":"modeling/Iteration_40_try_coding_models/#vram-requirements","title":"VRAM requirements","text":"<p>When training on A6000 gpus that have 48GB of VRAM I can use a <code>max_seq_len</code> of 6144 with the 1.5B model. If I train the 7B model on 2GPUs the <code>max_seq_len</code> has to be 4096 or I get OOM errors.</p> <p>I have checked the trainings with Llama-3.1-8B and I also used a <code>max_seq_len</code> of 4096.</p> <p>If I could have access to a GPU with 80GB of VRAM I could increase the training context length.</p>"},{"location":"modeling/Iteration_40_try_coding_models/#omni-arc-state","title":"Omni-arc state","text":"<p>All these experiments were done with omni-arc at commit <code>589b6695d56ad2dbd7f37c78e9923cdec646ef54</code>. There were around 150 implemented training tasks.</p>"},{"location":"modeling/Iteration_40_try_coding_models/#results","title":"Results","text":""},{"location":"modeling/Iteration_40_try_coding_models/#training-speed","title":"Training speed","text":"<p>A training for 2k steps with batch size 32 takes:</p> <ul> <li>16000s (4.4h) for Qwen2.5-0.5B-Instruct</li> <li>21000s (5.8h) for Qwen2.5-Coder-1.5B-Instruct</li> <li>60000s (16.6h) for Qwen2.5-Coder-7B-Instruct</li> </ul> <p>The 7B mdoel is much slower, but the 1.5B has acceptable speed. Maybe the GPU is not being used at its maximum capacity when training the 0.5B model.</p>"},{"location":"modeling/Iteration_40_try_coding_models/#inference-speed","title":"Inference speed","text":"<ul> <li>300 seconds for Qwen2.5-0.5B-Instruct</li> <li>504 seconds for Qwen2.5-Coder-1.5B-Instruct</li> <li>1311.5445 seconds for Qwen2.5-Coder-7B-Instruct</li> </ul>"},{"location":"modeling/Iteration_40_try_coding_models/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_40_try_coding_models/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_40_try_coding_models/#todo","title":"TODO","text":"<ul> <li> Can we beat the baseline 5% accuracy of Qwen2.5-0.5B-Instruct with coder models?</li> <li> Inference and train speed comparison of the models</li> <li> What if I decrease the size or LoRA rank?</li> <li> How well do these models scale with the number of predictions?</li> </ul>"},{"location":"modeling/Iteration_41_next_steps/","title":"Iteration 41. Next steps","text":"<p>13-10-2024</p>"},{"location":"modeling/Iteration_41_next_steps/#goal","title":"Goal","text":"<p>There is less than one month until the end of the competition and the teams are improving their leaderboard scores. I need to think ideas and prioritize them.</p> <p></p>"},{"location":"modeling/Iteration_41_next_steps/#ideas","title":"Ideas","text":"<p>These are the most promising ideas, I present them from the most promising to the least. I believe I can try them all, but I should focus on OmniARC.</p>"},{"location":"modeling/Iteration_41_next_steps/#improve-omniarc-more-and-better-data-for-training","title":"Improve OmniARC, more and better data for training","text":"<p>There is a lot of room to improve the Omni-ARC dataset:</p> <ul> <li>Add more tasks to increase coverage</li> <li>Add more training inputs to have more variability (can I reuse re-arc for this?)</li> <li>Add task variations</li> <li>Add task to learn to use the primitives</li> </ul> <p>This path is not very funny, I just have to implement new code and do it with high quality. But it is very likely to pay off.</p>"},{"location":"modeling/Iteration_41_next_steps/#train-a-verifier","title":"Train a verifier","text":"<p>Voting is a good way to select between the predicted grids, but it is not perfect. It is possible that I might train the LLM to select the correct output, an additional task where the model just needs to choose between two possible output grids.</p> <p>I would have to create a dataset to do this. Probably the best way is to use the submission models, that have been trained on all the data and maybe increase the temperature to induce errors in the prediction.</p> <p>Benefits of this approach:</p> <ul> <li>If it works well I could use the verifier to create a better ensemble, by selecting the most promising candidates</li> <li>The data can be generated entirely automatically</li> <li>Predictions should be fast because the model only has to predict one token</li> </ul> <p>One good thing of this approach is that I can generate the data automatically, requires less manual work than the previous idea. And it is complimentary to the omni-arc idea, selecting between two options is another task to learn.</p>"},{"location":"modeling/Iteration_41_next_steps/#find-another-source-of-data-such-as-a-video-game-atari-30x30-pixels","title":"Find another source of data, such as a video game (Atari)? 30x30 pixels","text":"<p>Training a model to predict future frames given the past frames and some actions could induce some prior knowledge about objects, movements... into the LLM.</p> <p>I need to find games that can be represented on a image of 30x30 pixels or less, and with just 10 colors.</p> <p>This approach is interesting because I don't need to create the data myself just like in the OmniARC approach.</p> <p>For example the Snake game could fit the requirements.</p> <p></p>"},{"location":"modeling/Iteration_41_next_steps/#revisit-smollm-and-how-to-train-it-for-long-context","title":"Revisit SmolLM and how to train it for long context","text":"<p>It might be possible that we more effort I can make SmolLM models work with bigger context window. Or <code>AMD-Llama-135m</code>. I could make experiments with a synthetic task that requires the longer context window to be solved.</p> <p>A smaller model could allow for faster training and inference, which can be very helpful for the Kaggle submission.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/","title":"Iteration 42. Improve Omni-ARC","text":"<p>14-10-2024</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#goal","title":"Goal","text":"<p>The idea is to devote at least a week to improve Omni-ARC dataset. At the same time I will run daily trainings with the updated versions of the dataset and measure progress on the evaluation dataset.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#motivation","title":"Motivation","text":"<p>The initial version of Omni-ARC has around 150 training tasks. A model trained on that version is able to solve just 5% of the evaluation dataset. There should be a lot of room for improvement:</p> <ul> <li>Add more tasks to increase coverage</li> <li>Add more training inputs to have more variability (can I reuse re-arc for this?)</li> <li>Add task variations</li> <li>Add task to learn to use the primitives</li> <li>Implement the evaluation tasks (although this will only help to improve on leaderboard and will leak information of the evaluation set to my head)</li> </ul>"},{"location":"modeling/Iteration_42_improve_omniarc/#development","title":"Development","text":""},{"location":"modeling/Iteration_42_improve_omniarc/#implement-more-training-tasks","title":"Implement more training tasks","text":"<p>I have increased the number of training tasks from 150 to 269.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#make-execution-safer","title":"Make execution safer","text":"<p>I have found that one of the evaluations created a <code>colors.txt</code> file. I do not want the execution to have input/output capabilities.</p> <p>TODO: Once I have a model that is compatible with 0.3.0 version update the execution code to be safer: https://chatgpt.com/c/671217ca-e944-8012-a3b5-8b3a004a013a</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#creating-more-training-inputs","title":"Creating more training inputs","text":"<p>Let's analyze how can I add more inputs for each task:</p> <ul> <li>Use re-arc dataset. This would be a great option if the re-arc dataset follows the same distribution   as the original tasks (I'm not sure about this), because otherwise the task implementations won't work.</li> <li>Write my own python generators. It's very likely that with the right level of abstraction I can   quickly implement generator for the training tasks. Requires more work than simply using re-arc.</li> <li>Use the model to generate new inputs. I have already trained models to learn the input distribution,   thus it is possible to use those models to generate new inputs. The disadvantage of this approach   is that I would have to manually verify the inputs. It is very likely that the model would fail   to generate some inputs, so in those cases I would have to write python code. I might also need   some grid editor to correct small mistakes in the inputs.</li> </ul>"},{"location":"modeling/Iteration_42_improve_omniarc/#generating-more-inputs-with-llms","title":"Generating more inputs with LLMs","text":"<p>The idea is to use one of the submission models to generate new inputs. This full fine-tuned model <code>/mnt/hdd0/Kaggle/arc24/models/20240925_submission_models/06_continue-full-fine-tuning-Qwen2.5-0.5B-Instruct_lr1.5e-5_1e5steps_2gpus_8192msl/checkpoint-100000/</code> could be a good candidate.</p> <pre><code>python inference.py \\\n--model_path /mnt/hdd0/Kaggle/arc24/models/20240925_submission_models/06_continue-full-fine-tuning-Qwen2.5-0.5B-Instruct_lr1.5e-5_1e5steps_2gpus_8192msl/checkpoint-100000 \\\n--prompt_version input-from-inputs-v0 \\\n--dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json \\\n--output_filepath /mnt/hdd0/Kaggle/arc24/input_generations/inference.json \\\n--predictions_per_task 8 \\\n--temperature 0.7 \\\n--n_tasks 10\n</code></pre>"},{"location":"modeling/Iteration_42_improve_omniarc/#using-re-arc-generators","title":"Using RE-ARC generators","text":"<p>I have been visualizing some of the grids generated by the RE-ARC dataset. The main problem I see is that in the RE-ARC repo the black is used as a normal color, not as the background color. This breaks many of my functions at omni-arc.</p> <p>So my idea is to adapt the RE-ARC dataset so it generates grids with the same distribution as the original ones.</p> <p>I have been able to adapt 226/285 generators with minor changes. Sampling speed has decreased from 250 samples/s to 50 samples/s. I believe it is fast enough to train as the same speed as previously.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#results","title":"Results","text":""},{"location":"modeling/Iteration_42_improve_omniarc/#first-results","title":"First results","text":"omni-arc training tasks training coverage training pass_8 evaluation pass_8 150 37.50% 35.80% 4.50% 269 67.25% 62.60% 3.75% <p>First validation results do not show improvements on evaluation after increasing the number of tasks from 100 to 269. The model is able to solve more training tasks, but its accuracy does not improve on the evaluation set. These seems like a clear sign of overfitting.</p> <p>I have visualized the tasks that it does correctly and they are all very similar to the training tasks. Thus another evidence for overfitting.</p> <p>Another explanation is that coverage on the evaluation dataset has not increased despite close to doubling the coverage on the training dataset. But that seems to be very unlikely. I could measure coverage on the evaluation set by implementing the evaluation tasks.</p> <p>How could we reduce overfitting and improve generalization?</p> <ul> <li>Add more input samples. Currently we are just using the original task inputs with some data augmentation.</li> <li>Add more tasks. I could create task variations from the original tasks, or create entirely new   tasks using the omni-arc domain specific language</li> </ul> <p>This two actions should force the model to better learn the relation between examples and code.</p> <p>It's very likely that the bad scaling behavior that we observed in recent iterations was caused by the poor generalization. Thus if we unlock generalization it is possible that we are going to improve the accuracy of the model, but also the model could improve given more compute (more predictions).</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#final-results","title":"Final results","text":"omni-arc training tasks training coverage training pass_8 evaluation pass_8 150 37.50% 35.80% 4.50% 269 67.25% 62.60% 3.75% 285 71.25% 65.25% 3.50% 285 + re-arc inputs 71.25% 49.90% 5.20% <p>Adding generator for the inputs improved the evaluation accuracy slightly, but it is still very low.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#conclusion","title":"Conclusion","text":"<p>A lot of work was devoted to make this approach work, but only 5% of the evaluation tasks are solved.</p> <p>I believe I should revisit this approach in the future, once the 2024 challenge is over and design a better approach that does not require too much human work and instead relies on automated dataset generation. F.e. we could create random drawings, or drawing modifications and ask the model to guess which code was used to do the modification.</p>"},{"location":"modeling/Iteration_42_improve_omniarc/#next-steps","title":"Next steps","text":"<ul> <li>Many times I do an initial implementation and there is some little detail wrong. I correct the implementation   and then it's fine. The ideal system should also have this opportunity. However training such a model   will require a larger context length, and I cannot afford it with the current hardware.</li> <li>I have found one task that required looking at the test input to properly implement the function.   Create a new version of the prompt that uses also the test input.</li> </ul>"},{"location":"modeling/Iteration_42_improve_omniarc/#todo","title":"TODO","text":"<ul> <li> Restrict globals</li> <li> Analyze RE-ARC input distribution</li> <li> Analyze generated input distribution</li> </ul>"},{"location":"modeling/Iteration_43_train_a_verifier/","title":"Iteration 43. Train a verifier","text":"<p>21-10-2024</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#goal","title":"Goal","text":"<p>Can we improve the LB score by better selecting the predictions?</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#motivation","title":"Motivation","text":"<p>Currently I use voting to select the predictions of the model for the submission. On Iteration 9 we saw that voting can select the best answer with an accuracy of 30-50%.</p> <p>If we can use a model to improve the answer selection there could be a lot of room for improvement, in the best case we might triple the score! (it's not going to happen)</p> <p>The idea is to train a model that is able to select between two possible answers for a task. Instead of predicting the whole answer it just has to select the correct one.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#development","title":"Development","text":""},{"location":"modeling/Iteration_43_train_a_verifier/#dataset-design","title":"Dataset design","text":"<p>Original tasks have the following format:</p> <pre><code>train: [{input:, output:}, .., {input:, output:}]\ntest: [{input:, output:}, .., {input:, output:}]\n</code></pre> <p>I have reviewed the data augmentation code and it is applied to any field for each sample, so I could add additional fields to <code>input</code> and <code>output</code> and they will also be data augmented. For example I could add <code>attempt_1</code>, <code>attempt_2</code> and so on.</p> <p>Then at training I should select:</p> <ol> <li>Which would be the test sample</li> <li>Which prediction will be used for training</li> </ol>"},{"location":"modeling/Iteration_43_train_a_verifier/#local-run-to-verify-that-it-is-working","title":"Local run to verify that it is working","text":"Click to see bash commands <pre><code>python fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241023_debug_verifier/01_select-output \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/verifier/training_v0.json select-output-from-examples-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 10 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--verbose\n\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241023_debug_verifier/02_verify-output \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/verifier/training_v0.json verify-output-from-examples-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/new_partitions/smaller_5_tasks.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 10 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--verbose\n</code></pre>"},{"location":"modeling/Iteration_43_train_a_verifier/#experiment-design","title":"Experiment design","text":"<p>We need to train a verifier an check if it allows to improve the accuracy of the prediction selection. Later on following iterations we could optimize the training.</p> <p>I believe the experiment that is most likely to succeed is to use a LoRA that has been already fine-tuned on ARC tasks and fine-tune again for output selection. I could run a few trainings with different durations.</p> <p>F.e. the model <code>20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000</code> could be a good candidate.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#prediction-ranking","title":"Prediction ranking","text":"<p>What is the best ranking system to find the top k players among a group of n players?</p> <p>This are the requirements:</p> <ul> <li>The matches are 1vs1</li> <li>The system should be as efficient as possible, it should work with the minimun number of matches</li> <li>The system should be robust to unexpected results, there is some randomness in the results of the matches</li> <li>Ideally the system will allocate the matches to extract the most information from the result</li> <li>There could be some level of uncertainty in the results that should decrease if the budget for the number of matches increases</li> </ul> <p>Some options:</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#trueskill","title":"Trueskill","text":"<ul> <li>https://trueskill.org/</li> <li>https://www.microsoft.com/en-us/research/project/trueskill-ranking-system/</li> <li>https://en.wikipedia.org/wiki/TrueSkill</li> <li>https://stackoverflow.com/questions/15054004/trueskill-matchmaking-implementation</li> </ul> <p>Player ranks are displayed as the conservative estimate of their skill, <code>R=\\mu -3\\times \\sigma</code>. This is conservative, because the system is 99% sure that the player's skill is actually higher than what is displayed as their rank.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#swiss-system-tournament","title":"Swiss-system tournament","text":"<ul> <li>https://en.wikipedia.org/wiki/Swiss-system_tournament</li> </ul> <p>A Swiss-system tournament is a non-eliminating tournament format that features a fixed number of rounds of competition, but considerably fewer than for a round-robin tournament; thus each competitor (team or individual) does not play all the other competitors. Competitors meet one-on-one in each round and are paired using a set of rules designed to ensure that each competitor plays opponents with a similar running score, but does not play the same opponent more than once. The winner is the competitor with the highest aggregate points earned in all rounds. With an even number of participants, all competitors play in each round.</p> <p>The Swiss system seeks to provide a clear winner with a large number of competitors and a relatively small number of rounds of competition, without a single bad result terminating participation. In a Swiss system the match pairing for each round is done after the previous round has ended and depends on its results.</p> <p>Assuming no drawn games, determining a clear winner (and, incidentally, a clear loser) would require the same number of rounds as that of a knockout tournament, which is the binary logarithm of the number of players rounded up.</p> <p>Notice that it requires the same number of rounds, but the number of matches will be higher because it is constant throughout the rounds compared to teh knockout tournament.</p> <p>Another advantage compared to knockout tournaments is that the final ranking gives some indication of the relative strengths of all contestants, not just of the tournament winner.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#round-robin-tournament","title":"Round-robin tournament","text":"<p>https://en.wikipedia.org/wiki/Round-robin_tournament</p> <p>A round-robin tournament or all-play-all tournament is a competition format in which each contestant meets every other participant, usually in turn.</p> <p>In theory, a round-robin tournament is the fairest way to determine the champion from among a known and fixed number of contestants. Each contestant, whether player or team, has equal chances against all other opponents because there is no prior seeding of contestants that will preclude a match between any given pair. The element of luck is seen to be reduced as compared to a knockout system since one or two bad performances need not ruin a competitor's chance of ultimate victory. Final records of participants are more accurate, in the sense that they represent the results over a longer period against the same opposition.</p> <p>Round-robins can suffer from being too long compared to other tournament types, and with later scheduled games potentially not having any substantial meaning. They may also require tie-breaking procedures.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#results","title":"Results","text":""},{"location":"modeling/Iteration_43_train_a_verifier/#generating-wrong-predictions","title":"Generating wrong predictions","text":"<p>It is surprisingly difficult to generate wrong predictions for the training dataset. That is why I'm going to train new models that do not use the training dataset for training. We can modify the temperature of the inference to force the errors, but it also increases the number of non valid predictions.</p> <p></p>"},{"location":"modeling/Iteration_43_train_a_verifier/#accuracy-of-voting","title":"Accuracy of voting","text":"<p>For these models voting is able to select the best response in the first position around 60% of the times, and around 70% in the top two positions.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#first-results-with-verifier","title":"First results with verifier","text":"<p>It's very nice to see that the first attempt to select predictions with a verifier already shows improvements over the voting baseline. The top_2 selection improves specially, from ~70% to ~80%. An improvement of that magnitude on leaderboard would improve the LB score from 40 to 45-46.</p> <ul> <li>It seems that using more than 32 verifications per prediction does not increase the accuracy of the selection</li> <li>Verifying is faster than generating grids, 30% faster, but I would expected much faster because we are generating very few output tokens compared to generating a grid. It seems that the long input prompt is dominating over the inference.</li> <li>The plot above is for the predictions: <code>20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json</code></li> <li>As a verifier model I used: <code>/mnt/hdd0/Kaggle/arc24/models/20241023_first_verifiers/05_verify-and-select_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_8000steps_2gpus_8192msl/checkpoint-8000</code></li> </ul>"},{"location":"modeling/Iteration_43_train_a_verifier/#trying-with-different-models","title":"Trying with different models","text":"<p>I trained different models, just varying the number of training steps. We can visualize how the accuracy is affected.</p> <p></p> <ul> <li>The best model seems to be the one trained for 8k steps</li> <li>The quality of the model is relevant, f.e. the models trained for 1k and 2k steps are clearly worse.   Thus it is likely that we can train even better models that outputperform this initial trainings.</li> </ul>"},{"location":"modeling/Iteration_43_train_a_verifier/#first-submissions","title":"First submissions","text":""},{"location":"modeling/Iteration_43_train_a_verifier/#problems-with-submission-and-evaluations","title":"Problems with submission and evaluations","text":"<p>The first submission attempts have timeout out when doing 32 predictions per tasks and 32 verifications per prediction.</p> <p>Running a validation submission with 20 tasks took 2h40 seconds.</p> <ul> <li>Training took 1h49</li> <li>Inference took 13 min (32 predictions per task)</li> <li>Verification took 30min (32 verifications per prediction)</li> </ul> <p>The problem was that I was using Qwen2.5 as the base model, when I trained the verifiers on Qwen2.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#first-results","title":"First results","text":"<p>Please go to Iteration 45 to see the results.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#conclusion","title":"Conclusion","text":"<p>The verifier has predictive power to select the predictions, but we need to improve it.</p>"},{"location":"modeling/Iteration_43_train_a_verifier/#next-steps","title":"Next steps","text":"<ul> <li>How could I improve the verifier?<ul> <li>Use a bigger dataset</li> <li>Train also on the test set (just for submission)</li> <li>More data augmentation, allow task augmentation</li> <li>Maybe use an ensemble of models instead of a single model</li> <li>I could use a bigger model, full fine-tuned</li> <li>It's likely that a model trained in all the prediction tasks will perform better</li> <li>Use voting to solve ties</li> <li>I could make more efficient use of compute by using uncertainty and only making verifications for   the predictions that are not significative different from the top prediction.</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_43_train_a_verifier/#todo","title":"TODO","text":"<ul> <li> Maybe I can force VLLM to generate different predictions for the same prompt?</li> <li> Train a model without the train dataset to generate wrong predictions</li> <li> Create a dataset that can be used to train a verifier.<ul> <li> How do the wrong answers look like?</li> <li> It has to be of the train dataset, so I can measure the improvement on the evaluation set.</li> </ul> </li> <li> Create prompts to select between answers</li> <li> How to integrate this new task into the training script?<ul> <li> How should I format the dataset?</li> <li> How to apply data augmentation?</li> <li> ~Add support for task augmentation~ Currently is only applied to input or output, so it would not work with a new key</li> </ul> </li> <li> Train a model to select answers</li> <li> What is the best way to use the model? There might be some compute intensive way and a faster and approximate one</li> <li> Measure the improvement over voting</li> <li> Can I train a single model to do all the tasks?</li> <li> What if instead of choosing two options I just request the model to answer if the proposed output is correct or not?   That would simplify the post-processing a lot.</li> <li> Create a script to select predictions using a verifier (use the notebook as a template). </li> <li> I would probably need an easy run script also.</li> <li> Differences between verifier models and selection accuracy</li> <li> First submission results</li> </ul>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/","title":"Iteration 44. Learn to use Strong Compute","text":"<p>22-10-2024</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#goal","title":"Goal","text":"<p>Learn to use the Strong Compute cluster.</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#motivation","title":"Motivation","text":"<p>Strong compute has graciously granted me with compute credits to speedup my development in the last weeks of the challenge. That means I will have access to GPUs with 80GBs of memory and I could train much faster than with the Veridas cluster.</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#development","title":"Development","text":"<ul> <li>Strong compute documentation</li> <li>ISC demos repo</li> <li>Strong compute Discord</li> </ul>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#quick-guide-to-connect-to-strong-compute","title":"Quick guide to connect to Strong compute","text":"<ol> <li>Go to Strong compute control panel and start a workstation.</li> <li>Start the vpn with <code>sudo wg-quick up wg0</code></li> <li>Connect to the workstation using vscode</li> <li>After all the work has been done disconnect the vpn <code>sudo wg-quick down wg0</code></li> <li>And stop the workstation</li> </ol>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#creating-a-python-environment-for-the-experiments","title":"Creating a python environment for the experiments","text":"<p>I already have the requirements on <code>requirements.txt</code> file, so I just have to clone the repo into the workstation.</p> <p>To do so I have created an ssh key doing <code>ssh-keygen</code> on the workstation and added the public key to github.</p> <pre><code>cd ~/code/arc24\npython3 -m virtualenv ~/envs/arc24\nsource ~/envs/arc24/bin/activate\npip install -r requirements.txt\npip install flash-attn --no-build-isolation\n</code></pre>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#first-training","title":"First training","text":"<pre><code>isc_project_id = \"46f4672b-2489-457f-b302-eab855b36b70\"\nexperiment_name = \"first_arc24_training\"\ngpu_type = \"24GB VRAM GPU\"\ngpus = 8\ncompute_mode = \"burst\"\noutput_path = \"~/outputs/first_arc24_training\"\ncommand = \"source ~/envs/arc24/bin/activate &amp;&amp; cd ~/code/arc24/scripts &amp;&amp; ~/jobs/job.sh\"\nburst_shape_priority_list = [\"oblivus-mon1-h100n\"]\n</code></pre> <p>Better configuration with the whole command:</p> <pre><code>isc_project_id = \"46f4672b-2489-457f-b302-eab855b36b70\"\nexperiment_name = \"first_arc24_training_A100n_v2\"\ngpu_type = \"24GB VRAM GPU\"\ngpus = 8\ncompute_mode = \"burst\"\ndataset_id = \"0cfd54a3-4096-494e-93d5-a073126e81e2\"\noutput_path = \"~/outputs\"\nburst_shape_priority_list = [\"oblivus-mon1-a100n\"]\ncommand = '''\nsource ~/envs/arc24/bin/activate &amp;&amp; \nsource ~/jobs/secrets.sh &amp;&amp; \naccelerate launch --num_processes 8 --num_machines 1 --mixed_precision bf16 --multi_gpu \n~/code/arc24/scripts/fine-tuning.py \n--max_steps 10000 \n--model_path=Qwen/Qwen2.5-0.5B-Instruct \n--lora_r 64 \n--output_dir ${OUTPUT_PATH}/models/20241022_no_training/08_A100n_lora064-Qwen2.5-0.5B-Instruct_lr1e-4_bs16_10000steps_2gpus_8192msl\n--n_gpus=8 \n--batch_size=16 \n--device_map None \n--no-verbose \n--compose_new_task_probability 0.5 \n--compose_new_task_weights 1 1 1 1 \n--max_seq_len 8192 \n--learning_rate=1e-4 \n--train_datasets ~/code/arc24/data/original_data/arc-agi_evaluation_challenges.json output-from-examples-v1 \n--train_datasets ~/code/arc24/data/external_data/kaggle.json output-from-examples-v1  \n--train_datasets ~/code/arc24/data/external_data/pqa-dataset-1k.json output-from-examples-v1  \n--train_datasets ~/code/arc24/data/external_data/neoeye_tama.json output-from-examples-v1  \n--train_datasets ~/code/arc24/data/external_data/MINI-ARC.json output-from-examples-v1  \n--train_datasets ~/code/arc24/data/original_data/arc-agi_evaluation_challenges.json input-from-inputs-v0 \n--train_datasets ~/code/arc24/data/external_data/kaggle.json input-from-inputs-v0  \n--train_datasets ~/code/arc24/data/external_data/pqa-dataset-1k.json input-from-inputs-v0  \n--train_datasets ~/code/arc24/data/external_data/neoeye_tama.json input-from-inputs-v0  \n--train_datasets ~/code/arc24/data/external_data/MINI-ARC.json input-from-inputs-v0  \n--val_dataset ~/code/arc24/data/original_data/arc-agi_training_challenges.json output-from-examples-v1 \n--remove_train_samples_to_fit_max_seq_len \n--eval_steps 200\n--warmup_ratio 1e-1'''\n</code></pre>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#initial-idea","title":"Initial idea","text":"<ul> <li>Since my datasets are small I believe I can work on the root folder.</li> <li>They have said that it only has sense to use GPUs in multiples of 8.</li> <li>H100 is newer and faster than a100</li> </ul>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#h100-vs-a100","title":"H100 vs A100","text":"<p>https://oblivus.com/pricing/</p> <p></p> <p>The Nvlink machines are slightly more expensive than the pcie. For multi-gpu training it should be faster so probably it's better to just avoid using the PCIE machines.</p> <p>The H100 is more expensive than the A100, we have to see if the speedup is worth it.</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#debugging-burst-errors","title":"Debugging burst errors","text":"<p>We can find a <code>.tar.zst</code> file in the exports folder. We should copy it first to a different folder because the exports folder is a fused folder. Then we can untar it.</p> <pre><code>cp exports/183e895a-bbb1-4e3a-b9e8-f3ee02c5e5cb.tar.zst copied_exports\napt-get install -y zstd\ntar --use-compress-program=unzstd -xvf 183e895a-bbb1-4e3a-b9e8-f3ee02c5e5cb.tar.zst\n</code></pre>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#copy-big-datasets","title":"Copy big datasets","text":"<pre><code>scp -P 51468 evaluation_v0.json root@192.168.127.70:~/code/arc24/data/verifier\nscp -P 51468 training_v1.json  root@192.168.127.70:~/code/arc24/data/verifier\nscp -P 51468 /mnt/hdd0/Kaggle/arc24/data/rearc/v2/re-arc.json root@192.168.127.70:~/code/arc24/data/external_data\n\n# other machine\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/verifier/evaluation_v0.json root@94.156.8.239:~/code/arc24/data/verifier\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/verifier/training_v1.json  root@94.156.8.239:~/code/arc24/data/verifier\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/rearc/v2/re-arc.json root@94.156.8.239:~/code/arc24/data/external_data\n</code></pre>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#continue-from-checkpoints","title":"Continue from checkpoints","text":"<p>The trainings are unstable and fail without explanation with <code>strong_fail</code>. The good thing is that those fails do not cost me money, the bad thing is that it requires me to baby sit the experiments. However it trains 6x faster so it is worth it.</p> <p>I believe I have to implement two scripts:</p> <ol> <li>Copy checkpoints from failed experiments. It will copy the last checkpoint from a failed experiment    to a common folder in the root directory. I will call this script manually.</li> <li>When starting a new training, check if there are checkpoints available and copy them. This way    the training will continue from the last checkpoint.</li> </ol> <p>Useful commands:</p> <pre><code>for checkpoint in  af05f40c-f0b0-417c-8285-bc77f2978c61 57f38e65-f2cd-4777-80cd-004dd6da45b6 191aed3c-9409-48b5-81d1-4ed738c9b44d e4c6b8dc-415a-41cc-9034-4b1a70b7fce4 a4891197-4253-4fb2-89e6-d1a1696b01a6 9fafa3c2-5272-46a8-87ea-19ae39f4de4b f4a9c4fe-46a4-490f-9eb9-fde3d9c991e8; do copy-checkpoint ${checkpoint}; done\n\nfor submission in *; do isc train  ${submission}; done\n</code></pre>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#results","title":"Results","text":"<p>First training with 8xA100 is 5x times faster than using 2xA6000.</p> <p>However in the following days I have been unable to run new trainings, and finally after 2 days of struggling I have managed to run two jobs (on A100 and H100) but they were 4 times slower than the previous run and they have finished with <code>strong_error</code> after less than 2 hours of running.</p> <p>Today is Sunday and I have been able to run 3 fast trainings, one slow. I don't see any speed difference between A100 and H100. I have tried using a batch size of 2 per GPU but it is not faster.</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#conclusion","title":"Conclusion","text":"<p>So far it seems that Strong Compute cluster is very unstable. But we have seen that we can train at least 5 times faster in a machine with 8xA100. So we could go directly to a cloud provider and do a fast training if necessary.</p>"},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_44_learn_to_use_strong_compute/#todo","title":"TODO","text":"<ul> <li> Create a python environment for the experiments</li> <li> Copy the data and code to the ISC (instant super computer)</li> <li> Train a submission model with strong compute</li> <li> How much faster can I train?</li> <li> Differences between A100 and H100</li> <li> https://huggingface.co/docs/accelerate/en/usage_guides/low_precision_training On H100</li> <li> Multi-line submit files TOML https://toml.io/en/</li> </ul>"},{"location":"modeling/Iteration_45_improve_verifier/","title":"Iteration 45. Improve the verifier approach","text":"<p>26-10-2024</p>"},{"location":"modeling/Iteration_45_improve_verifier/#goal","title":"Goal","text":"<p>On the previous iteration we have seen signs that the verifier approach might work. Let's try to improve that approach.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#motivation","title":"Motivation","text":"<p>Having a more accurate method than voting to select predictions could improve the LB score.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#development","title":"Development","text":""},{"location":"modeling/Iteration_45_improve_verifier/#create-bigger-datasets-for-training","title":"Create bigger datasets for training","text":""},{"location":"modeling/Iteration_45_improve_verifier/#training-set","title":"Training set","text":"<p>By generating more wrong predictions I have increased the size of the training dataset from 48 to 130MB. The mean number of wrong predictions per training sample has increased from 54 to 155, and the total number of wrong predictions has increased from 92k to 267k.s</p>"},{"location":"modeling/Iteration_45_improve_verifier/#evaluation-set","title":"Evaluation set","text":"<p>I have created a first dataset with a mean number of wrong predictions per sample of 163, the file weights 260MB.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#add-task-augmentation-to-verification-task","title":"Add task augmentation to verification task","text":"<p>I have to refactor the code to enable using task augmentation with verification, because currently it is only prepared for <code>input</code> and <code>output</code> grids, not for <code>wrong_prediction</code> grid.</p> Click to see bash commands <pre><code>python fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241026_debug_task_augmentation/01_baseline_no_task_augmentation \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241026_debug_task_augmentation/02_task_augmentation_refactor \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--compose_new_task_probability 0.5 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241026_debug_task_augmentation/03_revert_refactor \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--compose_new_task_probability 0.5 \\\n--verbose\n\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241026_debug_task_augmentation/04_verify_no_task_augmentation \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/verifier/training_v0.json verify-output-from-examples-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--compose_new_task_probability 0.0 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B \\\n--device_map None \\\n--lora_r 128 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241026_debug_task_augmentation/05_verify_with_task_augmentation \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/verifier/training_v0.json verify-output-from-examples-v0 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--compose_new_task_probability 0.5 \\\n--verbose\n</code></pre>"},{"location":"modeling/Iteration_45_improve_verifier/#more-efficient-rank-estimation-using-uncertainty","title":"More efficient rank estimation using uncertainty","text":"<p>Currently I'm doing n verifications with all the predictions. F.e. I have seen that 32 verifications per prediction can be enough to select the best predictions.</p> <p>This works, but my interest is just to select the best 2 predictions and I'm using a lot of compute to get the ranking for all the predictions. If I estimate the uncertainty for the verification ratio of all the predictions I could early discard wrong predictions and just focus on verifying the most promising predictions.</p> <p>I also thought of using voting as a way to solve ties, but I won't have voting numbers for the 2020 solutions. So I should focus on improving the efficiency of estimating the ranking with a verifier model.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#probability-of-using-a-wrong-prediction-for-training","title":"Probability of using a wrong prediction for training","text":"<p>The first implementation has hardcoded the probability of using a wrong prediction for training a verifier to 50%. It uses a balanced dataset of correct and wrong samples.</p> <p>The problem with this approach is that we have around 1700 correct samples and around 270k wrong predictions. If we train for 8k steps with a batch size of 32 the model will have seen each correct sample an average of 75 times. In the other hand the model will have seen on average each wrong prediction 0.5 times. Maybe it has sense to decrease the frequency of using correct samples for training.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#results","title":"Results","text":""},{"location":"modeling/Iteration_45_improve_verifier/#confidence-level-and-verification-time","title":"Confidence level and verification time","text":"<pre><code>without confidence it would take around 2300s\n\n32 verifications max\n80%       90%        95%\n11440\n2960     4168       5152\n2448     2816       3296\n1712     2480       2864\n---------------------------\n938s     1087s      1159s\n</code></pre> <p>23% more time when increasing confidence from 80% to 95%. It is probably worth it. I can reduce the time to 1100 seconds if I do 4 predictions per round instead of 8.</p> <p>With this setup I could use up to 128 verifications per prediction in just 2036 seconds.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#does-the-verifier-work-on-different-models","title":"Does the verifier work on different models?","text":"model top 1 accuracy top 2 accuracy voting baseline 60.00% 70.00% model 1 62.90% 80.40% model 2 55.90% 72.80% model 3 59.00% 79.10% <p>We can see that the verifier can work on different models with a similar level of accuracy. Voting accuracy was almost exactly the same across all the 3 models. The current method does not seem to be better than voting when selecting the top 1.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#does-training-on-a-bigger-dataset-improve-the-accuracy","title":"Does training on a bigger dataset improve the accuracy?","text":"<p>Top 1 accuracy table:</p> training steps baseline more wrong predictions 4000 55.80% 54.60% 8000 61.70% 54.20% 16000 57.10% 62.50% <p>Top 2 accuracy table:</p> training steps baseline more wrong predictions 4000 72.90% 72.50% 8000 77.10% 69.60% 16000 74.60% 78.80% <p>It is unclear if adding more wrong predictions was beneficial.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#does-using-task-augmentation-improve-the-accuracy","title":"Does using task augmentation improve the accuracy?","text":"<p>Top 1 accuracy table:</p> training steps baseline task augmentation 4000 54.60% 52.50% 8000 54.20% 52.10% 16000 62.50% 54.2 <p>Top 2 accuracy table:</p> training steps baseline task augmentation 4000 72.50% 72.90% 8000 69.60% 72.10% 16000 78.80% 76.70% <p>It is unclear if adding task augmentation improves the accuracy. In fact in other experiments the results are worse.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#can-i-achieve-perfect-accuracy-if-training-on-the-evaluation-set","title":"Can I achieve perfect accuracy if training on the evaluation set?","text":"training steps top_1 top_2 4000 63.30% 81.20% 8000 62.10% 89.20% 16000 70.40% 93.80% 32000 75% 93.30% <p>It is surprising that after 32k training steps the model still does not perfectly classify all the tasks from the evaluation set. After reviewing the failed predictions I have seen that in all the cases there were ties with other predictions.</p> <p>On average it would have seen each task 320 times (<code>16000/4/400*32</code>), so if a task has 4 samples it would have seen around 80 times each sample.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#should-i-change-the-probability-of-training-with-a-correct-prediction","title":"Should I change the probability of training with a correct prediction?","text":"correct_probability top_1 top_2 0.1 48.30% 68.30% 0.2 56.20% 79.20% 0.3 52.10% 79.20% 0.5 50.00% 79.20% <p>There is no evidence that suggest that decreasing the probability of using correct predictions gives higher accuracy.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#does-training-for-multiple-tasks-improve-the-accuracy","title":"Does training for multiple tasks improve the accuracy?","text":"<p>Let's train new models from scratch:</p> <ul> <li>Add the new verify and select tasks, without task augmentation</li> <li>Qwen2.5</li> <li>Do the same also for submission, including the evaluation set</li> <li>Train for 40k steps with batch size 32.</li> </ul> model lora_r batch_size training steps top_2 accuracy top_1 accuracy Qwen2.5-0.5B 64 32 4E+04 78.8% 57.1% Qwen2.5-0.5B 96 32 4E+04 72.1% 47.1% NanoLM-0.3B-Instruct-v2 64 32 4E+04 62.1% 39.2% NanoLM-0.3B-Instruct-v2 128 32 4E+04 63.7% 40.4% SmolLM-135M-Instruct-20k fft 32 4E+04 58.3% 38.0% <p>We don't see improvements when training a multi-task model. However I have the feeling that these models are undertrained.</p> <p>TODO: update results with the continuation of the trainings.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#submission-results","title":"Submission results","text":"<p>When using a model to verify the predictions from LLM and 2020 solution I have only achieved a score of 33 when training on the whole ARC, and 30 when training only on the train dataset.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#conclusion","title":"Conclusion","text":"<p>I have not been able to improve the accuracy of using a prediction verifier. It is still around 60% for top_1 selection and 80% for top_2 selection. Remember that voting gets 60% and 70%. Thus we only see an improving on top_2 prediction.</p>"},{"location":"modeling/Iteration_45_improve_verifier/#next-steps","title":"Next steps","text":"<ul> <li>Could the verifier benefit from test-time fine-tuning?</li> <li>Could I improve the selection of predictions by using selection instead of verifying? I might create a select script by tweaking the verify script.</li> <li>If I define some correctness metric over the predictions, that could open the door to a much more   training dataset that won't be using the correct prediction over an over. It is unclear if this   would work better.</li> </ul>"},{"location":"modeling/Iteration_45_improve_verifier/#todo","title":"TODO","text":"<ul> <li> Create bigger dataset for training<ul> <li> Training set</li> <li> Evaluation set</li> </ul> </li> <li> More data augmentation, allow task augmentation</li> <li> Maybe use an ensemble of models instead of a single model</li> <li> It's likely that a model trained in all the prediction tasks will perform better</li> <li> ~Use voting to solve ties~ I won't have voting on 2020 solution.</li> <li> I could make more efficient use of compute by using uncertainty and only making verifications for   the predictions that are not significative different from the top prediction.</li> <li> Verify that it works on Kaggle</li> <li> Review all new code</li> <li> Experiments<ul> <li> Does training on a bigger dataset improve the accuracy? IN PROGRESS</li> <li> Does using task augmentation improve the accuracy? IN PROGRESS</li> <li> Should I change the probability of training with a wrong prediction? IN PROGRESS</li> <li> Does training for multiple tasks improve the accuracy?</li> <li> Train new submission models</li> </ul> </li> <li> Measure improvements over voting in other model predictions</li> <li> Maybe the model is not as accurate in the test set as in the evaluation set?</li> <li> Why cheating did not get perfect accuracy?</li> <li> How many verifications I have to do until it reaches the perfect ranking? 128 verifications does not reach significative differences.       There are ties that avoid reaching the stop point.</li> </ul>"},{"location":"modeling/Iteration_46_revisit_small_llms/","title":"Iteration 46. Revisit small LLMs","text":"<p>28-10-2024</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#goal","title":"Goal","text":"<p>Can I train a smaller LLM than Qwen2.5-0.5B to achieve the same accuracy?</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#motivation","title":"Motivation","text":"<p>I don't have too many ideas that can be implemented in 2 weeks. If I can train a smaller model than then 0.5B Qwen then I could do a longer test-time fine-tuning during the submission or do more inference steps. That could translate to higher LB scores.</p> <p>I encountered two problems on past iterations:</p> <ol> <li>Small models typically have a context length of 2k or less</li> <li>Some models don't even have a chat template</li> </ol>"},{"location":"modeling/Iteration_46_revisit_small_llms/#development","title":"Development","text":""},{"location":"modeling/Iteration_46_revisit_small_llms/#available-options","title":"Available options","text":"<p>It's quite difficult to search for an LLM with a certain configuration. I have found a leaderboard that allows to sort by the number of parameters. I have also found awesome-mobile-llm.</p> model parameters (M) max_position_embeddings rope_theta attention heads has chat-template? AMD-Llama-135m 135 2048 1.00E+04 12 FALSE HuggingFaceTB/SmolLM-135M-Instruct 135 2048 1.00E+04 9 TRUE TinyMistral-248M-Instruct 248 32768 1.00E+04 32 FALSE OpenELM-270M 270 2048 - ? FALSE Mxode/NanoLM-0.3B-Instruct-v2 365 131072 1.00E+06 14 TRUE Qwen2.5-0.5B-Instruct 500 32768 1.00E+06 14 TRUE <p>The SmolLM model has an uneven number of attention heads and VLLM does not support model parallel in that case. However I might not need to use 2 GPUs for such an small model.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#adding-a-chat-template","title":"Adding a chat template","text":"<p>I have noticed that Qwen has the chat template in the tokenizer_config.json.</p> <p>It seems that I can simply copy it and assign to the AMD-Llama-135m model. How do I create a chat template?</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#problems-when-adding-the-chat-template","title":"Problems when adding the chat template","text":"<p>Despite the code being very simple, I get a weird error with the collator. It does not find the keys, although they are in the text.</p> <pre><code>  warnings.warn(\n/home/gbarbadillo/miniconda3/envs/arc/lib/python3.10/site-packages/trl/trainer/utils.py:198: UserWarning: Could not find instruction key `&lt;|im_start|&gt;user` in the following instance: &lt;s&gt; &lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nLet's see if you can solve this simple Abstraction and Reasoning Challenge (ARC) task.\nBelow there are some input-output grid examples that define the task.\n...\n...\n&lt;|im_end|&gt;\n This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n  warnings.warn(\n</code></pre>"},{"location":"modeling/Iteration_46_revisit_small_llms/#local-trainings-to-verify-i-can-train-the-small-models","title":"Local trainings to verify I can train the small models","text":"Click to see bash commands <pre><code>python fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241028_debug_small_LLMs/01_Qwen2.5-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/NanoLM-0.3B-Instruct-v2 \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241028_debug_small_LLMs/02_NanoLM-0.3B-Instruct-v2 \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/SmolLM-135M-Instruct \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241028_debug_small_LLMs/03_SmolLM-135M-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/AMD-Llama-135m \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241028_debug_small_LLMs/04_AMD-Llama-135m \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 1 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 1024 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--remove_train_samples_to_fit_max_seq_len \\\n--verbose\n\n</code></pre>"},{"location":"modeling/Iteration_46_revisit_small_llms/#debug-long-context-fine-tuning","title":"Debug long context fine-tuning","text":"<p>I'm going to create a temporal fine-tuning script to validate the idea of long context fine-tuning.</p> <p>The idea is to try with synthetic questions and responses that cannot be answered if not using a big enough context. If the model has a big enough context answering the questions is trivial. That should be a very clear test to see if the context window of the model has been extended.</p> Click to see bash commands <pre><code>export model=Qwen2.5-0.5B-Instruct\nexport prompt_tokens_target=4000\n\nexport model=SmolLM-135M-Instruct\nexport prompt_tokens_target=4000\npython long-context-fine-tuning.py \\\n--prompt_tokens_target ${prompt_tokens_target} \\\n--model_path /home/gbarbadillo/data/${model} \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241029_debug_long_context/${model}_${prompt_tokens_target}prompt-length \\\n--max_steps 30 \\\n--max_seq_len 4096\n\nexport model=SmolLM-135M-Instruct\nexport prompt_tokens_target=8000\npython long-context-fine-tuning.py \\\n--prompt_tokens_target ${prompt_tokens_target} \\\n--model_path /home/gbarbadillo/data/${model} \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241029_debug_long_context/${model}_${prompt_tokens_target}prompt-length_rope-theta-1e5 \\\n--max_steps 30 \\\n--max_seq_len 8096\n\nexport model=SmolLM-135M-Instruct-20k\nexport prompt_tokens_target=8000\npython long-context-fine-tuning.py \\\n--prompt_tokens_target ${prompt_tokens_target} \\\n--model_path /home/gbarbadillo/data/${model} \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241029_debug_long_context/${model}_${prompt_tokens_target}prompt \\\n--max_steps 30 \\\n--max_seq_len 8096\n</code></pre> <p>Token indices sequence length is longer than the specified maximum sequence length for this model (2511 &gt; 2048). Running this sequence through the model will result in indexing errors</p> <p>TODO: I have proben that by changing rope_theta from 1e4 to 1e5 the model can work with inputs of 8k tokens correctly.</p> <p>One way of increasing the context window is modifying the model at loading, but that adds complexity to the training script:</p> <pre><code>config = AutoConfig.from_pretrained(model_path)\nconfig.max_position_embeddings = 10240\nconfig.rope_theta = 1e5\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    config=config)\n\ntokenizer = AutoTokenizer.from_pretrained(\n        model_path,\n        trust_remote_code=True,\n        model_max_length=10240,\n        max_length=10240,)\n</code></pre> <p>The other and maybe easier option is to modify the <code>.json</code> config files of the model and tokenizer. The result is exactly the same but it does not increase the complexity of the fine-tuning script.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#error-when-resuming-training","title":"Error when resuming training","text":"<p>Error invalid argument at line 396 in file /src/csrc/pythonInterface.cpp https://github.com/bitsandbytes-foundation/bitsandbytes/issues/782</p> <p>One user says that using <code>adamw_torch</code> solves the issue. And it was true, adding <code>--optim adamw_torch</code> to the training arguments solved the problem.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#problem-with-smollm-predictions","title":"Problem with SmolLM predictions","text":"<p>I'm facing a weird error with some fine-tuned SmolLM models:</p> <ol> <li>I saw NaN losses when retraining in Kaggle</li> <li>Inference is empty</li> </ol> <pre><code>\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_training_models/04_full-fine-tune-SmolLM-135M-Instruct-20k_lr2e-4_bs32_40000steps_2gpus_8192msl_adamw-torch/checkpoint-40000\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_training_models/07_continue_full-fine-tune-SmolLM-135M-Instruct-20k_lr1e-3_bs16_40000steps_2gpus_8192msl_adamw-torch/checkpoint-40000/\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241031_smollm_learning_rate/lr1e-4_fft-SmolLM-135M-Instruct-20k_bs16_10000steps_1gpus_8192msl/checkpoint-10000\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_submission_models/06_fft-SmolLM-135M-Instruct-20k_lr1e-3_bs16_100000steps_2gpus_8192msl/checkpoint-36000\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_submission_models/06_fft-SmolLM-135M-Instruct-20k_lr1e-3_bs16_200000steps_2gpus_8192msl/checkpoint-13000\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_submission_models/06_fft-SmolLM-135M-Instruct-20k_lr1e-3_bs16_400000steps_2gpus_8192msl/checkpoint-13500\n\nexport model_path=/mnt/hdd0/Kaggle/arc24/models/20241028_training_models/08_fft-SmolLM-135M-Instruct-20k_lr1e-3_bs16_100000steps_2gpus_8192msl/checkpoint-100000 &amp;&amp;\npython inference.py --model_path ${model_path} --output_filepath /mnt/hdd0/Kaggle/arc24/debug/smollm_problems/debug.json --predictions_per_task 8 --grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" --dataset_path /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json --prompt_version output-from-examples-v1 --temperature 0.0 --n_tasks 1\n</code></pre> <p>After analyzing the output it is always predicting <code>&lt;|endoftext|&gt;</code>, which is the pad token.</p> <p>There is a workaround that could be tried: https://github.com/vllm-project/vllm/issues/3361</p> <p>However the problem was that all inference logits were NaNs, so it was selecting the first token which happened to be <code>&lt;|endoftext|&gt;</code>.</p> <p>The problem was related to training on <code>bfloat16</code> and doing inference with <code>float16</code>.</p> <p>While bfloat16 uses the same number of bits as float16, it has a wider dynamic range but lower precision.</p> <p>It seemed that the model was working on a regime where <code>float16</code> fails but <code>bfloat16</code> works due to its higher dynamic range. That could be solved by using <code>dtype='auto',</code> on VLLM, but I have concerns that in Kaggle might not work, or work more slowly.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#results","title":"Results","text":""},{"location":"modeling/Iteration_46_revisit_small_llms/#increasing-the-context-length-by-increasing-rope_theta","title":"Increasing the context length by increasing <code>rope_theta</code>","text":"<p>On a synthetic task I have probed that I can increase the context length of <code>SmolLM-135M-Instruct</code> to be able to work with prompts of 8k tokens by increasing <code>rope_theta</code> from 1e4 to 1e5.</p> <p></p> <p>The plot above shows how quickly the task is learned once the model has enough context window. Using <code>rope_scaling</code> almost did not have any effect.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#first-evaluation-results","title":"First evaluation results","text":"model lora_r batch_size training steps multi-tasks accuracy pass_n vote_2 vote_1 Qwen2-0.5B fft 16 4E+04 1 12.25% 31.13% 22.62% 18.50% Qwen2-0.5B 32 16 4E+04 1 11.10% 30.25% 22.62% 18.88% Qwen2-0.5B 128 16 4E+04 1 12.73% 32.25% 22.25% 19.00% Qwen2.5-0.5B 64 32 4E+04 4 8.02% 23.75% 18.12% 12.75% Qwen2.5-0.5B 96 32 4E+04 4 7.93% 24.62% 16.38% 12.62% NanoLM-0.3B-Instruct-v2 64 32 4E+04 4 3.93% 18.25% 10.50% 7.25% NanoLM-0.3B-Instruct-v2 128 32 4E+04 4 5.27% 20.00% 12.88% 9.12% SmolLM-135M-Instruct-20k fft 32 4E+04 4 1.83% 9.00% 5.62% 3.88% <ul> <li>First observation is that we are not getting the same results as the baseline Qwen2 models. My believe   is that the new models are undertrained and they need to be trained for longer. I already have launched   training continuations.</li> <li>Second observation is that NanoLM and SmolLM get worse results than Qwen for the same amount of training steps.   \u00bfMaybe we have to train the smaller models for longer? I need to think about this.</li> </ul>"},{"location":"modeling/Iteration_46_revisit_small_llms/#studying-training-dynamics","title":"Studying training dynamics","text":""},{"location":"modeling/Iteration_46_revisit_small_llms/#qwen-vs-nanolm","title":"Qwen vs NanoLM","text":"<p>NanoLM models learns more slowly than Qwen, but so far there is no sign of plateau and it seems that if trained for longer it would have reached the same point as the bigger model.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#qwen-vs-smollm","title":"Qwen vs SmolLM","text":"<p>However the training dynamic of SmolLM is totally different. It learns at teh beginning but quickly decreases the learning speed. Why could this be happening?</p> <ul> <li>Lack of capacity. This might be possible, although the total size of the model is bigger than most of the LoRA adapters that I have trained so far.</li> <li>Bad learning rate schedule</li> <li>Local minima, this might be solved with a different learning rate schedule.</li> </ul>"},{"location":"modeling/Iteration_46_revisit_small_llms/#smollm-optimal-learning-rate","title":"SmolLM optimal learning rate","text":"<p>I have found that to fine-tune SmolLM model I have to use a learning rate almost 10 times bigger than the one I was using.</p> <p>However at the same time using a higher learning rate could result at a model that fails when using <code>float16</code> at inference, as shown in this section</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#trained-models-for-longer","title":"Trained models for longer","text":"<p>I have trained all the models for longer:</p> model lora_r batch_size training steps multi-tasks accuracy pass_n vote_2 vote_1 Qwen2-0.5B 32 16 4.0E+04 1 11.10% 30.25% 22.62% 18.88% Qwen2.5-0.5B 64 16 1.2E+05 4 10.32% 27.62% 19.38% 16.88% NanoLM-0.3B-Instruct-v2 128 16 2.0E+05 4 6.38% 23.12% 14.37% 10.88% SmolLM-135M-Instruct-20k fft 16 2.0E+05 4 4.96% 19.62% 13.38% 9.50% <p>We don't reach the results from the baseline yet, but we are very close. It is likely that we just simply have to train for a bit longer.</p> <p>This experiment shows that smaller LLMs do not reach the accuracy of Qwen despite being trained for longer. Maybe I should go in the opposite direction and try bigger models (although I could not do test-time fine-tuning with them)</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#conclusion","title":"Conclusion","text":"<p>Smaller LLMs do not reach results as good as Qwen2.5-0.5B despite being trained for longer. It seems that we have to go the other direction, try with bigger models instead.</p>"},{"location":"modeling/Iteration_46_revisit_small_llms/#next-steps","title":"Next steps","text":"<ul> <li>Try SmolLM2 on a next iteration</li> </ul>"},{"location":"modeling/Iteration_46_revisit_small_llms/#todo","title":"TODO","text":"<ul> <li> Experiment to validate that I can extend the context window of the model. At the beginning is a simple instruction, then a lot of distraction text. If the model has enough context length the task is trivial, otherwise is impossible.</li> <li> How can I add a chat template to a model?</li> <li> Can I reach the same validation results as old Qwen?<ul> <li> Qwen2.5<ul> <li> 09_lora64-Qwen2.5-0.5B-Instruct_lr1e-4_bs16_120000steps_2gpus_8192msl</li> </ul> </li> <li> Mxode/NanoLM-0.3B-Instruct-v2<ul> <li> 10_lora128-NanoLM-0.3B-Instruct-v2_lr1e-4_bs16_200000steps_2gpus_8192msl</li> </ul> </li> <li> SmolLM-135M-Instruct-20k<ul> <li> 08_fft-SmolLM-135M-Instruct-20k_lr1e-3_bs16_400000steps_2gpus_8192msl</li> </ul> </li> </ul> </li> <li> Make SmolLM great again, do multiple short trainings with different learning rate schedules</li> <li> ~~Does it help to pretrain SmolLM-20k model on text?~~ Cancelled because SmolLM2 was released.<ul> <li> Datasets for long context fine-tuning. https://huggingface.co/blog/wenbopan/long-context-fine-tuning#long-text-data</li> </ul> </li> <li> Check the problem of dtype on Kaggle. Is <code>float32</code> or <code>bfloat16</code> slower on Kaggle? No problem was found.</li> </ul>"},{"location":"modeling/Iteration_47_select_instead_of_verify/","title":"Iteration 47. Select instead of verify","text":"<p>29-10-2024</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#goal","title":"Goal","text":"<p>Can I improve prediction selection if instead of verifying the correctness I select between pairs of predictions?</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#motivation","title":"Motivation","text":"<p>We have seen that verifying the predictions has predictive power and is a method that can improve over voting in some situations. But it is not improving the leaderboard score.</p> <p>Maybe the problem is that if many predictions are considered correct by the model, there is no way to separate them just by verifying. However if we compare those predictions directly and ask the model to select the best one, in that case it is possible that we could discriminate the predictions.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#development","title":"Development","text":""},{"location":"modeling/Iteration_47_select_instead_of_verify/#create-select-predictions-script","title":"Create select predictions script","text":"<pre><code>export checkpoint_folder=/mnt/hdd0/Kaggle/arc24/models/20241026_improve_verifiers/01_verify-and-select_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_16000steps_2gpus_8192msl/checkpoint-16000\npython easy_select_and_evaluation.py \"${checkpoint_folder}\" \\\n--dataset-path /mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json \\\n--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl_checkpoint-80000_inference_evaluation_x032_just_correct_tasks.json \\\n--n-rounds 4\n</code></pre>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#efficient-prediction-selection","title":"Efficient prediction selection","text":"<p>So far we have seen that selection can achieve better accuracy than verify, but at the cost of a much longer computation time. We have to improve the efficiency of the selection.</p> <pre><code>I'm looking for ranking and a match making system. These are the specifications:\n\n- n players\n- 1v1 matches, draw is not an option.\n- I'm only interested in finding the best and second best player, I'm not interested in the other players ranking\n- Skill does not change over time, is frozen. Thus the order of the matches is irrelevant\n- The ranking and match making system should be as efficient as possible, I want to run the smallest number possible of matches because running a match is expensive\n- The method should be robust, there is randomness in the results of the matches. F.e. a double elimination tournament is not robust enough because a player can lost two matches just by chance\n</code></pre> <p>My idea is to work first with a small subset of tasks, and once I have an implementation that feels good enough evaluate all the tasks.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#bradley-terry-model","title":"Bradley-terry model","text":"<p>I believe Bradley-terry model is the correct way to build a ranking where we have static players that do not evolve over time. ELO ranking and Trueskill rely on the history of matches, whereas Bradley-terry does a global optimization using all available matches and does not care about the order of the matches.</p> <p>My only doubt is how fast the algorithm is, but hopefully it should be very fast since the number of players is not very big.</p> <ul> <li>Bradley\u2013Terry model wikipedia</li> <li>Uncertainty quantification in the Bradley-Terry-Luce model</li> <li>Bradley-Terry rating system for Kaggle sim comps</li> </ul>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#successive-halving-algorithms","title":"Successive halving algorithms","text":"<p>The idea is that each player plays <code>n</code> matches per round, and only the top half players pass to the next round. If <code>n</code> is big enough is very likely that the top 2 players will end up being selected.</p> <p>This method should be able to find the best players while being much more efficient than doing all vs all comparisons.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#other-links-from-the-search","title":"Other links from the search","text":"<ul> <li>Active Learning for Top-K Rank Aggregation from Noisy Comparisons</li> <li>Active Ranking using Pairwise Comparisons</li> <li>ChatGPT ideas: 1, 2, 3</li> </ul>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#implementation-details","title":"Implementation details","text":"<pre><code>export checkpoint_folder=/mnt/hdd0/Kaggle/arc24/models/20241026_improve_verifiers/01_verify-and-select_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_16000steps_2gpus_8192msl/checkpoint-16000\nrm /mnt/hdd0/Kaggle/arc24/debug/*debug*.json; python easy_select_and_evaluation.py \"${checkpoint_folder}\" \\\n--dataset-path /mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json \\\n--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl_checkpoint-80000_inference_evaluation_x032_just_correct_tasks.json \\\n--n-rounds 8\n\n#quick command to just run inference\npython select_predictions_with_halving.py --model-path /home/gbarbadillo/data/temp_model --output-path /mnt/hdd0/Kaggle/arc24/debug/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl_checkpoint-80000_inference_evaluation_x032_just_correct_tasks_m14a9650f_001rounds_debug10_selection.json --dataset-path /mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json --predictions-path /mnt/hdd0/Kaggle/arc24/debug/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl_checkpoint-80000_inference_evaluation_x032_just_correct_tasks.json --n-rounds 10\n</code></pre> <p>When using the first 10 tasks, the naive approach does 953 comparisons each round with the all vs all setup.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#check-that-it-can-work-with-empty-predictions","title":"Check that it can work with empty predictions","text":"<p>One of the experiments that I want to try is to make a better selection of the 2020 solution. I know that the 2020 solution scores 26 on the leaderboard, however if I use a single attempt I only score 24. Maybe using the selection model I can keep scoring 26.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#try-on-different-predictions","title":"Try on different predictions","text":"<pre><code>export checkpoint_folder=/mnt/hdd0/Kaggle/arc24/models/20241026_improve_verifiers/01_verify-and-select_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_16000steps_2gpus_8192msl/checkpoint-16000\npython easy_select_and_evaluation.py \"${checkpoint_folder}\" \\\n--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl_checkpoint-80000_inference_evaluation_x032_just_correct_tasks.json\n\npython easy_select_and_evaluation.py \"${checkpoint_folder}\" \\\n--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl_checkpoint-40000_inference_evaluation_x032_just_correct_tasks.json\n\npython easy_select_and_evaluation.py \"${checkpoint_folder}\" \\\n--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl_checkpoint-40000_inference_evaluation_x032_just_correct_tasks.json\n</code></pre>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#results","title":"Results","text":""},{"location":"modeling/Iteration_47_select_instead_of_verify/#analyze-submission","title":"Analyze submission","text":"<p>On this notebook the 2020 solution is used with just 1 attempt (the 2 attempt is empty) and scores 24. With 2 attempts it scores 26.</p> <p>My current submission is using that single attempt 2020 submission that scores 24 and combining it with the result of voting the predictions generated by the transformer. I could use the model to better select the predictions from 2020 and from the transformer, or I could combine all the predictions and select among them. My experience with the verifier model suggests that is better to do the selection of the submissions before combining the two approaches.</p> <p>There are 3 strategies to improve the submission:</p> <ol> <li>Concatenate all the predictions and select the best 2 predictions for each task</li> <li>Select the best 2 predictions for the 2020 solution and my model predictions and combine the predictions</li> <li>Select only best 2 predictions for my model, and combine with the combination of 2020 solution</li> </ol>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#analyze-verifications","title":"Analyze verifications","text":"<p>I have studied the failed verifications and found that 36% of them fail with ties. If I can use the select mode instead to fix that I might have a better prediction selection mode.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#first-results-with-all-vs-all-comparisons","title":"First results with all vs all comparisons","text":"<p>I have created a first implementation that does all vs all comparisons for all the predictions from a task.</p> n_rounds top_1 accuracy top_2 accuracy runtime (s) 1 59.0% 75.4% 895 2 61.9% 79.5% 1773 4 69.4% 81.7% 3655 8 74.3% 79.5% 7226 16 70.5% 83.6% 14371 <p>These results are very promising because we have improved the top_1 accuracy from ~60% to 74% and the top_2 accuracy from ~80% to 83%. Thus it seems that doing comparisons between the predictions is a more accurate method than verifying the predictions.</p> <p>The drawback is that this implementation is very slow. We are doing all versus all comparisons and that gives the same compute to the best prediction and to the last. Since we are just interested in the top_2 predictions I should be able to optimize the runtime by a good margin while preserving or even improving the accuracy.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#improving-the-selection-of-2020-predictions","title":"Improving the selection of 2020 predictions","text":"<p>Using the evaluation dataset we can estimate if we can use model prediction selection to improve the selection of the submission predictions.</p> solution top_2 accuracy top_1 accuracy icecuber 54.0% 48.5% icecuber + model select predictions 54.0% 53.5% program search 14.0% 14.0% combined submission with preference for program search 56.5% 49.0% model select predictions 57.0% 56.5% perfect combination 57.0% 57.0% <p>The table shows that when doing the baseline submission combination we get 49% top_1 accuracy and when we use the model to select predictions we get 56.5% top_1 accuracy which is an amazing improvement. This has great potential to improve the LB score.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#efficient-implementation-results","title":"Efficient implementation results","text":""},{"location":"modeling/Iteration_47_select_instead_of_verify/#scale-runtime","title":"Scale runtime","text":"n_matches per round top_1 accuracy top_2 accuracy runtime (s) 8 65.70% 80.60% 873 32 65.70% 83.20% 1101 32 69.40% 79.90% 1101 64 69.80% 82.80% 1658 128 70.90% 82.10% 3064 256 73.50% 82.10% 6141 512 71.30% 81.00% 12175 <p>There is too much uncertainty in the metrics to see a clear trend, but using more compute seems to lead to more accuracy.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#different-models","title":"Different models","text":"model training tasks top 1 accuracy top 2 accuracy Qwen2-0.5B 2 69.4% 79.9% Qwen2.5-0.5B 4 70.9% 82.1% NanoLM-0.3B-Instruct-v2 4 54.5% 69.4% <p>The table shows that we can achieve similar results when training a model to do more tasks. It also shows that smaller models give worse results.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#can-i-achieve-perfect-accuracy-if-training-on-the-evaluation-set","title":"Can I achieve perfect accuracy if training on the evaluation set?","text":"<p>I have used the model <code>/mnt/hdd0/Kaggle/arc24/models/20241026_improve_verifiers/04_cheat-with-evaluation-no-task-augmentation_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_16000steps_2gpus_8192msl/checkpoint-16000</code> from the previous iteration and with <code>max_matches_per_round=32</code> it achieves top_1 accuracy of 84.3% and top_2 accuracy of 94.8%. When using a verifier this numbers were 75% and 93.30% so there is a clear improvement here.</p> <p>If I use the model <code>/mnt/hdd0/Kaggle/arc24/models/20241028_submission_models/01_lora96-Qwen2.5-0.5B-Instruct_lr1e-4_bs32_100000steps_2gpus_8192msl/checkpoint-100000</code> the accuracy is even higher: 90.7% and 96.3%. So maybe that is the model that we should be using for submission. This model was also trained to predict the outputs and the inputs for the ARC tasks. So Omni-ARC approach seems to be working.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#submission-results","title":"Submission results","text":"<p>I have created a new notebook to see if I can improve the selection of 2020 solution predictions.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#2020-solution","title":"2020 solution","text":"<p>First attempts get scores of 20 and 21.5, which implies a top_1 accuracy of 76% and 82% respectively. This is better than the accuracy on the evaluation set.</p> <p>The drawback is that is not as a good as the proof of concept of the 2020 solution on the evaluation data, which achieved an impressing 99% top_1 accuracy.</p> <p>Maybe is better to do just use the previous 2020 prediction selection which scored 24.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#full-system","title":"Full system","text":"<p>I have made two submissions that use the model to select the predictions of the test-time fine-tuning model.</p> selection method LB score voting 37, 38, 38 model 34, 34 <p>The results are clear, voting get's higher accuracy than the selection model.</p> <p>This contradicts my results on the evaluation dataset. Maybe the predictions generated by a test-time fine-tuned model are more favorable for voting than the predictions of a frozen model.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#conclusion","title":"Conclusion","text":"method top 1 accuracy top 2 accuracy voting 60.0% 70.0% verification 59.3% 77.4% selection 68.7% 81.0% <p>Using a model to select predictions is the more accurate method of all. It improves an absolute 10% on top_1 and top_2 over voting.</p> <p>We implemented an efficient selection algorithm that uses successive halving to focus on the most promising selections, and the Bradley-Terry ranking system to sort the predictions.</p> <p>However submission results do not show improvements over voting. My hypothesis is that the predictions of the test-time fine-tuned model follow a different distribution than the frozen model. And that different distribution could improve the accuracy of the voting algorithm.</p>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#next-steps","title":"Next steps","text":"<ul> <li>Test time fine-tuning could be use to train a verifier, if wrong predictions are created for the   the test data. However I don't believe I have enough compute to do that. If the small LLMs would have worked that would   have been a real option.</li> </ul>"},{"location":"modeling/Iteration_47_select_instead_of_verify/#todo","title":"TODO","text":"<ul> <li> I need more information to understand the failures of verifying. I need to save the results of the verifications for   deeper analysis.</li> <li> Create a first script to do prediction selection with comparisons. First do an all vs all comparison.</li> <li> Update the script to be more efficient and do more selective comparisons, using maybe trueskill.</li> <li> How does the efficient method scales with more compute? (even when it is not feasible, just to know).</li> <li> Try the models trained on the previous iteration 46.</li> <li> Submission results</li> </ul>"},{"location":"modeling/Iteration_48_more_external_data/","title":"Iteration 48. More External data","text":"<p>03-11-2024</p>"},{"location":"modeling/Iteration_48_more_external_data/#goal","title":"Goal","text":"<p>Can we improve the LB score by using more external data?</p>"},{"location":"modeling/Iteration_48_more_external_data/#motivation","title":"Motivation","text":"<p>The paper Combining Induction and Transduction for Abstract Reasoning along with a 400k tasks dataset has been just published.</p> <p>It is very likely that training my models in this extra data could result on improved accuracy, so I have to do it and do it fast.</p>"},{"location":"modeling/Iteration_48_more_external_data/#development","title":"Development","text":""},{"location":"modeling/Iteration_48_more_external_data/#download-datasets","title":"Download datasets","text":"<pre><code># with git\ngit lfs install\ngit clone git@hf.co:datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems\ngit clone git@hf.co:datasets/barc0/100k-gpt4-description-gpt4omini-code_generated_problems\ngit clone git@hf.co:datasets/barc0/100k-gpt4omini-description-gpt4omini-code_generated_problems\n# with wget\nwget https://huggingface.co/datasets/barc0/100k-gpt4-description-gpt4omini-code_generated_problems/resolve/main/100k-gpt4-description-gpt4omini-code_generated_problems.jsonl?download=true -O 100k-gpt4-description-gpt4omini-code_generated_problems.jsonl\nwget https://huggingface.co/datasets/barc0/100k-gpt4omini-description-gpt4omini-code_generated_problems/resolve/main/100k_gpt4o-mini_generated_problems.jsonl?download=true -O 100k_gpt4o-mini_generated_problems.jsonl\nwget https://huggingface.co/datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/resolve/main/data_100k.jsonl?download=true -O data_100k.jsonl\nwget https://huggingface.co/datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/resolve/main/data_suggestfunction_100k.jsonl?download=true -O data_suggestfunction_100k.jsonl\n</code></pre>"},{"location":"modeling/Iteration_48_more_external_data/#barc-dataset-object","title":"Barc dataset object","text":"<p>Since I have seen that the dataset uses a new format, I'm going to create dataset to handle all the particularities and just expose a sample task method.</p>"},{"location":"modeling/Iteration_48_more_external_data/#local-experiments-to-validate-implementation","title":"Local experiments to validate implementation","text":"<p>Let's verify that we can train with the BARC datasets:</p> Click to see bash commands <pre><code># mixed use of datasets\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241103_debug_BARC/01_Qwen2.5-0.5B-Instruct \\\n--train_datasets /mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--train_datasets barc-400-10-/mnt/hdd0/Kaggle/arc24/data/barc/100k-gpt4-description-gpt4omini-code_generated_problems/100k-gpt4-description-gpt4omini-code_generated_problems.jsonl output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\n# just use barc\npython fine-tuning.py \\\n--model_path /home/gbarbadillo/data/Qwen2.5-0.5B-Instruct \\\n--output_dir /mnt/hdd0/Kaggle/arc24/models/20241103_debug_BARC/02_Qwen2.5-0.5B-Instruct \\\n--train_datasets barc-400-10-/mnt/hdd0/Kaggle/arc24/data/barc/100k-gpt4-description-gpt4omini-code_generated_problems/100k-gpt4-description-gpt4omini-code_generated_problems.jsonl output-from-examples-v1 \\\n--val_dataset /mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--grid_encoder \"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\" \\\n--device_map None \\\n--lora_r 32 \\\n--max_steps 10 \\\n--logging_steps 1 \\\n--eval_steps 200 \\\n--batch_size 16 \\\n--learning_rate 1e-4 \\\n--max_seq_len 4096 \\\n--no-resume_from_checkpoint \\\n--random_seed 7 \\\n--verbose\n\n\n</code></pre>"},{"location":"modeling/Iteration_48_more_external_data/#experiment-design","title":"Experiment design","text":"<p>The goal of the experimentation is to see if the Barc datasets are useful, and how much weight should I give to them when training along other datasets. I'm going to train for just 40k steps and just in the task of predicting the output from the examples to simplify the experiment.</p>"},{"location":"modeling/Iteration_48_more_external_data/#results","title":"Results","text":"original datasets weight accuracy pass_n vote_2 vote_1 100% 12.8% 33.4% 23.6% 19.4% 50% 13.3% 30.9% 23.9% 19.1% 33% 13.3% 31.0% 23.4% 19.8% 20% 11.6% 28.3% 20.4% 16.5% 0% 9.2% 23.1% 17.3% 14.8% <p>Accuracy is the metric we can trust more because it is computed with all the predictions, the other metrics are computed task-wise and have more uncertainty.</p> <p>It seems that combining the original datasets with the new BARC datasets increases the accuracy. It is a very small improvement, but it might give an extra point in the leaderboard.</p> <p>It is interesting to notice that they were able to achieve a vote_2 accuracy of 30%. Maybe the difference is just the LLM model size: they used Llama-3-8B and I'm using Qwen2.5-0.5B</p>"},{"location":"modeling/Iteration_48_more_external_data/#conclusion","title":"Conclusion","text":"<p>We observe a small increase in accuracy when adding BARC datasets to the previous training datasets. I recommend to use the BARC datasets in the final trainings.</p>"},{"location":"modeling/Iteration_48_more_external_data/#next-steps","title":"Next steps","text":"<ul> <li>Train a model to generate code (for next years edition)</li> </ul>"},{"location":"modeling/Iteration_48_more_external_data/#todo","title":"TODO","text":"<ul> <li> Explore the dataset</li> <li> How can I train on this new dataset? It is much bigger than the other datasets</li> <li> Does it improve the evaluation accuracy?</li> <li> ~Does it improve the LB score?~ No time to test it, I will use it on the final training</li> </ul>"},{"location":"modeling/Iteration_49_smolLM2/","title":"Iteration 49. SmolLM2","text":"<p>04-11-2024</p>"},{"location":"modeling/Iteration_49_smolLM2/#goal","title":"Goal","text":"<p>Does SmolLM2 improve over SmolLM?</p>"},{"location":"modeling/Iteration_49_smolLM2/#motivation","title":"Motivation","text":"<p>I have just seen the release of SmolLM2. The biggest change is that the model is prepared to handle inputs of 8192 tokens (and I believe even longer inputs up to 20k tokens.) The model also improves the accuracy on some datasets. Thus it is very likely that replacing SmolLM by SmolLM2 will give free improvements.</p>"},{"location":"modeling/Iteration_49_smolLM2/#development","title":"Development","text":""},{"location":"modeling/Iteration_49_smolLM2/#tiny-modifications-to-smollm2","title":"Tiny modifications to SmolLM2","text":"<p>I'm going to simply increase the <code>max_position_embedding</code> from 8192 to 20480, just like I did with <code>SmolLM</code>, both on the <code>config.json</code> and <code>tokenizer_config.json</code></p>"},{"location":"modeling/Iteration_49_smolLM2/#experiment-design","title":"Experiment design","text":"<p>The idea is to run the exact same training experiment I recently did with SmolLM with the new SmolLM2. Hopefully we will see a faster training loss decrease.</p>"},{"location":"modeling/Iteration_49_smolLM2/#results","title":"Results","text":"model accuracy pass_n vote_2 vote_1 Qwen2.5-0.5B-Instruct 10.32% 27.62% 19.38% 16.88% SmollLM-135M-Instruct 4.20% 18.25% 11.25% 9.3% SmollLM2-135M-Instruct 5.52% 20.50% 12.62% 9.8% <p>We can see a noticeable improvement when using SmolLM2 over SmolLM, but it is still far from the accuracy of Qwen2 model.</p>"},{"location":"modeling/Iteration_49_smolLM2/#conclusion","title":"Conclusion","text":"<p>SmolLM2 improves over SmolLM, but not enough to be able to compete with Qwen2.5.</p>"},{"location":"modeling/Iteration_49_smolLM2/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_49_smolLM2/#todo","title":"TODO","text":"<ul> <li> Compare the accuracy on evaluation dataset vs SmolLM</li> </ul>"},{"location":"modeling/Iteration_50_last_trainings/","title":"Iteration 50. Last trainings","text":"<p>06-11-2024</p>"},{"location":"modeling/Iteration_50_last_trainings/#goal","title":"Goal","text":"<p>Can we improve the LB score by training the last models on a powerful server?</p>"},{"location":"modeling/Iteration_50_last_trainings/#motivation","title":"Motivation","text":"<p>There is not enough time to train models on Veridas cluster, I need to use a 8xA100 server to train the models fast enough for the end of the challenge.</p>"},{"location":"modeling/Iteration_50_last_trainings/#development","title":"Development","text":""},{"location":"modeling/Iteration_50_last_trainings/#steps-to-train-the-model","title":"Steps to train the model","text":"<ol> <li>Add the public SSH key of the machine to Github. <code>cat ~/.ssh/id_rsa.pub</code></li> <li>Clone the arc 24 repo to the machine: <code>cd ~/code; git clone git@github.com:ironbar/arc24.git</code></li> <li>Create a python environment for training</li> </ol> <pre><code>cd ~/code/arc24\npython3 -m virtualenv ~/envs/arc24\nsource ~/envs/arc24/bin/activate\npip install -r requirements.txt\npip install flash-attn --no-build-isolation\n</code></pre> <ol> <li>Do some first trainings to see if the training speed is enough</li> </ol> <pre><code>source ~/envs/arc24/bin/activate\nexport gpus=8\nexport batch_size=16\nexport steps=200\nexport per_device_train_batch_size=1\nexport model_path=Qwen/Qwen2.5-0.5B-Instruct\nexport WANDB_API_KEY=\n\naccelerate launch --num_processes ${gpus} --num_machines 1 --mixed_precision bf16 --multi_gpu \\\n/root/code/arc24/scripts/fine-tuning.py \\\n--n_gpus ${gpus} \\\n--batch_size ${batch_size} \\\n--per_device_train_batch_size ${per_device_train_batch_size} \\\n--output_dir /root/models/20241106_debug_training_speed/${gpus}XA100_bs${batch_size}_pdtbs${per_device_train_batch_size}_${steps}steps_$(basename $model_path) \\\n--max_steps ${steps} \\\n--model_path ${model_path} \\\n--lora_r 128 \\\n--device_map None \\\n--no-verbose \\\n--max_seq_len 8192 \\\n--learning_rate 5e-5 \\\n--train_datasets /root/code/arc24/data/original_data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--val_dataset /root/code/arc24/data/original_data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--remove_train_samples_to_fit_max_seq_len \\\n--save_steps 500 \\\n--eval_steps 5000000 \\\n--warmup_ratio 1e-1\n</code></pre> <ol> <li>Copy extra data</li> </ol> <pre><code>export machine_ip=94.156.8.181\nexport machine_ip=94.156.8.119\nexport machine_ip=94.156.8.239\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/verifier/evaluation_v0.json root@${machine_ip}:~/code/arc24/data/verifier\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/verifier/training_v1.json  root@${machine_ip}:~/code/arc24/data/verifier\nscp -P 50022 /mnt/hdd0/Kaggle/arc24/data/rearc/v2/re-arc.json root@${machine_ip}:~/code/arc24/data/external_data\n</code></pre> <ol> <li>Download barc datasets</li> </ol> <pre><code>mkdir /root/code/arc24/data/barc\ncd /root/code/arc24/data/barc\nwget https://huggingface.co/datasets/barc0/100k-gpt4-description-gpt4omini-code_generated_problems/resolve/main/100k-gpt4-description-gpt4omini-code_generated_problems.jsonl?download=true -O 100k-gpt4-description-gpt4omini-code_generated_problems.jsonl\nwget https://huggingface.co/datasets/barc0/100k-gpt4omini-description-gpt4omini-code_generated_problems/resolve/main/100k_gpt4o-mini_generated_problems.jsonl?download=true -O 100k_gpt4o-mini_generated_problems.jsonl\nwget https://huggingface.co/datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/resolve/main/data_100k.jsonl?download=true -O data_100k.jsonl\nwget https://huggingface.co/datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/resolve/main/data_suggestfunction_100k.jsonl?download=true -O data_suggestfunction_100k.jsonl\n</code></pre> <ol> <li>Install packages: <code>apt install screen nvtop htop rsync</code></li> <li>Run the final training:</li> </ol> <pre><code>export model_path=Qwen/Qwen2.5-1.5B-Instruct\nexport learning_rate=5e-5\nexport lora_r=128\nexport gpus=8\nexport batch_size=16\nexport steps=200000\nexport per_device_train_batch_size=1\nexport max_seq_len=8192\nexport WANDB_API_KEY=\n\naccelerate launch --num_processes ${gpus} --num_machines 1 --mixed_precision bf16 --multi_gpu \\\n/root/code/arc24/scripts/fine-tuning.py \\\n--n_gpus ${gpus} \\\n--batch_size ${batch_size} \\\n--per_device_train_batch_size ${per_device_train_batch_size} \\\n--output_dir /root/models/20241106_final_training/$(basename $model_path)_lora${lora_r}_lr${learning_rate}_${steps}steps_${gpus}XA100_bs${batch_size}_pdtbs${per_device_train_batch_size}_msql${max_seq_len} \\\n--max_steps ${steps} \\\n--model_path ${model_path} \\\n--lora_r ${lora_r} \\\n--device_map None \\\n--no-verbose \\\n--max_seq_len ${max_seq_len} \\\n--learning_rate ${learning_rate} \\\n--train_datasets barc-200-20-/root/code/arc24/data/barc/100k-gpt4-description-gpt4omini-code_generated_problems.jsonl output-from-examples-v1 \\\n--train_datasets barc-200-20-/root/code/arc24/data/barc/100k_gpt4o-mini_generated_problems.jsonl output-from-examples-v1 \\\n--train_datasets barc-200-20-/root/code/arc24/data/barc/data_100k.jsonl output-from-examples-v1 \\\n--train_datasets barc-200-20-/root/code/arc24/data/barc/data_suggestfunction_100k.jsonl output-from-examples-v1 \\\n--train_datasets /root/code/arc24/data/verifier/training_v1.json select-output-from-examples-v0 \\\n--train_datasets /root/code/arc24/data/verifier/training_v1.json verify-output-from-examples-v0 \\\n--train_datasets /root/code/arc24/data/verifier/evaluation_v0.json select-output-from-examples-v0 \\\n--train_datasets /root/code/arc24/data/verifier/evaluation_v0.json verify-output-from-examples-v0 \\\n--train_datasets /root/code/arc24/data/original_data/arc-agi_training_challenges.json output-from-examples-v1 \\\n--train_datasets /root/code/arc24/data/original_data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--train_datasets /root/code/arc24/data/external_data/re-arc.json output-from-examples-v1 \\\n--train_datasets /root/code/arc24/data/external_data/kaggle.json output-from-examples-v1  \\\n--train_datasets /root/code/arc24/data/external_data/pqa-dataset-1k.json output-from-examples-v1  \\\n--train_datasets /root/code/arc24/data/external_data/neoeye_tama.json output-from-examples-v1  \\\n--train_datasets /root/code/arc24/data/external_data/MINI-ARC.json output-from-examples-v1  \\\n--train_datasets /root/code/arc24/data/original_data/arc-agi_training_challenges.json input-from-inputs-v0 \\\n--train_datasets /root/code/arc24/data/original_data/arc-agi_evaluation_challenges.json input-from-inputs-v0 \\\n--train_datasets /root/code/arc24/data/external_data/re-arc.json input-from-inputs-v0 \\\n--train_datasets /root/code/arc24/data/external_data/kaggle.json input-from-inputs-v0  \\\n--train_datasets /root/code/arc24/data/external_data/pqa-dataset-1k.json input-from-inputs-v0  \\\n--train_datasets /root/code/arc24/data/external_data/neoeye_tama.json input-from-inputs-v0  \\\n--train_datasets /root/code/arc24/data/external_data/MINI-ARC.json input-from-inputs-v0  \\\n--val_dataset /root/code/arc24/data/original_data/arc-agi_evaluation_challenges.json output-from-examples-v1 \\\n--remove_train_samples_to_fit_max_seq_len \\\n--optim adamw_torch \\\n--save_steps 500 \\\n--eval_steps 5000000 \\\n--warmup_ratio 2e-2\n</code></pre>"},{"location":"modeling/Iteration_50_last_trainings/#syncronize-checkpoints","title":"Syncronize checkpoints","text":"<pre><code>while true; do\n    for machine_ip in 94.156.8.181 94.156.8.119; do\n        rsync -r -avP -e \"ssh -i ~/.ssh/id_rsa.pub -p 50022\" root@${machine_ip}:~/models/20241106_final_training/ /mnt/hdd0/Kaggle/arc24/models/20241106_final_training/\n    done\n    for machine_ip in 192.222.52.72; do\n        rsync -r -avP -e \"ssh -i ~/.ssh/id_rsa.pub\" ubuntu@${machine_ip}:~/models/20241106_final_training/ /mnt/hdd0/Kaggle/arc24/models/20241106_final_training/\n    done\n    sleep 1h\ndone\n\n\n\nwhile true; do\n    for machine_ip in 94.156.8.181 94.156.8.119 94.156.8.239; do\n        rsync -r -avP -e \"ssh -i ~/.ssh/id_rsa.pub -p 50022\" root@${machine_ip}:~/models/20241106_final_training/ /mnt/hdd0/Kaggle/arc24/models/20241106_final_training/\n    done\n    sleep 1h\ndone\n</code></pre>"},{"location":"modeling/Iteration_50_last_trainings/#train-bigger-models","title":"Train bigger models","text":"<p>It might be worth to train bigger models. I already studied the improvements of using bigger models on iteration 20. Also on iteration 46 I have seen that using smaller models produced worse results even when training the models for longer.</p> <p>Now that I have a model that can select predictions, it might be worth to use bigger models even when I cannot do test-time fine-tuning on them.</p>"},{"location":"modeling/Iteration_50_last_trainings/#fp8-training-on-h100","title":"FP8 training on H100","text":"<ul> <li>https://huggingface.co/docs/accelerate/en/usage_guides/low_precision_training</li> <li>https://github.com/NVIDIA/TransformerEngine</li> </ul> <pre><code>pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n</code></pre> <p>I have not been able to install the backends required for fp8 training.</p>"},{"location":"modeling/Iteration_50_last_trainings/#plan-the-final-12-submissions","title":"Plan the final 12 submissions","text":"<p>I'm not sure if I will have a Qwen2.5-7B model, it depends on the availability of a 3rd training machine by compute strong and being able to train the model successfully.</p> <p>I have to answer the following questions:</p> <ul> <li>How good are the new models at selecting predictions? (and how fast also) Answering this question   will allow to better choose the combination methods for the predictions.</li> <li>Do the new models improve the LB score when using the classic test-time fine-tuning submission?</li> <li>Is it worth to make predictions with the model without test-time fine-tuning? That would be fast and   might solve some additional tasks (8 minutes to do 16 predictions per task using Qwen2.5-0.5B).</li> <li>Could I do test-time fine-tuning with the 1.5B model? I could test this in the Kaggle notebook without submission.</li> </ul> <p>There are 3 sources of predictions:</p> <ol> <li>Test-time fine-tuned model</li> <li>Frozen model</li> <li>2020 solution</li> </ol> <p>And there are 4 ways of combining the predictions:</p> <ol> <li>Selection using a model</li> <li>Voting</li> <li>Combination of predictions (first attempt from one submission and the second from other submission)</li> <li>Concatenation</li> </ol> <p>The submissions that I can create are the combination of this two variables.</p>"},{"location":"modeling/Iteration_50_last_trainings/#thursday-7-november","title":"Thursday 7 November","text":"<p>I'm going to have available Qwen2.5-0.5B trained for 200k steps. I have made a classic submission that scored 35 and two triple ensemble submission that scored 35 and 33.</p>"},{"location":"modeling/Iteration_50_last_trainings/#friday-8","title":"Friday 8","text":"<p>I'm going to have available Qwen2.5-0.5B trained for 400k steps and Qwen2.5-1.5B trained for 200k steps.</p> <ul> <li>Submission with Qwen2.5-0.5B and learning rate 4e-5: 38</li> <li>Submission with Qwen2.5-1.5B and learning rate 1e-5: 35 (but I had an error on <code>max_seq_len</code>, so score might be higher. Also I had a timeout on another submission with Qwen2.5-1.5B)</li> </ul>"},{"location":"modeling/Iteration_50_last_trainings/#saturday-9","title":"Saturday 9","text":"<p>I have available a Qwen2.5-0.5B trained for 400k steps, Qwen2.5-1.5B trained for 300k steps and Qwen2.5-7B trained for 100k steps.</p> <ul> <li>Submission with Qwen2.5-1.5B and learning rate 1e-5: 34</li> <li>Submission with Qwen2.5-1.5B and learning rate 2e-5: 38</li> <li>Submission with Qwen2.5-1.5B and learning rate 4e-5: 38</li> </ul>"},{"location":"modeling/Iteration_50_last_trainings/#sunday-10","title":"Sunday 10","text":"<p>13:00 is the last time to submit a system that takes 12 hours to create the submission</p> <ul> <li>Submission with Qwen2.5-0.5B and learning rate 8e-5: 35</li> <li>Triple ensemble with Qwen2.5-7B: 37 and 33</li> </ul>"},{"location":"modeling/Iteration_50_last_trainings/#test-time-fine-tuning-with-qwen25-15b","title":"Test-time fine-tuning with Qwen2.5-1.5B","text":"<p>On this notebook I have verified that I can do test-time fine-tuning with Qwen2.5-1.5B.</p> <p>I had to decrease the <code>max_seq_len</code> from 5120 to 3584 to avoid OOM errors.</p> <ul> <li>Fine-tuning took 2.6 minutes per task for 100 steps</li> <li>Inference took 1.2 minutes per task to do 32 predictions</li> </ul> <p>Under that conditions a submission would take around 6 hours. It seems we have margin to increase either the training steps or the number of predictions.</p> <p>Experiment with bigger steps and predictions:</p> <ul> <li>64 predictions -&gt; 1.5 minutes per tasks</li> <li>200 steps -&gt; 4.3 minutes per task</li> </ul> <p>Which learning rate to use? I can't use the evaluation set to tune the learning rate. However I can have a look at the Qwen2.5-0.5B models. We trained the models with learning rate between 5e-5 and 1e-4. And the test-time fine-tuning learning rates were 4e-5 and 8e-5 most frequently. Thus we can see that we used almost the exact same learning rate despite using a batch size of 1 for test-time fine-tuning.</p> <p>The Qwen2.5-1.5B was trained with a learning rate of 5e-5. A conservative learning rate for test-time fine-tuning would be 1e-5. If it works we could try increasing it to 2e-5.</p>"},{"location":"modeling/Iteration_50_last_trainings/#test-time-fine-tuning-with-qwen25-7b","title":"Test-time fine-tuning with Qwen2.5-7B","text":"<p>Let's see if we can use test-time fine-tuning with Qwen2.5-7B.</p> <p>If I decrease the <code>max_seq_len</code> to 896 I can fine-tune on Kaggle machines, at a speed of around 2.45 seconds per sample. In those conditions I could only use around 50% of the tasks for training.</p> <p>If I use 4 bit quantization the speed slows down to around 11.5 seconds per sample. Then I have tried disabling gradient checkpointing and the speed increases to 7 seconds per sample. Is better but still very slow.</p> <p>My idea is to create a notebook for the 7B model, that only does test-time fine-tuning on the 50% smallest tasks and does inference with the frozen model for the grouped other 50% tasks. I'm not sure if this might work better than the smaller models but it is worth to try it.</p> <p>I have found that inference is extremely slow because I have to merge the heavy model with the lora adapter before using VLLM. I have tried modifying the inference script to support using LoRAs directly but VLLM does not support dora, and I have used it for training. This would imply that half of the submission time would go to inference (because it's taking 4 minutes to make 8 predictions per task)</p> <p>Thus I don't believe it is worth to use Qwen2.5-7B for test-time fine-tuning. I could use it in an ensemble without fine-tuning.</p>"},{"location":"modeling/Iteration_50_last_trainings/#results","title":"Results","text":""},{"location":"modeling/Iteration_50_last_trainings/#training-speed","title":"Training speed","text":"<p>I have done some initial training speed experiments to verify that the machines work well. I haven't seen any speed improvement by increasing the batch size or increasing the per device train batch size.</p> model hardware it/s Qwen2.5-0.5B-Instruct 8xA100 2.38 Qwen2.5-1.5B-Instruct 8xA100 1.44 Qwen2.5-7B-Instruct 8xA100 0.68 Qwen2.5-7B-Instruct 8xH100 1.09 <p>The H100 might be a good idea for <code>Qwen2.5-7B-Instruct</code>.</p>"},{"location":"modeling/Iteration_50_last_trainings/#qwen25-7b-instruct-is-not-showing-better-results-than-qwen25-15b-instruct","title":"Qwen2.5-7B-Instruct is not showing better results than Qwen2.5-1.5B-Instruct","text":"<p>We show a great improvement of <code>Qwen2.5-1.5B-Instruct</code> over <code>Qwen2.5-0.5B-Instruct</code> on training loss. However we don't observe the same improvement with <code>Qwen2.5-7B-Instruct</code>. I have tried with 3 different learning rates: 1e-5, 2e-5, 5e-5 (diverged)</p> model parameters LoRA parameters for rank 128 0.5B 17M 1.5B 35M 7B 80M <p>We can see that the trainable number of parameters is different, but can it explain the different training dynamics?</p>"},{"location":"modeling/Iteration_50_last_trainings/#effect-of-the-number-of-predictions","title":"Effect of the number of predictions","text":"<p>I'm reusing data from iteration 30 to create the plot below.</p> <p></p> <p>The plot shows that the <code>pass_n</code> metric increases linearly with the logarithmic of the number of predictions. However voting improves with a much lower slope. This is likely due to the fact that the solutions that are found when using a higher number of predictions do not reach the needed majority of the votes.</p> <p>\u00bfMaybe a selection model could scale better? So far I have only used the selection model for 32 predictions.</p>"},{"location":"modeling/Iteration_50_last_trainings/#frozen-model-results","title":"Frozen model results","text":"models LB score Qwen2.5-0.5B-Instruct 13 Qwen2.5-1.5B-Instruct 11 Qwen2.5-7B-Instruct 13 <p>The bigger models do not show better results on LB when making a submission with the frozen model. This submissions were done after the competition end.</p>"},{"location":"modeling/Iteration_50_last_trainings/#conclusion","title":"Conclusion","text":"<p>We have not been able to improve the LB score by training new models. Maybe we should have trained them for longer since we were training for more tasks.</p>"},{"location":"modeling/Iteration_50_last_trainings/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_50_last_trainings/#todo","title":"TODO","text":"<ul> <li> How to sync checkpoints between the servers and my machine?</li> <li> Verify that I can use the bigger models in Kaggle<ul> <li> Upload base models</li> <li> Upload loras from early checkpoints</li> <li> Notebook to do inference without test-time fine-tuning (I believe it already exists)</li> </ul> </li> <li> train Qwen2.5-7B-Instruct with lora_r=64</li> <li> Plan the last 12 submissions</li> <li> Voting and selection dynamics could be different after test-time fine-tuning</li> <li> Could I do test-time fine-tuning with the 1.5B model?</li> <li> Create a notebook for triple ensemble</li> <li> Add submission results from the models without ttft</li> <li> Conclusion</li> </ul>"},{"location":"modeling/Iteration_n/","title":"Iteration n. Iteration_title","text":"<p>start date</p>"},{"location":"modeling/Iteration_n/#goal","title":"Goal","text":""},{"location":"modeling/Iteration_n/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_n/#development","title":"Development","text":""},{"location":"modeling/Iteration_n/#results","title":"Results","text":""},{"location":"modeling/Iteration_n/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_n/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_n/#todo","title":"TODO","text":"<ul> <li>[ ]</li> </ul>"},{"location":"utils/00_Challenge_Workflow/","title":"Challenge workflow","text":""},{"location":"utils/00_Challenge_Workflow/#start-of-the-challenge","title":"Start of the challenge","text":"<ol> <li>Create a repository for the code using cookiecutter</li> <li>Add dates to the calendar</li> <li>Download rules of the challenge</li> <li>Bookmark challenge folder on file explorer</li> <li>Create a Google keep label for tasks and ideas of the challenge</li> <li>Download the challenge data</li> <li>Create a conda environment for the challenge and add it to jupyter</li> </ol> <pre><code>conda create -n arc pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y\nconda activate arc\npython -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\"\nmake env-export\n</code></pre> <ol> <li>Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public.</li> <li>Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge.</li> <li>Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle.</li> <li>Add a nice picture to README</li> </ol>"},{"location":"utils/00_Challenge_Workflow/#end-of-the-challenge","title":"End of the challenge","text":"<ol> <li>Prepare a report with a summary of the approach to the challenge</li> <li>Download the Google keep tasks to the repository in pdf format</li> <li>Delete the tasks on google keep and the label</li> <li>Delete unnecessary data</li> <li>Update the environment yml</li> </ol>"},{"location":"utils/markdown_cheatsheet/","title":"Markdown cheatsheet","text":""},{"location":"utils/markdown_cheatsheet/#examples-of-attaching-images","title":"Examples of attaching images","text":"<p>First an image with markdown syntax</p> <p></p> <p>Next an image with html syntax that allows to control the size</p> <p></p>"},{"location":"utils/markdown_cheatsheet/#examples-of-equations","title":"Examples of equations","text":"<p>Equation on a different line:</p> \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] <p>Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\).</p>"},{"location":"utils/markdown_cheatsheet/#easy-way-to-create-tables","title":"Easy way to create tables","text":"<p>http://www.tablesgenerator.com/markdown_tables#</p> representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216"},{"location":"utils/methodology/","title":"Methodology","text":"<p>I'm following CRISP-DM 1.0 methodology for the reports.</p> <p>I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle.</p> <ol> <li>Business understanding</li> <li>Data understanding</li> <li>Modeling</li> <li>Solution summary</li> </ol>"}]}